[{"title":"The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.","url":"/2021/12/06/数据库/ 解决MySQL数据库同步1236错误/","content":"> 在前面的几篇文章中，介绍了MYSQL主从复制相关的内容，包括主从环境搭建，具体文章可参考：\n>\n> 使用传统方式搭建MySQL 5.7 异步复制环境：http://www.seiang.com/?p=296\n>\n> 基于GTID方式搭建MySQL 5.7 主从复制环境：http://www.seiang.com/?p=334\n\n最近遇到mysql开启gtid做复制时，主从同步断开，从库出现1236错误，导致同步无法进行，本文就这问题记录下处理步骤\n~~~Visual Basic\nLast_IO_Errno: 1236\n\nLast_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.'\n\n~~~\n\n>一般两种情况会出现以上现象\n>\n>1.在主库上手动执行清除二进制日志文件\n>\n>2.主库重启，重新同步时\n\n\n<!--more-->\n解决方法：\n\n1、在主库上执行以下命令，查询gtid_purged，记录下改值\n```\nmysql> show global variables like '%gtid%' \\G;\n```\n\n2、在从库上执行以下命令，查询已经执行过的gtid即gtid_executed，记录下主库的值，本机的不需\n```\nmysql> show global variables like '%gtid%' \\G;\n```\n3、在从库上执行以下命令停止同步线程及重置同步相关信息\n```\nroot@localhost [(none)]> stop slave;\nroot@localhost [(none)]> reset slave;\nroot@localhost [(none)]> reset master;\n```\n4、在从库上设置gtid_purged\n\n该值有两个来源，一是在主库上查询的gtid_purged，二是在从库上查询的已经执行过的gtid_executed值（本机的就不需要，主库上gtid）\n\n注意：一定记得加上从库上已经执行过的gtid，若只设置了主库上的gtid_purged，此时从库会重新拉取主库上所有的二进制日志文件，同步过程会出现其他错误，导致同步无法进行\n```\nmysql>  set @@global.gtid_purged='你的GITD值';\nQuery OK, 0 rows affected (2.12 sec)\n```\n具体如下所示\n```\nroot@localhost [(none)]> set @@global.gtid_purged='dc299ff4-79e5-11e8-8d10-525400cf9369:1-2,dc299ff4-79e5-11e8-8d10-525400cf9369:1-64566';\nQuery OK, 0 rows affected (0.01 sec)\n```\n注意：设置gtid_purged值时，gtid_executed值必须为空否则报错，该值清空的方法就是reset master命令\n\n执行完，再次查看相关信息\n\n5、重新开启同步\n```\nmysql> change master to master_host='MASTER_IP',master_port=PORT,master_user='USERNAME',master_password='PASSWORD',master_auto_position=1;\nQuery OK, 0 rows affected, 2 warnings (5.55 sec)\n\nmysql> start slave;\nQuery OK, 0 rows affected (0.40 sec)\n\n```\n\n\n解决完要验证是否有数据丢失，我做完同步后有少量**数据丢失**！！！！\n\n参考文档：\nhttps://www.cnblogs.com/dukuan/p/8744295.html\nhttps://cloud.tencent.com/developer/article/1796099\nhttps://blog.51cto.com/hnr520/1883282","tags":["mysql"],"categories":["数据库"]},{"title":"k8s教程day5-数据存储、安全认证和dashboard","url":"/2021/12/01/K8S/day5/k8s_day5/","content":"k8s 课程规划\n![](https://user-images.githubusercontent.com/28568478/144197771-e2ed53bf-bb06-46a8-af7b-8ac948fc2cf1.png)\n\n<!--more-->\n\n# 第八章 数据存储\n\n​    在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。\n\n​    Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。\n\nkubernetes的Volume支持多种类型，比较常见的有下面几个：\n\n- 简单存储：EmptyDir、HostPath、NFS\n- 高级存储：PV、PVC\n- 配置存储：ConfigMap、Secret\n\n## 基本存储\n\n### EmptyDir\n\n​    EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。\n\n​    EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下：\n\n- 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留\n\n- 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）\n\n接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。\n\n​    在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。\n\n<img src=\"k8s_day5/image-20200413174713773.png\" style=\"zoom:80%;border:solid 1px\" />\n\n创建一个volume-emptydir.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14-alpine\n    ports:\n    - containerPort: 80\n    volumeMounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"] # 初始命令，动态读取指定文件中内容\n    volumeMounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountPath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptyDir\n  - name: logs-volume\n    emptyDir: {}\n~~~\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@master ~]# kubectl get pods volume-emptydir -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ...... \nvolume-emptydir   2/2     Running   0          97s   10.244.1.100   node1  ......\n\n# 通过podIp访问nginx\n[root@master ~]# curl 10.244.1.100\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@master ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.244.0.0 - - [13/Apr/2020:10:58:47 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.29.0\" \"-\"\n~~~\n\n### HostPath\n\n​    上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。\n\n​    HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。\n\n<img src=\"k8s_day5/image-20200413214031331.png\" style=\"zoom:100%;border:1px solid\" />\n\n创建一个volume-hostpath.yaml：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"]\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    hostPath: \n      path: /root/logs\n      type: DirectoryOrCreate  # 目录存在就使用，不存在就先创建后使用\n~~~\n\n~~~markdown\n关于type的值的一点说明：\n\tDirectoryOrCreate 目录存在就使用，不存在就先创建后使用\n\tDirectory\t目录必须存在\n\tFileOrCreate  文件存在就使用，不存在就先创建后使用\n\tFile 文件必须存在\t\n    Socket\tunix套接字必须存在\n\tCharDevice\t字符设备必须存在\n\tBlockDevice 块设备必须存在\n~~~\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看Pod\n[root@master ~]# kubectl get pods volume-hostpath -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ......\npod-volume-hostpath   2/2     Running   0          16s   10.244.1.104   node1  ......\n\n#访问nginx\n[root@master ~]# curl 10.244.1.104\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n~~~\n\n### NFS\n\n​    HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。\n\n​    NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。\n\n<img src=\"k8s_day5/image-20200413215133559.png\" style=\"zoom:100%;border:1px solid\" />\n\n1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器\n\n~~~powershell\n# 在master上安装nfs服务\n[root@master ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@master ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.109.0/24网段中的所有主机\n[root@master ~]# vim /etc/exports\n[root@master ~]# more /etc/exports\n/root/data/nfs     192.168.109.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@master ~]# systemctl start nfs\n~~~\n\n2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备\n\n~~~powershell\n# 在node上安装nfs服务，注意不需要启动\n[root@master ~]# yum install nfs-utils -y\n~~~\n\n3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"tail -f /logs/access.log\"] \n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.109.100  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n~~~\n\n4）最后，运行下pod，观察结果\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@master ~]# kubectl get pods volume-nfs -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\nvolume-nfs        2/2     Running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@master ~]# ls /root/data/\naccess.log  error.log\n~~~\n\n##高级存储\n\n### PV和PVC\n\n​    前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。\n\n​    PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\n\n​    PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。\n\n<img src=\"k8s_day5/image-20200514194111567.png\" style=\"zoom:100%;border:1px solid\" />\n\n使用了PV和PVC之后，工作可以得到进一步的细分：\n\n- 存储：存储工程师维护\n- PV：  kubernetes管理员维护\n- PVC：kubernetes用户维护\n\n### PV\n\nPV是存储资源的抽象，下面是资源清单文件:\n\n~~~yaml\napiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n~~~\n\nPV 的关键配置参数说明：\n\n- **存储类型**\n\n  底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异\n\n- **存储能力（capacity）**\n\n​      目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置\n\n- **访问模式（accessModes）**\n\n  用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\n\n  - ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载\n  - ReadOnlyMany（ROX）：  只读权限，可以被多个节点挂载\n  - ReadWriteMany（RWX）：读写权限，可以被多个节点挂载\n\n  `需要注意的是，底层不同的存储类型可能支持的访问模式不同`\n\n- **回收策略（persistentVolumeReclaimPolicy）**\n\n  当PV不再被使用了之后，对其的处理方式。目前支持三种策略：\n\n  - Retain  （保留）  保留数据，需要管理员手工清理数据\n  - Recycle（回收）  清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*\n  - Delete  （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务\n\n  `需要注意的是，底层不同的存储类型可能支持的回收策略不同`\n\n- **存储类别**\n\n  PV可以通过storageClassName参数指定一个存储类别\n\n  - 具有特定类别的PV只能与请求了该类别的PVC进行绑定\n\n  - 未设定类别的PV则只能与不请求任何类别的PVC进行绑定\n\n- **状态（status）**\n\n  一个 PV 的生命周期中，可能会处于4中不同的阶段：\n\n  - Available（可用）：     表示可用状态，还未被任何 PVC 绑定\n  - Bound（已绑定）：     表示 PV 已经被 PVC 绑定\n  - Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明\n  - Failed（失败）：         表示该 PV 的自动回收失败\n\n**实验**\n\n使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。\n\n1) 准备NFS环境\n\n~~~powershell\n# 创建目录\n[root@master ~]# mkdir /root/data/{pv1,pv2,pv3} -pv\n\n# 暴露服务\n[root@master ~]# more /etc/exports\n/root/data/pv1     192.168.109.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.109.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.109.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@master ~]#  systemctl restart nfs\n~~~\n\n2) 创建pv.yaml\n\n~~~yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.109.100\n\n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.109.100\n    \n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.109.100\n~~~\n\n~~~powershell\n# 创建 pv\n[root@master ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@master ~]# kubectl get pv -o wide\nNAME   CAPACITY   ACCESS MODES  RECLAIM POLICY  STATUS      AGE   VOLUMEMODE\npv1    1Gi        RWX            Retain        Available    10s   Filesystem\npv2    2Gi        RWX            Retain        Available    10s   Filesystem\npv3    3Gi        RWX            Retain        Available    9s    Filesystem\n~~~\n\n### PVC\n\nPVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\n\n~~~yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n~~~\n\nPVC 的关键配置参数说明：\n\n- **访问模式（accessModes）**\n\n​       用于描述用户应用对存储资源的访问权限\n\n- **选择条件（selector）**\n\n  通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选\n\n- **存储类别（storageClassName）**\n\n  PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出\n\n- **资源请求（Resources ）**\n\n  描述对存储资源的请求\n\n**实验**\n\n1)  创建pvc.yaml，申请pv\n\n~~~yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n      \n---\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n     \n---\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n~~~\n\n~~~powershell\n# 创建pvc\n[root@master ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@master ~]# kubectl get pvc  -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX                           15s   Filesystem\npvc2   Bound    pv2      2Gi        RWX                           15s   Filesystem\npvc3   Bound    pv3      3Gi        RWX                           15s   Filesystem\n\n# 查看pv\n[root@master ~]# kubectl get pv -o wide\nNAME  CAPACITY ACCESS MODES  RECLAIM POLICY  STATUS    CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWx        Retain          Bound    dev/pvc1    3h37m    Filesystem\npv2    2Gi        RWX        Retain          Bound    dev/pvc2    3h37m    Filesystem\npv3    3Gi        RWX        Retain          Bound    dev/pvc3    3h37m    Filesystem   \n~~~\n\n2)  创建pods.yaml, 使用pv\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"while true;do echo pod1 >> /root/out.txt; sleep 10; done;\"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc1\n        readOnly: false\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"while true;do echo pod2 >> /root/out.txt; sleep 10; done;\"]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc2\n        readOnly: false        \n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP            NODE   \npod1   1/1     Running   0          14s   10.244.1.69   node1   \npod2   1/1     Running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@master ~]# kubectl get pvc -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES      AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX               94m   Filesystem\npvc2   Bound    pv2      2Gi        RWX               94m   Filesystem\npvc3   Bound    pv3      3Gi        RWX               94m   Filesystem\n\n# 查看pv\n[root@master ~]# kubectl get pv -n dev -o wide\nNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWX            Retain           Bound    dev/pvc1    5h11m   Filesystem\npv2    2Gi        RWX            Retain           Bound    dev/pvc2    5h11m   Filesystem\npv3    3Gi        RWX            Retain           Bound    dev/pvc3    5h11m   Filesystem\n\n# 查看nfs中的文件存储\n[root@master ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@master ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n~~~\n\n### 生命周期\n\nPVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期：\n\n- **资源供应**：管理员手动创建底层存储和PV\n\n- **资源绑定**：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定\n\n  在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的\n\n  - 一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了\n\n  - 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV\n\n  PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了\n\n- **资源使用**：用户可在pod中像volume一样使用pvc\n\n  Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。\n\n- **资源释放**：用户删除pvc来释放pv\n\n  当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。\n\n- **资源回收**：kubernetes根据pv设置的回收策略进行资源的回收\n\n  对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用\n\n<img src=\"k8s_day5/image-20200515002806726.png\" style=\"zoom:100%;border:1px solid\" />\n\n## 配置存储\n\n### ConfigMap\n\nConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n\n创建configmap.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n~~~\n\n接下来，使用此配置文件创建configmap\n\n~~~powershell\n# 创建configmap\n[root@master ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@master ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\n\nData\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nEvents:  <none>\n~~~\n\n接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@master ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@master ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key--->文件     value---->文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n~~~\n\n### Secret\n\n​    在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n\n1)  首先使用base64对数据进行编码\n\n~~~yaml\n[root@master ~]# echo -n 'admin' | base64 #准备username\nYWRtaW4=\n[root@master ~]# echo -n '123456' | base64 #准备password\nMTIzNDU2\n~~~\n\n2)  接下来编写secret.yaml，并创建Secret\n\n~~~yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n~~~\n\n~~~powershell\n# 创建secret\n[root@master ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@master ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       <none>\nAnnotations:  <none>\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n~~~\n\n3) 创建pod-secret.yaml，将上面创建的secret挂载进去：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@master ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@master ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n~~~\n\n至此，已经实现了利用secret实现了信息的编码。\n\n# 第九章 安全认证\n\n本章节主要介绍Kubernetes的安全认证机制。\n\n## 访问控制概述\n\n​    Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种**客户端**进行**认证和鉴权**操作。\n\n**客户端**\n\n在Kubernetes集群中，客户端通常有两类：\n\n- **User Account**：一般是独立于kubernetes之外的其他服务管理的用户账号。\n\n- **Service Account**：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。\n\n<img src=\"k8s_day5/image-20200520102949189.png\" style=\"zoom:100%;border:1px solid\" />\n\n**认证、授权与准入控制**   \n\nApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程：\n\n- Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证\n- Authorization（授权）：  判断用户是否有权限对访问的资源执行特定的动作\n- Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。\n\n<img src=\"k8s_day5/image-20200520103942580.png\" style=\"zoom:100%; border:1px solid\" />\n\n## 认证管理\n\nKubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式：\n\n- HTTP Base认证：通过用户名+密码的方式认证\n\n  ~~~markdown\n      这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n  ~~~\n\n- HTTP Token认证：通过一个Token来识别合法用户\n\n  ~~~markdown\n      这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。\n  ~~~\n\n- HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式\n\n  ~~~markdown\n      这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n  ~~~\n  \n\n<img src=\"k8s_day5/image-20200518211037434.png\" style=\"zoom:100%;border:1px solid\" />\n\n**HTTPS认证大体分为3个过程：**\n\n1. 证书申请和下发\n  \n   ~~~markdown\n     HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者\n   ~~~\n  \n2. 客户端和服务端的双向认证\n  \n   ~~~markdown\n     1> 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n        客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n        客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n     2> 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n        在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n   ~~~\n  \n3. 服务器端和客户端进行通信\n  \n   ~~~markdown\n     服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n     服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n   ~~~\n\n> 注意:  Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n\n## 授权管理\n\n​     授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n\n​     每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\n\nAPI Server目前支持以下几种授权策略：\n\n- AlwaysDeny：表示拒绝所有请求，一般用于测试\n\n- AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略）\n\n- ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制\n\n- Webhook：通过调用外部REST服务对用户进行授权\n\n- Node：是一种专用模式，用于对kubelet发出的请求进行访问控制\n\n- RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项）\n\n\nRBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：**给哪些对象授予了哪些权限**\n\n其中涉及到了下面几个概念：\n\n- 对象：User、Groups、ServiceAccount\n- 角色：代表着一组定义在资源上的可操作动作(权限)的集合\n- 绑定：将定义好的角色跟用户绑定在一起\n\n<img src=\"k8s_day5/image-20200519181209566.png\" style=\"zoom:100%;border:1px solid\" />\n\nRBAC引入了4个顶级资源对象：\n\n- Role、ClusterRole：角色，用于指定一组权限\n- RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象\n\n**Role、ClusterRole**\n\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n\n~~~yaml\n# Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [\"\"]  # 支持的API组列表,\"\" 空字符串，表示核心API群\n  resources: [\"pods\"] # 支持的资源对象列表\n  verbs: [\"get\", \"watch\", \"list\"] # 允许的对资源对象的操作方法列表\n~~~\n\n~~~yaml\n# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n~~~\n\n需要详细说明的是，rules中的参数：\n\n- apiGroups: 支持的API组列表\n\n  ```bash\n  \"\",\"apps\", \"autoscaling\", \"batch\"\n  ```\n\n- resources：支持的资源对象列表\n\n  ```bash\n  \"services\", \"endpoints\", \"pods\",\"secrets\",\"configmaps\",\"crontabs\",\"deployments\",\"jobs\",\n  \"nodes\",\"rolebindings\",\"clusterroles\",\"daemonsets\",\"replicasets\",\"statefulsets\",\n  \"horizontalpodautoscalers\",\"replicationcontrollers\",\"cronjobs\"\n  ```\n\n- verbs：对资源对象的操作方法列表\n\n  ```bash\n  \"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\", \"exec\"\n  ```\n\n**RoleBinding、ClusterRoleBinding**\n\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。\n\n~~~yaml\n# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n~~~\n\n~~~yaml\n# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n~~~\n\n**RoleBinding引用ClusterRole进行授权**\n\nRoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。\n\n~~~markdown\n    一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n~~~\n\n~~~yaml\n# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n~~~\n\n**实战：创建一个只能管理dev空间下Pods资源的账号**\n\n1) 创建账号\n\n~~~powershell\n# 1) 创建证书\n[root@master pki]# cd /etc/kubernetes/pki/\n[root@master pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@master pki]# openssl req -new -key devman.key -out devman.csr -subj \"/CN=devman/O=devgroup\"     \n# 2-2) 签署证书\n[root@master pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@master pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@master pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@master pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@master pki]# kubectl config use-context devman@kubernetes\nSwitched to context \"devman@kubernetes\".\n\n# 查看dev下pod，发现没有权限\n[root@master pki]# kubectl get pods -n dev\nError from server (Forbidden): pods is forbidden: User \"devman\" cannot list resource \"pods\" in API group \"\" in the namespace \"dev\"\n\n# 切换到admin账户\n[root@master pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context \"kubernetes-admin@kubernetes\".\n~~~\n\n2） 创建Role和RoleBinding，为devman用户授权\n\n~~~yaml\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n  \n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: devman\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n~~~\n\n~~~powershell\n[root@master pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n~~~\n\n3) 切换账户，再次验证\n\n~~~powershell\n# 切换账户到devman\n[root@master pki]# kubectl config use-context devman@kubernetes\nSwitched to context \"devman@kubernetes\".\n\n# 再次查看\n[root@master pki]# kubectl get pods -n dev\nNAME                                 READY   STATUS             RESTARTS   AGE\nnginx-deployment-66cb59b984-8wp2k    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     Running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@master pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context \"kubernetes-admin@kubernetes\".\n~~~\n\n## 准入控制\n\n通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。\n\n准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器：\n\n~~~powershell\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n~~~\n\n只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。\n\n当前可配置的Admission Control准入控制如下：\n\n- AlwaysAdmit：允许所有请求\n- AlwaysDeny：禁止所有请求，一般用于测试\n- AlwaysPullImages：在启动容器之前总去下载镜像\n- DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求\n- ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。\n- Service Account：实现ServiceAccount实现了自动化\n- SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效\n- ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标\n- LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制\n- InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置\n- NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。\n- DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节\n- DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min\n- PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制\n\n# 第十章 DashBoard\n\n​    之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。\n\n## 部署Dashboard\n\n1) 下载yaml，并运行Dashboard\n\n~~~powershell\n# 下载yaml\n[root@master ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的Service类型\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort  # 新增\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@master ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@master ~]# kubectl get pod,svc -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     Running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     Running   0          111s\n\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE\nservice/dashboard-metrics-scraper  ClusterIP  10.96.89.218    <none>       8000/TCP        111s\nservice/kubernetes-dashboard       NodePort   10.104.178.171  <none>       443:30009/TCP   111s\n~~~\n\n2）创建访问账户，获取token\n\n~~~powershell\n# 创建账号\n[root@master-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@master-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@master ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@master ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nName:         dashboard-admin-token-xbqhh\nNamespace:    kubernetes-dashboard\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw\nca.crt:     1025 bytes\n~~~\n\n3）通过浏览器访问Dashboard的UI\n\n在登录页面上输入上面的token\n\n<img src=\"k8s_day5/image-20200520144548997.png\" alt=\"image-20200520144548997\" style=\"zoom:80%;border:1px solid\" />\n\n出现下面的页面代表成功\n\n<img src=\"k8s_day5/image-20200520144959353.png\" alt=\"image-20200520144959353\" style=\"zoom:80%;border:1px solid\" />\n\n## 使用DashBoard\n\n本章节以Deployment为例演示DashBoard的使用\n\n**查看**\n\n选择指定的命名空间`dev`，然后点击`Deployments`，查看dev空间下的所有deployment\n\n<img src=\"k8s_day5/image-20200520154628679.png\" style=\"zoom:90%;border:1px solid\" />\n\n**扩缩容**\n\n在`Deployment`上点击`规模`，然后指定`目标副本数量`，点击确定\n\n<img src=\"k8s_day5/image-20200520162605102.png\" style=\"zoom:90%;border:1px solid\" />\n\n**编辑**\n\n在`Deployment`上点击`编辑`，然后修改`yaml文件`，点击确定\n\n<img src=\"k8s_day5/image-20200520163253644.png\" alt=\"image-20200520163253644\" style=\"zoom:100%;border:1px solid\" />\n\n**查看Pod**\n\n点击`Pods`, 查看pods列表\n\n<img src=\"k8s_day5/image-20200520163552110.png\" style=\"zoom:90%;border:1px solid\" />\n\n**操作Pod**\n\n选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\n\n<img src=\"k8s_day5/image-20200520163832827.png\" style=\"zoom:90%;border:1px solid\" />\n\n> Dashboard提供了kubectl的绝大部分功能，这里不再一一演示","tags":["k8s 教程"],"categories":["k8s"]},{"title":"k8s教程day4-pod控制器和service及ingress讲解","url":"/2021/12/01/K8S/day4/k8s_day4/","content":"k8s 课程规划\n![](https://user-images.githubusercontent.com/28568478/144197771-e2ed53bf-bb06-46a8-af7b-8ac948fc2cf1.png)\n\n<!--more-->\n# 第六章 Pod控制器详解\n\n本章节主要介绍各种Pod控制器的详细使用。\n\n## Pod控制器介绍\n\nPod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类：\n\n- 自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建\n\n- 控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建       \n\n> **`什么是Pod控制器`** \n>\n> ​    Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。\n\n在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些：\n\n- ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代\n\n- ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级\n\n- Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本\n\n- Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷\n\n- DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务\n\n- Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务\n\n- Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行\n\n- StatefulSet：管理有状态应用\n\n## ReplicaSet(RS)\n\n​    ReplicaSet的主要作用是**保证一定数量的pod正常运行**，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。\n\n![](image-20200612005334159.png)\n\nReplicaSet的资源清单文件：\n\n~~~yaml\napiVersion: apps/v1 # 版本号\nkind: ReplicaSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n~~~\n\n在这里面，需要新了解的配置项就是`spec`下面几个选项：\n\n- replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1\n\n- selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制\n\n  ​               在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了\n\n- template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义\n\n**创建ReplicaSet**\n\n创建pc-replicaset.yaml文件，内容如下：\n\n~~~yaml\napiVersion: apps/v1\nkind: ReplicaSet   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n~~~\n\n~~~powershell\n# 创建rs\n[root@master ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# DESIRED:期望副本数量  \n# CURRENT:当前副本数量  \n# READY:已经准备好提供服务的副本数量\n[root@master ~]# kubectl get rs pc-replicaset -n dev -o wide\nNAME          DESIRED   CURRENT READY AGE   CONTAINERS   IMAGES             SELECTOR\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@master ~]# kubectl get pod -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          54s\npc-replicaset-fmb8f   1/1     Running   0          54s\npc-replicaset-snrk2   1/1     Running   0          54s\n~~~\n\n**扩缩容**\n\n~~~powershell\n# 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@master ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@master ~]# kubectl get pods -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          114m\npc-replicaset-cftnp   1/1     Running   0          10s\npc-replicaset-fjlm6   1/1     Running   0          10s\npc-replicaset-fmb8f   1/1     Running   0          114m\npc-replicaset-s2whj   1/1     Running   0          10s\npc-replicaset-snrk2   1/1     Running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@master ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@master ~]# kubectl get pods -n dev\nNAME                       READY   STATUS        RESTARTS   AGE\npc-replicaset-6vmvt   0/1     Terminating   0          118m\npc-replicaset-cftnp   0/1     Terminating   0          4m17s\npc-replicaset-fjlm6   0/1     Terminating   0          4m17s\npc-replicaset-fmb8f   1/1     Running       0          118m\npc-replicaset-s2whj   0/1     Terminating   0          4m17s\npc-replicaset-snrk2   1/1     Running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@master ~]# kubectl get pods -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npc-replicaset-fmb8f   1/1     Running   0          119m\npc-replicaset-snrk2   1/1     Running   0          119m\n~~~\n\n**镜像升级**\n\n~~~powershell\n# 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@master ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@master ~]# kubectl get rs -n dev -o wide\nNAME                DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@master ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@master ~]# kubectl get rs -n dev -o wide\nNAME                 DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n~~~\n\n**删除ReplicaSet**\n\n~~~powershell\n# 使用kubectl delete命令会删除此RS以及它管理的Pod\n# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除\n[root@master ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps \"pc-replicaset\" deleted\n[root@master ~]# kubectl get pod -n dev -o wide\nNo resources found in dev namespace.\n\n# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@master ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps \"pc-replicaset\" deleted\n[root@master ~]# kubectl get pods -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\npc-replicaset-cl82j   1/1     Running   0          75s\npc-replicaset-dslhb   1/1     Running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@master ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps \"pc-replicaset\" deleted\n~~~\n\n## Deployment(Deploy)\n\n​    为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。\n\n![](image-20200612005524778.png)\n\nDeployment主要功能有下面几个：\n\n- 支持ReplicaSet的所有功能\n- 支持发布的停止、继续\n- 支持滚动升级和回滚版本\n\nDeployment的资源清单文件：\n\n~~~yaml\napiVersion: apps/v1 # 版本号\nkind: Deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionHistoryLimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n~~~\n\n**创建deployment**\n\n创建pc-deployment.yaml，内容如下：\n\n~~~yaml\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n~~~\n\n~~~powershell\n# 创建deployment\n[root@master ~]# kubectl create -f pc-deployment.yaml --record=true\ndeployment.apps/pc-deployment created\n\n# 查看deployment\n# UP-TO-DATE 最新版本的pod的数量\n# AVAILABLE  当前可用的pod的数量\n[root@master ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   3/3     3            3           15s\n\n# 查看rs\n# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串\n[root@master ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   3         3         3       23s\n\n# 查看pod\n[root@master ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          107s\npc-deployment-6696798b78-smpvp   1/1     Running   0          107s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          107s\n~~~\n\n**扩缩容**\n\n~~~powershell\n# 变更副本数量为5个\n[root@master ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@master ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@master ~]#  kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          94s\npc-deployment-6696798b78-mktqv   1/1     Running   0          93s\npc-deployment-6696798b78-smpvp   1/1     Running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@master ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@master ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     Running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          5m23s\n~~~\n\n**镜像更新**\n\ndeployment支持两种更新策略:`重建更新`和`滚动更新`,可以通过`strategy`指定策略类型,支持两个属性:\n\n~~~markdown\nstrategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod\n  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。\n    maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n~~~\n\n重建更新\n\n1) 编辑pc-deployment.yaml,在spec节点下添加更新策略\n\n~~~yaml\nspec:\n  strategy: # 策略\n    type: Recreate # 重建更新\n~~~\n\n2) 创建deploy进行验证\n\n~~~powershell\n# 变更镜像\n[root@master ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@master ~]#  kubectl get pods -n dev -w\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-65qcw   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     Running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     Pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     Pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     Pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     ContainerCreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     Running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     Running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     Running             0          2s\n~~~\n\n滚动更新\n\n1) 编辑pc-deployment.yaml,在spec节点下添加更新策略\n\n~~~yaml\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate:\n      maxSurge: 25% \n      maxUnavailable: 25%\n~~~\n\n2) 创建deploy进行验证\n\n~~~powershell\n# 变更镜像\n[root@master ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@master ~]# kubectl get pods -n dev -w\nNAME                           READY   STATUS    RESTARTS   AGE\npc-deployment-c848d767-8rbzt   1/1     Running   0          31m\npc-deployment-c848d767-h4p68   1/1     Running   0          31m\npc-deployment-c848d767-hlmz4   1/1     Running   0          31m\npc-deployment-c848d767-rrqcn   1/1     Running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     Pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     Running             0          1s\npc-deployment-c848d767-h4p68    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     Pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     Running             0          2s\npc-deployment-c848d767-hlmz4    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     Pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     Running             0          0s\npc-deployment-c848d767-8rbzt    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     Pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     Running             0          2s\npc-deployment-c848d767-rrqcn    0/1     Terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n~~~\n\n滚动更新的过程：\n\n<img src=\"k8s_day4/image-20200416140251491.png\" style=\"zoom:150%;border:1px solid\" />\n\n镜像滚动更新中rs的变化:\n\n~~~powershell\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@master ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n~~~\n\n**版本回退**\n\ndeployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\n\nkubectl rollout： 版本升级相关功能，支持下面的选项：\n\n- status       显示当前升级状态\n- history     显示 升级历史记录\n\n- pause       暂停版本升级过程\n- resume    继续已经暂停的版本升级过程\n- restart      重启版本升级过程\n- undo        回滚到上一级版本（可以使用--to-revision回滚到指定版本）\n\n~~~powershell\n# 查看当前升级版本的状态\n[root@master ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment \"pc-deployment\" successfully rolled out\n\n# 查看升级历史记录\n[root@master ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nREVISION  CHANGE-CAUSE\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@master ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@master ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n~~~\n\n**金丝雀发布**\n\n​    Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。\n\n​    比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n\n```powershell\n# 更新deployment的版本，并配置暂停deployment\n[root@master ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@master ~]# kubectl rollout status deploy pc-deployment -n dev　\nWaiting for deployment \"pc-deployment\" rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@master ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@master ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-rj8sq   1/1     Running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     Running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     Running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@master ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@master ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@master ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6c9f56fcfb-7bfwh   1/1     Running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     Running   0          37s\n```\n\n**删除Deployment**\n\n~~~powershell\n# 删除deployment，其下的rs和pod也将被删除\n[root@master ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps \"pc-deployment\" deleted\n~~~\n\n## Horizontal Pod Autoscaler(HPA)\n\n​    在前面的课程中，我们已经可以实现通过手工执行`kubectl scale`命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标--自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。\n\n​    HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。\n\n<img src=\"k8s_day4/image-20200608155858271.png\" style=\"border: 1px solid; zoom: 80%;\"/>\n\n接下来，我们来做一个实验\n\n**1 安装metrics-server**\n\nmetrics-server可以用来收集集群中的资源使用情况\n\n~~~powershell\n# 安装git\n[root@master ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@master ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@master ~]# cd /root/metrics-server/deploy/1.8+/\n[root@master 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostNetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n~~~\n\n![image-20200608163326496](image-20200608163326496.png)\n\n~~~powershell\n# 安装metrics-server\n[root@master 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@master 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     Running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@master 1.8+]# kubectl top node\nNAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nmaster   98m          4%     1067Mi          62%\nnode1    27m          1%     727Mi           42%\nnode2    34m          1%     800Mi           46%\n[root@master 1.8+]# kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\ncoredns-6955765f44-7ptsb          3m           9Mi\ncoredns-6955765f44-vcwr5          3m           8Mi\netcd-master                       14m          145Mi\n...\n# 至此,metrics-server安装完成\n~~~\n\n**2 准备deployment和servie**\n\n为了操作简单,直接使用命令\n\n~~~powershell\n# 创建deployment \n[root@master 1.8+]# kubectl run nginx --image=nginx:latest --requests=cpu=100m -n dev\n# 创建service\n[root@master 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev\n\n# 查看\n[root@master 1.8+]# kubectl get deployment,pod,svc -n dev\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           47s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-7df9756ccc-bh8dr   1/1     Running   0          47s\n\nNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/nginx   NodePort   10.109.57.248   <none>        80:31136/TCP   35s\n~~~\n\n**3 部署HPA**\n\n创建pc-hpa.yaml\n\n~~~yaml\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minReplicas: 1  #最小pod数量\n  maxReplicas: 10 #最大pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标\n  scaleTargetRef:   # 指定要控制的nginx信息\n    apiVersion: apps/v1\n    kind: Deployment  \n    name: nginx  \n~~~\n\n~~~powershell\n# 创建hpa\n[root@master 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n[root@master 1.8+]# kubectl get hpa -n dev\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          62s\n~~~\n\n**4 测试**\n\n使用压测工具对service地址`192.168.109.100:31136`进行压测，然后通过控制台查看hpa和pod的变化\n\n`hpa变化`\n\n~~~powershell\n[root@master ~]# kubectl get hpa -n dev -w\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          4m11s\npc-hpa   Deployment/nginx   0%/3%     1         10        1          5m19s\npc-hpa   Deployment/nginx   22%/3%    1         10        1          6m50s\npc-hpa   Deployment/nginx   22%/3%    1         10        4          7m5s\npc-hpa   Deployment/nginx   22%/3%    1         10        8          7m21s\npc-hpa   Deployment/nginx   6%/3%     1         10        8          7m51s\npc-hpa   Deployment/nginx   0%/3%     1         10        8          9m6s\npc-hpa   Deployment/nginx   0%/3%     1         10        8          13m\npc-hpa   Deployment/nginx   0%/3%     1         10        1          14m\n~~~\n\n`deployment变化`\n\n~~~powershell\n[root@master ~]# kubectl get deployment -n dev -w\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n~~~\n\n`pod变化`\n\n~~~powershell\n\n[root@master ~]# kubectl get pods -n dev -w\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-7df9756ccc-bh8dr   1/1     Running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     Pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     Pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     Pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     Pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     Pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     Running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     Running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     Running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     Running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     Running             0          33s\nnginx-7df9756ccc-g56qb   1/1     Running             0          48s\nnginx-7df9756ccc-fgst7   1/1     Running             0          66s\nnginx-7df9756ccc-fgst7   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     Terminating         0          6m50s\n~~~\n\n## DaemonSet(DS)\n\n​    DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。\n\n![](image-20200612010223537.png)\n\nDaemonSet控制器的特点：\n\n- 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上\n- 当节点从集群中移除时，Pod 也就被垃圾回收了\n\n下面先来看下DaemonSet的资源清单文件\n\n~~~yaml\napiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [nginx-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n~~~\n\n创建pc-daemonset.yaml，内容如下：\n\n~~~yaml\napiVersion: apps/v1\nkind: DaemonSet      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n~~~\n\n~~~powershell\n# 创建daemonset\n[root@master ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@master ~]#  kubectl get ds -n dev -o wide\nNAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个Node上都运行一个pod\n[root@master ~]#  kubectl get pods -n dev -o wide\nNAME                 READY   STATUS    RESTARTS   AGE   IP            NODE    \npc-daemonset-9bck8   1/1     Running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     Running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@master ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps \"pc-daemonset\" deleted\n~~~\n\n## Job\n\nJob，主要用于负责**批量处理(一次要处理指定数量任务)**短暂的**一次性(每个任务仅运行一次就结束)**任务。Job特点如下：\n\n- 当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量\n- 当成功结束的pod达到指定的数量时，Job将完成执行\n\n<img src=\"k8s_day4/image-20200618213054113.png\" style=\"zoom:80%;\" />\n\nJob的资源清单文件：\n\n~~~yaml\napiVersion: batch/v1 # 版本号\nkind: Job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1\n  activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualSelector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - {key: app, operator: In, values: [counter-pod]}\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never # 重启策略只能设置为Never或者OnFailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done\"]\n~~~\n\n~~~markdown\n关于重启策略设置的说明：\n    如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变\n    如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1\n    如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always\n~~~\n\n创建pc-job.yaml，内容如下：\n\n~~~yaml\napiVersion: batch/v1\nkind: Job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\"]\n~~~\n\n~~~powershell\n# 创建job\n[root@master ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@master ~]# kubectl get job -n dev -o wide  -w\nNAME     COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES         SELECTOR\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态\n[root@master ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS     RESTARTS      AGE\npc-job-rxg96   1/1     Running     0            29s\npc-job-rxg96   0/1     Completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行Pods的次数为6\n#  parallelism: 3 # 指定job并发运行Pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@master ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS    RESTARTS   AGE\npc-job-684ft   1/1     Running   0          5s\npc-job-jhj49   1/1     Running   0          5s\npc-job-pfcvh   1/1     Running   0          5s\npc-job-684ft   0/1     Completed   0          11s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     ContainerCreating   0          0s\npc-job-jhj49   0/1     Completed           0          11s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-pfcvh   0/1     Completed           0          11s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-fhwf7   0/1     ContainerCreating   0          0s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-5vg2j   0/1     ContainerCreating   0          0s\npc-job-fhwf7   1/1     Running             0          2s\npc-job-v7rhr   1/1     Running             0          2s\npc-job-5vg2j   1/1     Running             0          3s\npc-job-fhwf7   0/1     Completed           0          12s\npc-job-v7rhr   0/1     Completed           0          12s\npc-job-5vg2j   0/1     Completed           0          12s\n\n# 删除job\n[root@master ~]# kubectl delete -f pc-job.yaml\njob.batch \"pc-job\" deleted\n~~~\n\n## CronJob(CJ)\n\n​    CronJob控制器以Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行**时间点**及**重复运行**的方式。也就是说，**CronJob可以在特定的时间点(反复的)去运行job任务**。\n\n<img src=\"k8s_day4/image-20200618213149531.png\" style=\"zoom:80%;\" />\n\nCronJob的资源清单文件：\n\n~~~yaml\napiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - {key: app, operator: In, values: [counter-pod]}\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done\"]\n~~~\n\n~~~markdown\n需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n\t*/1    *      *    *     *\n\t<分钟> <小时> <日> <月份> <星期>\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy:\n\tAllow:   允许Jobs并发运行(默认)\n\tForbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n\tReplace: 替换，取消当前正在运行的作业并用新作业替换它\n~~~\n\n创建pc-cronjob.yaml，内容如下：\n\n~~~yaml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: \"*/1 * * * *\"\n  jobTemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [\"bin/sh\",\"-c\",\"for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\"]\n~~~\n\n~~~powershell\n# 创建cronjob\n[root@master ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@master ~]# kubectl get cronjobs -n dev\nNAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\npc-cronjob   */1 * * * *   False     0        <none>          6s\n\n# 查看job\n[root@master ~]# kubectl get jobs -n dev\nNAME                    COMPLETIONS   DURATION   AGE\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@master ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     Completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     Completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     Running     0          24s\n\n\n# 删除cronjob\n[root@master ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch \"pc-cronjob\" deleted\n~~~\n\n\n\n# 第七章 Service详解\n\n本章节主要介绍kubernetes的流量负载组件：Service和Ingress。\n\n## Service介绍\n\n​    在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。\n\n​    为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。\n\n<img src=\"k8s_day4/image-20200408194716912.png\" style=\"zoom:100%;border:1px solid\" />\n\n​    Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后**它会将最新的Service信息转换成对应的访问规则**。\n\n<img src=\"k8s_day4/image-20200509121254425.png\" style=\"border:1px solid\" />\n\n~~~powershell\n# 10.97.97.97:80 是service提供的访问入口\n# 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用，\n# kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去\n# 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n~~~\n\nkube-proxy目前支持三种工作模式:\n\n**userspace 模式**\n\n​    userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。\n​    该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n\n<img src=\"k8s_day4/image-20200509151424280.png\" style=\"border: 1px solid; zoom: 57%;\" />\n\n**iptables 模式**\n\n​    iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。\n​    该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。\n\n<img src=\"k8s_day4/image-20200509152947714.png\" style=\"zoom: 57%;\"  />\n\n**ipvs 模式**\n\n​    ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。\n\n<img src=\"k8s_day4/image-20200509153731363.png\" style=\"zoom: 57%\" />\n\n~~~powershell\n# 此模式必须安装ipvs内核模块，否则会降级为iptables\n# 开启ipvs\n[root@master ~]# kubectl edit cm kube-proxy -n kube-system\n[root@master ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n~~~\n\n## Service类型\n\nService的资源清单文件：\n\n~~~yaml\nkind: Service  # 资源类型\napiVersion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # Service类型，指定service的访问方式\n  clusterIP:  # 虚拟服务的ip地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项\n  ports: # 端口信息\n    - protocol: TCP \n      port: 3017  # service端口\n      targetPort: 5003 # pod端口\n      nodePort: 31122 # 主机端口\n~~~\n\n- ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问\n- NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务\n- LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持\n- ExternalName： 把集群外部的服务引入集群内部，直接使用\n\n## Service使用\n\n### 实验环境准备\n\n在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置`app=nginx-pod`的标签\n\n创建deployment.yaml，内容如下：\n\n~~~yaml\napiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n~~~\n\n~~~powershell\n[root@master ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@master ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                             READY   STATUS     IP            NODE     LABELS\npc-deployment-66cb59b984-8p84h   1/1     Running    10.244.1.40   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     Running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     Running    10.244.1.39   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo \"10.244.1.40\" > /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@master ~]# curl 10.244.1.40\n10.244.1.40\n[root@master ~]# curl 10.244.2.33\n10.244.2.33\n[root@master ~]# curl 10.244.1.39\n10.244.1.39\n~~~\n\n### ClusterIP类型的Service\n\n创建service-clusterip.yaml文件\n\n~~~yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: ClusterIP\n  ports:\n  - port: 80  # Service端口       \n    targetPort: 80 # pod端口\n~~~\n\n~~~powershell\n# 创建service\n[root@master ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@master ~]# kubectl get svc -n dev -o wide\nNAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-clusterip   ClusterIP   10.97.97.97   <none>        80/TCP    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口\n[root@master ~]# kubectl describe svc service-clusterip -n dev\nName:              service-clusterip\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                10.97.97.97\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看ipvs的映射规则\n[root@master ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@master ~]# curl 10.97.97.97:80\n10.244.2.33\n~~~\n\n**Endpoint**\n\n​    Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。\n\n​    一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，**Endpoints是实现实际服务的端点集合**。换句话说，service和pod之间的联系是通过endpoints实现的。\n\n![image-20200509191917069](image-20200509191917069.png)\n\n**负载分发策略**\n\n对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略：\n\n- 如果不定义，默认使用kube-proxy的策略，比如随机、轮询\n\n- 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上\n\n  此模式可以使在spec中添加`sessionAffinity:ClientIP`选项\n\n~~~powershell\n# 查看ipvs的映射规则【rr 轮询】\n[root@master ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@master ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionAffinity:ClientIP\n\n# 查看ipvs规则【persistent 代表持久】\n[root@master ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr persistent 10800\n  -> 10.244.1.39:80               Masq    1      0          0\n  -> 10.244.1.40:80               Masq    1      0          0\n  -> 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@master ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@master ~]# kubectl delete -f service-clusterip.yaml\nservice \"service-clusterip\" deleted\n~~~\n\n### Headless 类型的Service\n\n​    在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了Headless Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。\n\n> 域名 = **service_name**.**namespace**.svc.cluster.local (svc.cluster.local是默认的)\n\n创建service-headless.yaml\n\n~~~yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-headless\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headless Service\n  type: ClusterIP\n  ports:\n  - port: 80    \n    targetPort: 80\n~~~\n\n~~~powershell\n# 创建service\n[root@master ~]# kubectl create -f service-headless.yaml\nservice/service-headless created\n\n# 获取service， 发现CLUSTER-IP未分配\n[root@master ~]# kubectl get svc service-headless -n dev -o wide\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-headless   ClusterIP   None         <none>        80/TCP    11s   app=nginx-pod\n\n# 查看service详情\n[root@master ~]# kubectl describe svc service-headless  -n dev\nName:              service-headless\nNamespace:         dev\nLabels:            <none>\nAnnotations:       <none>\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                None\nPort:              <unset>  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            <none>\n\n# 查看域名的解析情况\n[root@master ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@master ~]# dig @10.96.0.10 service-headless.dev.svc.cluster.local\nservice-headless.dev.svc.cluster.local. 30 IN A 10.244.1.40\nservice-headless.dev.svc.cluster.local. 30 IN A 10.244.1.39\nservice-headless.dev.svc.cluster.local. 30 IN A 10.244.2.33\n~~~\n\n### NodePort类型的Service\n\n​    在之前的样例中，创建的Service的ip地址只有集群内部才可以访问（即master和worker node上可以服务，外部服务器无法访问），如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是**将service的端口映射到Node的一个端口上**，然后就可以通过`NodeIp:NodePort`来访问service了。\n\n<img src=\"k8s_day4/image-20200620175731338.png\" style=\"border:1px solid\"  />\n\n创建service-nodeport.yaml\n\n~~~yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: NodePort # service类型\n  ports:\n  - port: 80  # Service端口   \n    nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetPort: 80  # pod端口   \n~~~\n\n~~~powershell\n# 创建service\n[root@master ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@master ~]# kubectl get svc -n dev -o wide\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)       SELECTOR\nservice-nodeport   NodePort   10.105.64.191   <none>        80:30002/TCP  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n~~~\n\n### LoadBalancer类型的Service\n\n​    LoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n\n<img src=\"k8s_day4/image-20200510103945494.png\" style=\"border:1px solid\" />\n\n\n### ExternalName类型的Service\n\n​     ExternalName类型的Service用于引入集群外部的服务，它通过`externalName`属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。\n\n<img src=\"k8s_day4/image-20200510113311209.png\" style=\"border:1px solid\" />\n\n~~~yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: ExternalName # service类型\n  externalName: www.baidu.com  #改成ip地址也可以\n~~~\n\n~~~powershell\n# 创建service\n[root@master ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@master ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.\nwww.baidu.com.          30      IN      CNAME   www.a.shifen.com.\nwww.a.shifen.com.       30      IN      A       39.156.66.18\nwww.a.shifen.com.       30      IN      A       39.156.66.14\n~~~\n\n## Ingress介绍\n\n​     在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点：\n\n- NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显\n- LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持\n\n​    基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示：\n\n<img src=\"k8s_day4/image-20200623092808049.png\" style=\"border:1px solid\"/>\n\n​    实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在**Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务**。在这里有两个核心概念：\n\n- ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则\n- ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等\n\nIngress（以Nginx为例）的工作原理如下：\n\n1. 用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service\n2. Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置\n3. Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新\n4. 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则\n\n<img src=\"k8s_day4/image-20200516112704764.png\" style=\"border: 1px solid; zoom: 100%;\" />\n\n## Ingress使用\n\n### 环境准备\n\n**搭建ingress环境**\n\n~~~powershell\n# 创建文件夹\n[root@master ~]# mkdir ingress-controller\n[root@master ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@master ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@master ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@master ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@master ingress-controller]# kubectl get pod -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     Running   0          12h\n\n# 查看service\n[root@master ingress-controller]# kubectl get svc -n ingress-nginx\nNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   NodePort   10.98.75.163   <none>        80:32240/TCP,443:31335/TCP   11h\n~~~\n\n**准备service和pod**\n\n为了后面的实验比较方便，创建如下图所示的模型\n\n<img src=\"k8s_day4/image-20200516102419998.png\" style=\"zoom:80%;border:1px solid\" />\n\n创建tomcat-nginx.yaml\n\n~~~yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerPort: 8080\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n~~~\n\n~~~powershell\n# 创建\n[root@master ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@master ~]# kubectl get svc -n dev\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nnginx-service    ClusterIP   None         <none>        80/TCP     48s\ntomcat-service   ClusterIP   None         <none>        8080/TCP   48s\n~~~\n\n### Http代理\n\n创建ingress-http.yaml\n\n~~~yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n~~~\n\n~~~powershell\n# 创建\n[root@master ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@master ~]# kubectl get ing ingress-http -n dev\nNAME           HOSTS                                  ADDRESS   PORTS   AGE\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@master ~]# kubectl describe ing ingress-http  -n dev\n...\nRules:\nHost                Path  Backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n~~~\n\n### Https代理\n\n创建证书\n\n~~~powershell\n# 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com\"\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n~~~\n\n创建ingress-https.yaml\n\n~~~yaml\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretName: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n~~~\n\n~~~powershell\n# 创建\n[root@master ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@master ~]# kubectl get ing ingress-https -n dev\nNAME            HOSTS                                  ADDRESS         PORTS     AGE\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@master ~]# kubectl describe ing ingress-https -n dev\n...\nTLS:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nRules:\nHost              Path Backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n~~~","tags":["k8s 教程"],"categories":["k8s"]},{"title":"k8s教程day3-Pod详解","url":"/2021/12/01/K8S/day3/k8s_day3/","content":"k8s 课程规划\n![](https://user-images.githubusercontent.com/28568478/144197771-e2ed53bf-bb06-46a8-af7b-8ac948fc2cf1.png)\n\n<!--more-->\n# 第五章 Pod详解\n\n本章节将详细介绍Pod资源的各种配置（yaml）和原理。\n\n## Pod介绍\n\n### Pod结构\n\n<img src=\"k8s_day3/image-20200407121501907.png\" alt=\"image-20200407121501907\" style=\"zoom:80%;\" />\n\n每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类：\n\n- 用户程序所在的容器，数量可多可少\n\n- Pause容器，这是每个Pod都会有的一个**根容器**，它的作用有两个：\n  - 可以以它为依据，评估整个Pod的健康状态\n\n  - 可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信\n\n    ~~~md\n    这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel\n    ~~~\n\n### Pod定义\n\n下面是Pod的资源清单：\n\n~~~yaml\napiVersion: v1     #必选，版本号，例如v1\nkind: Pod       　 #必选，资源类型，例如 Pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，Pod名称\n  namespace: string  #Pod所属的命名空间,默认为\"default\"\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，Pod中容器的详细定义\n  containers:  #必选，Pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingDir: string  #容器的工作目录\n    volumeMounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerPort: int  #容器需要监听的端口号\n      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string    #端口协议，支持TCP和UDP，默认TCP\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #Cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n\t\tpostStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n\t\tpreStop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对Pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged: false\n  restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略\n  nodeName: <string> #设置NodeName表示将该Pod调度到指定到名称的node节点上\n  nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上\n  imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptyDir: {}       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n~~~\n\n\n\n~~~powershell\n#小提示：\n#\t在这里，可通过一个命令来查看每种资源的可配置项\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#\tkubectl explain 资源类型.属性     查看属性的子属性\n[root@master ~]# kubectl explain pod\nKIND:     Pod\nVERSION:  v1\nFIELDS:\n   apiVersion   <string>\n   kind <string>\n   metadata     <Object>\n   spec <Object>\n   status       <Object>\n\n[root@master ~]# kubectl explain pod.metadata\nKIND:     Pod\nVERSION:  v1\nRESOURCE: metadata <Object>\nFIELDS:\n   annotations  <map[string]string>\n   clusterName  <string>\n   creationTimestamp    <string>\n   deletionGracePeriodSeconds   <integer>\n   deletionTimestamp    <string>\n   finalizers   <[]string>\n   generateName <string>\n   generation   <integer>\n   labels       <map[string]string>\n   managedFields        <[]Object>\n   name <string>\n   namespace    <string>\n   ownerReferences      <[]Object>\n   resourceVersion      <string>\n   selfLink     <string>\n   uid  <string>\n~~~\n\n在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分：\n\n- apiVersion   \\<string>     版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到\n- kind \\<string>                类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到\n\n- metadata   \\<Object>     元数据，主要是资源标识和说明，常用的有name、namespace、labels等\n\n- spec \\<Object>               描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述                \n\n- status  \\<Object>            状态信息，里面的内容不需要定义，由kubernetes自动生成\n\n在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性:\n\n- containers   <[]Object>       容器列表，用于定义容器的详细信息 \n- nodeName \\<String>           根据nodeName的值将pod调度到指定的Node节点上\n- nodeSelector   <map[]>      根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上\n- hostNetwork  \\<boolean>    是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n- volumes      <[]Object>       存储卷，用于定义Pod上面挂在的存储信息 \n- restartPolicy\t\\<string>       重启策略，表示Pod在遇到故障的时候的处理策略\n\n## Pod配置\n\n本小节主要来研究`pod.spec.containers`属性，这也是pod配置中最为关键的一项配置。\n\n~~~powershell\n[root@master ~]# kubectl explain pod.spec.containers\nKIND:     Pod\nVERSION:  v1\nRESOURCE: containers <[]Object>   # 数组，代表可以有多个容器\nFIELDS:\n   name  <string>     # 容器名称\n   image <string>     # 容器需要的镜像地址\n   imagePullPolicy  <string> # 镜像拉取策略 \n   command  <[]string> # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     <[]string> # 容器的启动命令需要的参数列表\n   env      <[]Object> # 容器环境变量的配置\n   ports    <[]Object>     # 容器需要暴露的端口号列表\n   resources <Object>      # 资源限制和资源请求的设置\n~~~\n\n### 基本配置\n\n创建pod-base.yaml文件，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n~~~\n\n上面定义了一个比较简单Pod的配置，里面有两个容器：\n\n- nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器）\n- busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合）\n\n~~~powershell\n# 创建Pod\n[root@master pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看Pod状况\n# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪\n# RESTARTS  : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它\n[root@master pod]# kubectl get pod -n dev\nNAME       READY   STATUS    RESTARTS   AGE\npod-base   1/2     Running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的Pod，虽然它暂时有问题\n[root@master pod]# kubectl describe pod pod-base -n dev\n~~~\n\n### 镜像拉取\n\n创建pod-imagepullpolicy.yaml文件，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagePullPolicy: Always # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n~~~\n\nimagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略：\n\n- Always：总是从远程仓库拉取镜像（一直远程下载）\n- IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地  本地没远程下载）\n- Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）\n\n> 默认值说明：\n>\n> ​    如果镜像tag为具体版本号， 默认策略是：IfNotPresent\n>\n> ​\t如果镜像tag为：latest（最终版本） ，默认策略是always\n>\n\n~~~powershell\n# 创建Pod\n[root@master pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看Pod详情\n# 此时明显可以看到nginx镜像有一步Pulling image \"nginx:1.17.1\"的过程\n[root@master pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nEvents:\n  Type     Reason     Age               From               Message\n  ----     ------     ----              ----               -------\n  Normal   Scheduled  <unknown>         default-scheduler  Successfully assigned dev/pod-imagePullPolicy to node1\n  Normal   Pulling    32s               kubelet, node1     Pulling image \"nginx:1.17.1\"\n  Normal   Pulled     26s               kubelet, node1     Successfully pulled image \"nginx:1.17.1\"\n  Normal   Created    26s               kubelet, node1     Created container nginx\n  Normal   Started    25s               kubelet, node1     Started container nginx\n  Normal   Pulled     7s (x3 over 25s)  kubelet, node1     Container image \"busybox:1.30\" already present on machine\n  Normal   Created    7s (x3 over 25s)  kubelet, node1     Created container busybox\n  Normal   Started    7s (x3 over 25s)  kubelet, node1     Started container busybox\n~~~\n\n### 启动命令\n\n​    在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n\n​    原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。\n\n创建pod-command.yaml文件，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) >> /tmp/hello.txt; sleep 3; done;\"]\n~~~\n\ncommand，用于在pod中的容器初始化完毕之后运行一个命令。\n\n> 稍微解释下上面命令的意思：\n>\n> ​    \"/bin/sh\",\"-c\",  使用sh执行命令\n>\n> ​    touch /tmp/hello.txt;   创建一个/tmp/hello.txt 文件\n>\n> ​    while true;do /bin/echo $(date +%T) >> /tmp/hello.txt; sleep 3; done;  每隔3秒向文件中写入当前时间\n\n~~~powershell\n# 创建Pod\n[root@master pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看Pod状态\n# 此时发现两个pod都正常运行了\n[root@master pod]# kubectl get pods pod-command -n dev\nNAME          READY   STATUS   RESTARTS   AGE\npod-command   2/2     Runing   0          2s\n\n# 进入pod中的busybox容器，查看文件内容\n# 补充一个命令: kubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh  在容器内部执行命令\n# 使用这个命令就可以进入某个容器的内部，然后进行相关操作了\n# 比如，可以查看txt文件的内容\n[root@master pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n13:35:35\n13:35:38\n13:35:41\n~~~\n\n```md\n特别说明：\n    通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。\n 1 如果command和args均没有写，那么用Dockerfile的配置。\n 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command\n 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数\n 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数\n```\n\n###  环境变量\n\n创建pod-env.yaml文件，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [\"/bin/sh\",\"-c\",\"while true;do /bin/echo $(date +%T);sleep 60; done;\"]\n    env: # 设置环境变量列表\n    - name: \"username\"\n      value: \"admin\"\n    - name: \"password\"\n      value: \"123456\"\n~~~\n\nenv，环境变量，用于在pod中的容器设置环境变量。\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@master ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n~~~\n\n这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n\n### 端口设置\n\n本小节来介绍容器的端口设置，也就是containers的ports选项。\n\n首先看下ports支持的子选项：\n\n~~~powershell\n[root@master ~]# kubectl explain pod.spec.containers.ports\nKIND:     Pod\nVERSION:  v1\nRESOURCE: ports <[]Object>\nFIELDS:\n   name         <string>  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerPort<integer> # 容器要监听的端口(0<x<65536)\n   hostPort     <integer> # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostIP       <string>  # 要将外部端口绑定到的主机IP(一般省略)\n   protocol     <string>  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。\n~~~\n\n接下来，编写一个测试案例，创建pod-ports.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n~~~\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@master ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    ports:\n    - containerPort: 80\n      name: nginx-port\n      protocol: TCP\n......\n~~~\n\n访问容器中的程序需要使用的是`podIp:containerPort`\n\n### 资源配额\n\n​    容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项：\n\n- limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启\n\n- requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动\n\n可以通过上面两个选项设置资源的上下限。\n\n接下来，编写一个测试案例，创建pod-resources.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: \"2\" # CPU限制，单位是core数\n        memory: \"10Gi\" # 内存限制\n      requests: # 请求资源（下限）\n        cpu: \"1\"  # CPU限制，单位是core数\n        memory: \"10Mi\"  # 内存限制\n~~~\n\n在这对cpu和memory的单位做一个说明：\n\n- cpu：core数，可以为整数或小数\n\n- memory： 内存大小，可以使用Gi、Mi、G、M等形式\n\n~~~powershell\n# 运行Pod\n[root@master ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@master ~]# kubectl get pod pod-resources -n dev\nNAME            READY   STATUS    RESTARTS   AGE  \npod-resources   1/1     Running   0          39s   \n\n# 接下来，停止Pod\n[root@master ~]# kubectl delete  -f pod-resources.yaml\npod \"pod-resources\" deleted\n\n# 编辑pod，修改resources.requests.memory的值为10Gi\n[root@master ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@master ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看Pod状态，发现Pod启动失败\n[root@master ~]# kubectl get pod pod-resources -n dev -o wide\nNAME            READY   STATUS    RESTARTS   AGE          \npod-resources   0/2     Pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@master ~]# kubectl describe pod pod-resources -n dev\n......\nWarning  FailedScheduling  <unknown>  default-scheduler  0/2 nodes are available: 2 Insufficient memory.(内存不足)\n~~~\n\n## Pod生命周期\n\n我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程：\n\n- pod创建过程\n\n- 运行初始化容器（init container）过程\n\n- 运行主容器（main container）\n\n  - 容器启动后钩子（post start）、容器终止前钩子（pre stop）\n\n  - 容器的存活性探测（liveness probe）、就绪性探测（readiness probe）\n\n- pod终止过程\n\n<img src=\"k8s_day3/image-20200412111402706.png\" alt=\"image-20200412111402706\" style=\"border:solid 1px\" />\n\n\n在整个生命周期中，Pod会出现5种**状态**（**相位**），分别如下：\n\n- 挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中\n- 运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成\n- 成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启\n- 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态\n- 未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致\n\n### 创建和终止\n\n**pod的创建过程**\n\n1. 用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer\n\n2. apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端\n\n3. apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动\n\n5. scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer\n\n8. node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer\n\n6. apiServer将接收到的pod状态信息存入etcd中\n\n   <img src=\"k8s_day3/image-20200406184656917.png\" alt=\"image-20200406184656917\" style=\"zoom:100%;\" />\n\n\n**pod的终止过程**\n\n1. 用户向apiServer发送删除pod对象的命令\n2. apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead\n3. 将pod标记为terminating状态\n4. kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程\n5. 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除\n6. 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行\n7. pod对象中的容器进程收到停止信号\n8. 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号\n9. kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见\n\n### 初始化容器\n\n初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n\n1. 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成\n2. 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行\n\n初始化容器有很多的应用场景，下面列出的是最常见的几个：\n\n- 提供主容器镜像中不具备的工具程序或自定义代码\n- 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足\n\n接下来做一个案例，模拟下面这个需求：\n\n​    假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器\n\n​    为了简化测试，事先规定好mysql`(192.168.109.201)`和redis`(192.168.109.202)`服务器的地址\n\n创建pod-initcontainer.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n  initContainers:\n  - name: test-mysql\n    image: busybox:1.30\n    command: ['sh', '-c', 'until ping 192.168.109.201 -c 1 ; do echo waiting for mysql...; sleep 2; done;']\n  - name: test-redis\n    image: busybox:1.30\n    command: ['sh', '-c', 'until ping 192.168.109.202 -c 1 ; do echo waiting for reids...; sleep 2; done;']\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@master ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  49s   default-scheduler  Successfully assigned dev/pod-initcontainer to node1\n  Normal  Pulled     48s   kubelet, node1     Container image \"busybox:1.30\" already present on machine\n  Normal  Created    48s   kubelet, node1     Created container test-mysql\n  Normal  Started    48s   kubelet, node1     Started container test-mysql\n\n# 动态查看pod\n[root@master ~]# kubectl get pods pod-initcontainer -n dev -w\nNAME                             READY   STATUS     RESTARTS   AGE\npod-initcontainer                0/1     Init:0/2   0          15s\npod-initcontainer                0/1     Init:1/2   0          52s\npod-initcontainer                0/1     Init:1/2   0          53s\npod-initcontainer                0/1     PodInitializing   0          89s\npod-initcontainer                1/1     Running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化\n[root@master ~]# ifconfig ens33:1 192.168.109.201 netmask 255.255.255.0 up\n[root@master ~]# ifconfig ens33:2 192.168.109.202 netmask 255.255.255.0 up\n~~~\n\n### 钩子函数\n\n钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\n\nkubernetes在主容器的启动之后和停止之前提供了两个钩子函数：\n\n- post start：容器创建之后执行，如果失败了会重启容器\n- pre stop  ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作\n\n钩子处理器支持使用下面三种方式定义动作：\n\n- Exec命令：在容器内执行一次命令\n\n  ~~~yaml\n  ……\n    lifecycle:\n      postStart: \n        exec:\n          command:\n          - cat\n          - /tmp/healthy\n  ……\n  ~~~\n\n- TCPSocket：在当前容器尝试访问指定的socket\n\n  ~~~yaml\n  ……      \n    lifecycle:\n      postStart:\n        tcpSocket:\n          port: 8080\n  ……\n  ~~~\n\n- HTTPGet：在当前容器中向某url发起http请求\n\n  ~~~yaml\n  ……\n    lifecycle:\n      postStart:\n        httpGet:\n          path: / #URI地址\n          port: 80 #端口号\n          host: 192.168.109.100 #主机地址\n          scheme: HTTP #支持的协议，http或者https\n  ……\n  ~~~\n\n接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    lifecycle:\n      postStart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: [\"/bin/sh\", \"-c\", \"echo postStart... > /usr/share/nginx/html/index.html\"]\n      preStop:\n        exec: # 在容器停止之前停止nginx服务\n          command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@master ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nNAME           READY   STATUS     RESTARTS   AGE    IP            NODE    \npod-hook-exec  1/1     Running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@master ~]# curl 10.244.2.48\npostStart...\n~~~\n\n### 容器探测\n\n​    容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例\" 摘除 \"，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：\n\n- liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器\n\n- readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量\n\n> livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。\n\n上面两种探针目前均支持三种探测方式：\n\n- Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常\n\n  ~~~yaml\n  ……\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n  ……\n  ~~~\n\n- TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n\n  ~~~yaml\n  ……      \n    livenessProbe:\n      tcpSocket:\n        port: 8080\n  ……\n  ~~~\n\n- HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常\n\n  ~~~yaml\n  ……\n    livenessProbe:\n      httpGet:\n        path: / #URI地址\n        port: 80 #端口号\n        host: 127.0.0.1 #主机地址\n        scheme: HTTP #支持的协议，http或者https\n  ……\n  ~~~\n\n下面以liveness probes为例，做几个演示：\n\n**方式一：Exec**\n\n创建pod-liveness-exec.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      exec:\n        command: [\"/bin/cat\",\"/tmp/hello.txt\"] # 执行一个查看文件的命令\n~~~\n\n 创建pod，观察效果\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看Pod详情\n[root@master ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  Normal   Created    20s (x2 over 50s)  kubelet, node1     Created container nginx\n  Normal   Started    20s (x2 over 50s)  kubelet, node1     Started container nginx\n  Normal   Killing    20s                kubelet, node1     Container nginx failed liveness probe, will be restarted\n  Warning  Unhealthy  0s (x5 over 40s)   kubelet, node1     Liveness probe failed: cat: can't open '/tmp/hello11.txt': No such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@master ~]# kubectl get pods pod-liveness-exec -n dev\nNAME                READY   STATUS             RESTARTS   AGE\npod-liveness-exec   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n~~~\n\n**方式二：TCPSocket**\n\n创建pod-liveness-tcpsocket.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      tcpSocket:\n        port: 8080 # 尝试访问8080端口\n~~~\n\n 创建pod，观察效果\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看Pod详情\n[root@master ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  Normal   Scheduled  31s                            default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2\n  Normal   Pulled     <invalid>                      kubelet, node2     Container image \"nginx:1.17.1\" already present on machine\n  Normal   Created    <invalid>                      kubelet, node2     Created container nginx\n  Normal   Started    <invalid>                      kubelet, node2     Started container nginx\n  Warning  Unhealthy  <invalid> (x2 over <invalid>)  kubelet, node2     Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@master ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nNAME                     READY   STATUS             RESTARTS   AGE\npod-liveness-tcpsocket   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n~~~\n\n**方式三：HTTPGet**\n\n创建pod-liveness-httpget.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: HTTP #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #URI地址\n~~~\n\n 创建pod，观察效果\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看Pod详情\n[root@master ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  Normal   Pulled     6s (x3 over 64s)  kubelet, node1     Container image \"nginx:1.17.1\" already present on machine\n  Normal   Created    6s (x3 over 64s)  kubelet, node1     Created container nginx\n  Normal   Started    6s (x3 over 63s)  kubelet, node1     Started container nginx\n  Warning  Unhealthy  6s (x6 over 56s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    6s (x2 over 36s)  kubelet, node1     Container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@master ~]# kubectl get pod pod-liveness-httpget -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-liveness-httpget   1/1     Running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n~~~\n\n​    至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n\n~~~powershell\n[root@master ~]# kubectl explain pod.spec.containers.livenessProbe\nFIELDS:\n   exec <Object>  \n   tcpSocket    <Object>\n   httpGet      <Object>\n   initialDelaySeconds  <integer>  # 容器启动后等待多少秒执行第一次探测\n   timeoutSeconds       <integer>  # 探测超时时间。默认1秒，最小1秒\n   periodSeconds        <integer>  # 执行探测的频率。默认是10秒，最小1秒\n   failureThreshold     <integer>  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successThreshold     <integer>  # 连续探测成功多少次才被认定为成功。默认是1\n~~~\n\n下面稍微配置两个，演示下效果即可：\n\n~~~yaml\n[root@master ~]# more pod-liveness-httpget.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80 \n        path: /\n      initialDelaySeconds: 30 # 容器启动后30s开始探测\n      timeoutSeconds: 5 # 探测超时时间为5s\n~~~\n\n### 重启策略\n\n​    在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下：\n\n- Always ：容器失效时，自动重启该容器，这也是默认值。\n- OnFailure ： 容器终止运行且退出码不为0时重启\n- Never ： 不论状态为何，都不重启该容器\n\n​    重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。\n\n创建pod-restartpolicy.yaml：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80\n        path: /hello\n  restartPolicy: Never # 设置重启策略为Never\n~~~\n\n运行Pod测试\n\n~~~powershell\n# 创建Pod\n[root@master ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看Pod详情，发现nginx容器失败\n[root@master ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  Warning  Unhealthy  15s (x3 over 35s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    15s                kubelet, node1     Container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@master ~]# kubectl  get pods pod-restartpolicy -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-restartpolicy      0/1     Running   0          5min42s\n~~~\n\n## Pod调度\n\n​    在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式：\n\n- 自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出\n- 定向调度：NodeName、NodeSelector\n- 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity\n- 污点（容忍）调度：Taints、Toleration\n\n### 定向调度\n\n​    定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。\n\n**NodeName**\n\n​    NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。\n\n接下来，实验一下：创建一个pod-nodename.yaml文件\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 指定调度到node1节点上\n~~~\n\n~~~powershell\n#创建Pod\n[root@master ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@master ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE      ......\npod-nodename   1/1     Running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）\n[root@master ~]# kubectl delete -f pod-nodename.yaml\npod \"pod-nodename\" deleted\n[root@master ~]# vim pod-nodename.yaml\n[root@master ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@master ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP       NODE    ......\npod-nodename   0/1     Pending   0          6s    <none>   node3   ......           \n~~~\n\n**NodeSelector**\n\n​    NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。\n\n接下来，实验一下：\n\n1 首先分别为node节点添加标签\n\n~~~powershell\n[root@master ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@master ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n~~~\n\n2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeSelector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n~~~\n\n~~~powershell\n#创建Pod\n[root@master ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@master ~]# kubectl get pods pod-nodeselector -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP          NODE    ......\npod-nodeselector   1/1     Running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@master ~]# kubectl delete -f pod-nodeselector.yaml\npod \"pod-nodeselector\" deleted\n[root@master ~]# vim pod-nodeselector.yaml\n[root@master ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,Node的值为none\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP       NODE    \npod-nodeselector   0/1     Pending   0          2m20s   <none>   <none>\n\n# 查看详情,发现node selector匹配失败的提示\n[root@master ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn't match node selector.\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn't match node selector.\n~~~\n\n### 亲和性调度\n\n​    上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。\n\n​    基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\n\nAffinity主要分为三类：\n\n- nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题\n\n- podAffinity(pod亲和性) :  以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题\n\n- podAntiAffinity(pod反亲和性) :  以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题\n\n> 关于亲和性(反亲和性)使用场景的说明：\n>\n> **亲和性**：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n>\n> **反亲和性**：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。\n\n**NodeAffinity**\n\n首先来看一下`NodeAffinity`的可配置项：\n\n~~~markdown\npod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt\n\tweight 倾向权重，在范围1-100。\n~~~\n\n~~~markdown\n关系符的使用说明:\n\n- matchExpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: Exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是\"xxx\"或\"yyy\"的节点\n    operator: In\n    values: [\"xxx\",\"yyy\"]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于\"xxx\"的节点\n    operator: Gt\n    values: \"xxx\"\n~~~\n\n接下来首先演示一下`requiredDuringSchedulingIgnoredDuringExecution` ,\n\n创建pod-nodeaffinity-required.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        nodeSelectorTerms:\n        - matchExpressions: # 匹配env的值在[\"xxx\",\"yyy\"]中的标签\n          - key: nodeenv\n            operator: In\n            values: [\"xxx\",\"yyy\"]\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@master ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP       NODE    ...... \npod-nodeaffinity-required   0/1     Pending   0          16s   <none>   <none>  ......\n\n# 查看Pod的详情\n# 发现调度失败，提示node选择失败\n[root@master ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn't match node selector.\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 3 node(s) didn't match node selector.\n\n#接下来，停止pod\n[root@master ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod \"pod-nodeaffinity-required\" deleted\n\n# 修改文件，将values: [\"xxx\",\"yyy\"]------> [\"pro\",\"yyy\"]\n[root@master ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@master ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@master ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE  ...... \npod-nodeaffinity-required   1/1     Running   0          11s   10.244.1.89   node1 ......\n~~~\n\n接下来再演示一下`requiredDuringSchedulingIgnoredDuringExecution` ,\n\n创建pod-nodeaffinity-preferred.yaml\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      preferredDuringSchedulingIgnoredDuringExecution: # 软限制\n      - weight: 1\n        preference:\n          matchExpressions: # 匹配env的值在[\"xxx\",\"yyy\"]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: In\n            values: [\"xxx\",\"yyy\"]\n~~~\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@master ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nNAME                         READY   STATUS    RESTARTS   AGE\npod-nodeaffinity-preferred   1/1     Running   0          40s\n~~~\n\n~~~markdown\nNodeAffinity规则设置的注意事项：\n    1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上\n    2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可\n    3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功\n    4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化(意思是亲和性只有在调度的时候才会生效，当调度已完成，其他match标签发生变化，pod也不会再根据新的标签进行移动)\n~~~\n\n**PodAffinity**\n\nPodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。\n\n首先来看一下`PodAffinity`的可配置项：\n\n~~~markdown\npod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologyKey      指定调度作用域\n    labelSelector    标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容\n  preferredDuringSchedulingIgnoredDuringExecution 软限制\n    podAffinityTerm  选项\n      namespaces      \n      topologyKey\n      labelSelector\n        matchExpressions  \n          key    键\n          values 值\n          operator\n        matchLabels \n    weight 倾向权重，在范围1-100\n~~~\n\n~~~markdown\ntopologyKey用于指定调度时作用域,例如:\n    如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围, 即调度到同一node节点\n    如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分\n~~~\n\n接下来，演示下`requiredDuringSchedulingIgnoredDuringExecution`,\n\n1）首先创建一个参照Pod，pod-podaffinity-target.yaml：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 将目标pod名确指定到node1上\n~~~\n\n~~~powershell\n# 启动目标pod\n[root@master ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@master ~]# kubectl get pods  pod-podaffinity-target -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\npod-podaffinity-target   1/1     Running   0          4s\n~~~\n\n2）创建pod-podaffinity-required.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配env的值在[\"xxx\",\"yyy\"]中的标签\n          - key: podenv\n            operator: In\n            values: [\"xxx\",\"yyy\"]\n        topologyKey: kubernetes.io/hostname\n~~~\n\n上面配置表达的意思是：新Pod必须要与拥有标签podenv=xxx或者podenv=yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。\n\n~~~powershell\n# 启动pod\n[root@master ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@master ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod-podaffinity-required   0/1     Pending   0          9s\n\n# 查看详细信息\n[root@master ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  <unknown>  default-scheduler  0/3 nodes are available: 2 node(s) didn't match pod affinity rules, 1 node(s) had taints that the pod didn't tolerate.\n\n# 接下来修改  values: [\"xxx\",\"yyy\"]----->values:[\"pro\",\"yyy\"]\n# 意思是：新Pod必须要与拥有标签podenv=xxx或者podenv=yyy的pod在同一Node上\n[root@master ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@master ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod \"pod-podaffinity-required\" deleted\n[root@master ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时Pod运行正常\n[root@master ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE   LABELS\npod-podaffinity-required   1/1     Running   0          6s    <none>\n~~~\n\n关于`PodAffinity`的 `preferredDuringSchedulingIgnoredDuringExecution`，这里不再演示。\n\n**PodAntiAffinity**\n\nPodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。\n\n它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。\n\n1）继续使用上个案例中目标pod\n\n~~~powershell\n[root@master ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                     READY   STATUS    RESTARTS   AGE     IP            NODE    LABELS\npod-podaffinity-required 1/1     Running   0          3m29s   10.244.1.38   node1   <none>     \npod-podaffinity-target   1/1     Running   0          9m25s   10.244.1.37   node1   podenv=pro\n~~~\n\n2）创建pod-podantiaffinity-required.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAntiAffinity: #设置pod亲和性(反亲和性)\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配podenv的值在[\"pro\"]中的标签\n          - key: podenv\n            operator: In\n            values: [\"pro\"]\n        topologyKey: kubernetes.io/hostname\n~~~\n\n上面配置表达的意思是：新Pod必须要与拥有标签podenv=pro的pod不在同一Node上，运行测试一下。\n\n~~~powershell\n# 创建pod\n[root@master ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@master ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE   .. \npod-podantiaffinity-required   1/1     Running   0          30s   10.244.1.96   node2  ..\n~~~\n\n### 污点和容忍\n\n**污点（Taints）**\n\n​    前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加**污点**属性，来决定是否允许Pod调度过来。\n\n​    Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。\n\n污点的格式为：`key=value:effect`, key和value是污点的标签，effect描述污点的作用，支持如下三个选项：\n\n- PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度\n- NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod\n- NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离\n\n<img src=\"k8s_day3/image-20200605021831545.png\" alt=\"image-20200605021606508\" style=\"border:1px solid\" />\n\n使用kubectl设置和去除污点的命令示例如下：\n\n~~~powershell\n# 设置污点\nkubectl taint nodes node1 key=value:effect\n\n# 去除污点\nkubectl taint nodes node1 key:effect-\n\n# 去除所有污点\nkubectl taint nodes node1 key-\n~~~\n\n接下来，演示下污点的效果：\n\n1. 准备节点node1（为了演示效果更加明显，暂时停止node2节点）\n2. 为node1节点设置一个污点: `tag=heima:PreferNoSchedule`；然后创建pod1( pod1 可以 )\n3. 修改为node1节点设置一个污点: `tag=heima:NoSchedule`；然后创建pod2( pod1 正常  pod2 失败 )\n4. 修改为node1节点设置一个污点: `tag=heima:NoExecute`；然后创建pod3 ( 3个pod都失败 )\n\n~~~powershell\n# 为node1设置污点(PreferNoSchedule)\n[root@master ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule\n\n# 创建pod1\n[root@master ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE   \ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)\n# tag:PreferNoSchedule-  “—”代表要取消污点\n[root@master ~]# kubectl taint nodes node1 tag:PreferNoSchedule-\nnode/node1 untainted\n[root@master ~]# kubectl taint nodes node1 tag=heima:NoSchedule\nnode/node1 tainted\n\n# 创建pod2\n[root@master ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@master ~]# kubectl get pods taint2 -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE\ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     Pending   0          21s     <none>        <none>   \n\n# 为node1设置污点(取消NoSchedule，设置NoExecute)\n[root@master ~]# kubectl taint nodes node1 tag:NoSchedule-\n[root@master ~]# kubectl taint nodes node1 tag=heima:NoExecute\n\n# 创建pod3\n[root@master ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \ntaint1-7665f7fd85-htkmp   0/1     Pending   0          35s   <none>   <none>   <none>    \ntaint2-544694789-bn7wb    0/1     Pending   0          35s   <none>   <none>   <none>     \ntaint3-6d78dbd749-tktkq   0/1     Pending   0          6s    <none>   <none>   <none>     \n~~~\n\n~~~markdown\n小提示：\n    使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上.\n~~~\n\n**容忍（Toleration）**\n\n​    上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到**容忍**。\n\n![image-20200514095913741](image-20200514095913741.png)\n\n> 污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝\n>\n\n下面先通过一个案例看下效果：\n\n1. 上一小节，已经在node1节点上打上了`NoExecute`的污点，此时pod是调度不上去的\n2. 本小节，可以通过给pod添加容忍，然后将其调度上去\n\n创建pod-toleration.yaml,内容如下 \n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: \"tag\"        # 要容忍的污点的key\n    operator: \"Equal\" # 操作符\n    value: \"heima\"    # 容忍的污点的value\n    effect: \"NoExecute\"   # 添加容忍的规则，这里必须和标记的污点规则相同\n~~~\n\n~~~powershell\n# 添加容忍之前的pod\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \npod-toleration   0/1     Pending   0          3s    <none>   <none>   <none>           \n\n# 添加容忍之后的pod\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED\npod-toleration   1/1     Running   0          3s    10.244.1.62   node1   <none>        \n~~~\n\n下面看一下容忍的详细配置:\n\n~~~powershell\n[root@master ~]# kubectl explain pod.spec.tolerations\n......\nFIELDS:\n   key       # 对应着要容忍的污点的键，空意味着匹配所有的键\n   value     # 对应着要容忍的污点的值\n   operator  # key-value的运算符，支持Equal和Exists（默认）\n   effect    # 对应污点的effect，空意味着匹配所有影响\n   tolerationSeconds   # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间\n~~~\n\n","tags":["k8s 教程"],"categories":["k8s"]},{"title":"k8s教程day2-namespace、pod、label、development、service","url":"/2021/12/01/K8S/day2/k8s_day2/","content":"k8s 课程规划\n![](https://user-images.githubusercontent.com/28568478/144197771-e2ed53bf-bb06-46a8-af7b-8ac948fc2cf1.png)\n\n<!--more-->\n\n# 第四章 实战入门\n\n本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。\n\n## Namespace\n\n​    Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现**多套环境的资源隔离**或者**多租户的资源隔离**。\n\n​    默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的\"组\"，以方便不同的组的资源进行隔离使用和管理。\n\n​    可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。\n\n<img src=\"k8s_day2/image-20200407100850484.png\" alt=\"image-20200407100850484\" style=\"zoom:80%;border:1px solid\" />\n\nkubernetes在集群启动之后，会默认创建几个namespace\n\n~~~powershell\n[root@master ~]# kubectl  get namespace\nNAME              STATUS   AGE\ndefault           Active   45h     #  所有未指定Namespace的对象都会被分配在default命名空间\nkube-node-lease   Active   45h     #  集群节点之间的心跳维护，v1.13开始引入\nkube-public       Active   45h     #  此命名空间下的资源可以被所有人访问（包括未认证用户）\nkube-system       Active   45h     #  所有由Kubernetes系统创建的资源都处于这个命名空间\n~~~\n\n下面来看namespace资源的具体操作：\n\n**查看**\n\n~~~powershell\n# 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   45h\nkube-node-lease   Active   45h\nkube-public       Active   45h     \nkube-system       Active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nNAME      STATUS   AGE\ndefault   Active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: \"2020-04-05T04:44:16Z\"\n  name: default\n  resourceVersion: \"151\"\n  selfLink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nName:         default\nLabels:       <none>\nAnnotations:  <none>\nStatus:       Active  # Active 命名空间正在使用中  Terminating 正在删除命名空间\n\n# ResourceQuota 针对namespace做的资源限制\n# LimitRange针对namespace中的每个组件做的资源限制\nNo resource quota.\nNo LimitRange resource.\n~~~\n\n**创建**\n\n~~~powershell\n# 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n~~~\n\n**删除**\n\n~~~powershell\n# 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace \"dev\" deleted\n~~~\n\n**配置方式**\n\n首先准备一个yaml文件：ns-dev.yaml\n\n~~~yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n~~~\n\n然后就可以执行对应的创建和删除命令了：\n\n​    创建：kubectl  create  -f  ns-dev.yaml\n\n​    删除：kubectl  delete  -f  ns-dev.yaml\n\n## Pod\n\nPod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。\n\nPod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。\n\n<img src=\"k8s_day2/image-20200407121501907.png\" alt=\"image-20200407121501907\" style=\"zoom:80%;\" />\n\nkubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看：\n\n~~~powershell\n[root@master ~]# kubectl get pod -n kube-system\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6955765f44-68g6v         1/1     Running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     Running   0          2d1h\nkube-system   etcd-master                      1/1     Running   0          2d1h\nkube-system   kube-apiserver-master            1/1     Running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     Running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     Running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     Running   0          2d1h\nkube-system   kube-scheduler-master            1/1     Running   0          2d1h\n~~~\n\n**创建并运行**\n\nkubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的\n\n~~~powershell\n# 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定Pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:1.17.1 --port=80 --namespace dev \ndeployment.apps/nginx created\n~~~\n\n**查看pod信息**\n\n~~~powershell\n# 查看Pod基本信息\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-fg2db   1/1     Running   0          43s\n\n# 查看Pod的详细信息\n[root@master ~]# kubectl describe pod nginx-5ff7956ff6-fg2db -n dev\nName:         nginx-5ff7956ff6-fg2db\nNamespace:    dev\nPriority:     0\nNode:         node1/192.168.109.101\nStart Time:   Wed, 08 Apr 2020 09:29:24 +0800\nLabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nAnnotations:  <none>\nStatus:       Running\nIP:           10.244.1.23\nIPs:\n  IP:           10.244.1.23\nControlled By:  ReplicaSet/nginx-5ff7956ff6\nContainers:\n  nginx:\n    Container ID:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    Image:          nginx:1.17.1\n    Image ID:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 08 Apr 2020 09:30:01 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-hwvvw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hwvvw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  Normal  Pulling    4m11s      kubelet, node1     Pulling image \"nginx:1.17.1\"\n  Normal  Pulled     3m36s      kubelet, node1     Successfully pulled image \"nginx:1.17.1\"\n  Normal  Created    3m36s      kubelet, node1     Created container nginx\n  Normal  Started    3m36s      kubelet, node1     Started container nginx\n~~~\n\n**访问Pod**\n\n~~~powershell\n# 获取podIP\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME                     READY   STATUS    RESTARTS   AGE    IP             NODE    ... \nnginx-5ff7956ff6-fg2db   1/1     Running   0          190s   10.244.1.23   node1   ...\n\n#访问POD\n[root@master ~]# curl http://10.244.1.23:80\n<!DOCTYPE html>\n<html>\n<head>\n\t<title>Welcome to nginx!</title>\n</head>\n<body>\n\t<p><em>Thank you for using nginx.</em></p>\n</body>\n</html>\n~~~\n\n**删除指定Pod**\n\n~~~powershell\n# 删除指定Pod\n[root@master ~]# kubectl delete pod nginx-5ff7956ff6-fg2db -n dev\npod \"nginx-5ff7956ff6-fg2db\" deleted\n\n# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-jj4ng   1/1     Running   0          21s\n\n# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建\n# 此时要想删除Pod，必须删除Pod控制器\n\n# 先来查询一下当前namespace下的Pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此PodPod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps \"nginx\" deleted\n\n# 稍等片刻，再查询Pod，发现Pod被删除了\n[root@master ~]# kubectl get pods -n dev\nNo resources found in dev namespace.\n~~~\n\n**配置操作**\n\n创建一个pod-nginx.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:1.17.1\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n~~~\n\n然后就可以执行对应的创建和删除命令了：\n\n​    创建：kubectl  create  -f  pod-nginx.yaml\n\n​    删除：kubectl  delete  -f  pod-nginx.yaml\n\n## Label\n\nLabel是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\n\nLabel的特点：\n\n- 一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等\n- 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去\n- Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除\n\n可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n\n> 一些常用的Label 示例如下：\n>\n> - 版本标签：\"version\":\"release\", \"version\":\"stable\"......\n> - 环境标签：\"environment\":\"dev\"，\"environment\":\"test\"，\"environment\":\"pro\"\n> - 架构标签：\"tier\":\"frontend\"，\"tier\":\"backend\"\n\n标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即：\n\n​    Label用于给某个资源对象定义标识\n\n​    Label Selector用于查询和筛选拥有某些标签的资源对象\n\n当前有两种Label Selector：\n\n- 基于等式的Label Selector\n\n  name = slave: 选择所有包含Label中key=\"name\"且value=\"slave\"的对象\n\n  env != production: 选择所有包括Label中的key=\"env\"且value不等于\"production\"的对象\n\n- 基于集合的Label Selector\n\n  name in (master, slave): 选择所有包含Label中的key=\"name\"且value=\"master\"或\"slave\"的对象\n\n  name not in (frontend): 选择所有包含Label中的key=\"name\"且value不等于\"frontend\"的对象\n\n标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号\",\"进行分隔即可。例如：\n\n​\t\tname=slave，env!=production\n\n​\t\tname not in (frontend)，env!=production\n\n**命令方式**\n\n~~~powershell\n# 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nNo resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n~~~\n\n**配置方式**\n\n~~~yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\n  labels:\n    version: \"3.0\" \n    env: \"test\"\nspec:\n  containers:\n  - image: nginx:1.17.1\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n~~~\n\n然后就可以执行对应的更新命令了：kubectl  apply  -f  pod-nginx.yaml\n\n## Deployment\n\n​    在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。\n\n​     在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。\n\n<img src=\"k8s_day2/image-20200408193950807.png\" alt=\"image-20200408193950807\" style=\"border: 1px solid; zoom: 80%;\" />\n\n**命令操作**\n\n~~~powershell\n# 命令格式: kubectl run deployment名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:1.17.1 --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的Pod\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-6k8cb   1/1     Running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     Running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     Running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           2m42s\n\n# UP-TO-DATE：成功升级的副本数量\n# AVAILABLE：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME    READY UP-TO-DATE  AVAILABLE   AGE     CONTAINERS   IMAGES              SELECTOR\nnginx   3/3     3         3           2m51s   nginx        nginx:1.17.1        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nName:                   nginx\nNamespace:              dev\nCreationTimestamp:      Wed, 08 Apr 2020 11:14:14 +0800\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        nginx:1.17.1\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  <none>\nNewReplicaSet:   nginx-5ff7956ff6 (3/3 replicas created)\nEvents:\n  Type    Reason             Age    From                   Message\n  ----    ------             ----   ----                   -------\n  Normal  ScalingReplicaSet  5m43s  deployment-controller  Scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除 \n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps \"nginx\" deleted\n~~~\n\n**配置操作**\n\n创建一个deploy-nginx.yaml，内容如下：\n\n~~~yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:1.17.1\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n~~~\n\n然后就可以执行对应的创建和删除命令了：\n\n​    创建：kubectl  create  -f  deploy-nginx.yaml\n\n​    删除：kubectl  delete  -f  deploy-nginx.yaml\n\n##  Service\n\n通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。\n\n虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：\n\n- Pod IP 会随着Pod的重建产生变化\n- Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问\n\n这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。\n\nService可以看作是一组同类Pod**对外的访问接口**。借助Service，应用可以方便地实现服务发现和负载均衡。\n\n![image-20200408194716912](image-20200408194716912.png)\n\n**操作一：创建集群内部可访问的Service**\n\n~~~powershell\n# 暴露Service\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx -n dev -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR\nsvc-nginx1   ClusterIP   10.109.179.231   <none>        80/TCP    3m51s   run=nginx\n\n# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的\n# 可以通过这个IP访问当前service对应的POD\n[root@master ~]# curl 10.109.179.231:80\n<!DOCTYPE html>\n<html>\n<head>\n<title>Welcome to nginx!</title>\n</head>\n<body>\n<h1>Welcome to nginx!</h1>\n.......\n</body>\n</html>\n~~~\n\n**操作二：创建集群外部也可访问的Service**\n\n~~~powershell\n# 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问\n# 如果需要创建外部也可以访问的Service，需要修改type为NodePort\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）\n[root@master ~]# kubectl get svc  svc-nginx-1  -n dev -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nsvc-nginx2    NodePort    10.100.94.0      <none>        80:31928/TCP   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.109.100:31928/\n~~~\n\n**删除Service**\n\n~~~powershell\n[root@master ~]# kubectl delete svc svc-nginx-1 -n dev                                   service \"svc-nginx-1\" deleted\n~~~\n\n**配置方式**\n\n创建一个svc-nginx.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  clusterIP: 10.109.179.231\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx\n  type: ClusterIP\n~~~\n\n然后就可以执行对应的创建和删除命令了：\n\n​    创建：kubectl  create  -f  svc-nginx.yaml\n\n​    删除：kubectl  delete  -f  svc-nginx.yaml\n\n> **小结**\n>\n> ​    至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。\n\n\n","tags":["k8s 教程"],"categories":["k8s"]},{"title":"k8s教程day1-k8s介绍及环境搭建和YAML语言介绍","url":"/2021/12/01/K8S/day1/k8s_day1/","content":"k8s 课程规划\n![](https://user-images.githubusercontent.com/28568478/144197771-e2ed53bf-bb06-46a8-af7b-8ac948fc2cf1.png)\n\n<!--more-->\n\n# 第一章 kubernetes介绍\n\n本章节主要介绍应用程序在服务器上部署方式演变以及kubernetes的概念、组件和工作原理。\n\n## 应用部署方式演变\n\n在部署应用程序的方式上，主要经历了三个时代：\n\n- **传统部署**：互联网早期，会直接将应用程序部署在物理机上\n\n  > 优点：简单，不需要其它技术的参与\n  >\n  > 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n  \n- **虚拟化部署**：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n\n  > 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n  >\n  > 缺点：增加了操作系统，浪费了部分资源\n\n- **容器化部署**：与虚拟化类似，但是共享了操作系统\n\n  > 优点：\n  >\n  > ​    可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n  >\n  > ​    运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n  >\n  > ​    容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署\n\n![image-20200505183738289](image-20200505183738289.png)\n\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n\n- 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器\n- 当并发访问量变大的时候，怎么样做到横向扩展容器数量\n\n这些容器管理的问题统称为**容器编排**问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\n\n- **Swarm**：Docker自己的容器编排工具\n- **Mesos**：Apache的一个资源统一管控的工具，需要和Marathon结合使用\n- **Kubernetes**：Google开源的的容器编排工具\n\n<img src=\"image-20200524150339551.png\" alt=\"image-20200524150339551\" style=\"border:1px solid;zoom:110%;\" />\n\n\n## kubernetes简介\n\n<img src=\"k8s_day1/image-20200406232838722.png\" alt=\"image-20200406232838722\" style=\"zoom:100%;border:1px solid;\" />\n\n​    \n\n​    kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器----Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。\n\n​    kubernetes的本质是**一组服务器集群**，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n\n- **自我修复**：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器\n- **弹性伸缩**：可以根据需要，自动对集群中正在运行的容器数量进行调整\n- **服务发现**：服务可以通过自动发现的形式找到它所依赖的服务\n- **负载均衡**：如果一个服务起动了多个容器，能够自动实现请求的负载均衡\n- **版本回退**：如果发现新发布的程序版本有问题，可以立即回退到原来的版本\n- **存储编排**：可以根据容器自身的需求自动创建存储卷\n\n![image-20200526203726071](image-20200526203726071.png)\n\n## kubernetes组件\n\n一个kubernetes集群主要是由**控制节点(master)**、**工作节点(node)**构成，每个节点上都会安装不同的组件。\n\n**master：集群的控制平面，负责集群的决策  (  管理  )**\n\n> **ApiServer** : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制\n>\n> **Scheduler** : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上\n>\n> **ControllerManager** : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n>\n> **Etcd **：负责存储集群中各种资源对象的信息\n\n**node：集群的数据平面，负责为容器提供运行环境 ( 干活 ) **\n\n> **Kubelet** : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器\n>\n> **KubeProxy** : 负责提供集群内部的服务发现和负载均衡\n>\n> **Docker** : 负责节点上容器的各种操作\n\n<img src=\"k8s_day1/image-20200406184656917.png\" alt=\"image-20200406184656917\" style=\"zoom:200%;\" />\n\n下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：\n\n1. 首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中\n\n2. 一个nginx服务的安装请求会首先被发送到master节点的apiServer组件\n\n3. apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上\n\n   在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer\n\n4. apiServer调用controller-manager去调度Node节点安装nginx服务\n\n5. kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod\n\n   pod是kubernetes的最小操作单元，容器必须跑在pod中至此，\n\n6. 一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理\n\n​        这样，外界用户就可以访问集群中的nginx服务了\n\n## kubernetes概念\n\n**Master**：集群控制节点，每个集群需要至少一个master节点负责集群的管控\n\n**Node**：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行\n\n**Pod**：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器\n\n**Controller**：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等\n\n**Service**：pod对外服务的统一入口，下面可以维护者同一类的多个pod\n\n**Label**：标签，用于对pod进行分类，同一类pod会拥有相同的标签\n\n**NameSpace**：命名空间，用来隔离pod的运行环境\n\n<img src=\"k8s_day1/image-20200403224313355.png\" alt=\"image-20200403224313355\" style=\"zoom:200%;\" />\n\n\n\n# 第二章 集群环境搭建\n\n本章节主要介绍如何搭建kubernetes的集群环境\n\n## 环境规划\n\n### 集群类型\n\nkubernetes集群大体上分为两类：**一主多从**和**多主多从**。\n\n- 一主多从：一台Master节点和多台Node节点，搭建简单，但是有单机故障风险，适合用于测试环境\n- 多主多从：多台Master节点和多台Node节点，搭建麻烦，安全性高，适合用于生产环境\n\n![image-20200404094800622](image-20200404094800622.png)\n\n> `说明：为了测试简单，本次搭建的是  一主两从   类型的集群`\n\n### 安装方式\n\nkubernetes有多种部署方式，目前主流的方式有kubeadm、minikube、二进制包\n\n- minikube：一个用于快速搭建单节点kubernetes的工具\n- kubeadm：一个用于快速搭建kubernetes集群的工具\n- 二进制包 ：从官网下载每个组件的二进制包，依次去安装，此方式对于理解kubernetes组件更加有效\n\n> `说明：现在需要安装kubernetes的集群环境，但是又不想过于麻烦，所以选择使用kubeadm方式`\n\n### 主机规划\n\n| 作用   | IP地址          | 操作系统                    | 配置                     |\n| ------ | --------------- | --------------------------- | ------------------------ |\n| Master | 192.168.109.101 | Centos7.5    基础设施服务器 | 2颗CPU  2G内存   50G硬盘 |\n| Node1  | 192.168.109.102 | Centos7.5    基础设施服务器 | 2颗CPU  2G内存   50G硬盘 |\n| Node2  | 192.168.109.103 | Centos7.5    基础设施服务器 | 2颗CPU  2G内存   50G硬盘 |\n\n## 环境搭建\n\n​    本次环境搭建需要安装三台Centos服务器（一主二从），然后在每台服务器中分别安装docker（18.06.3），kubeadm（1.17.4）、kubelet（1.17.4）、kubectl（1.17.4）程序。\n\n### 主机安装\n\n安装虚拟机过程中注意下面选项的设置：\n\n- 操作系统环境：CPU（2C）    内存（2G）   硬盘（50G）    \n\n- 语言选择：中文简体\n\n- 软件选择：基础设施服务器\n\n- 分区选择：自动分区\n\n- 网络配置：按照下面配置网路地址信息\n\n  ~~~md\n  网络地址：192.168.109.100  （每台主机都不一样  分别为100、101、102）\n  子网掩码：255.255.255.0\n  默认网关：192.168.109.2\n  DNS：    223.5.5.5\n  ~~~\n\n  ![image-20200505213817934](image-20200505213817934.png)\n\n- 主机名设置：按照下面信息设置主机名\n\n  ~~~md\n  master节点： master\n  node节点：   node1\n  node节点：   node2\n  ~~~\n\n  ![image-20200505214156148](image-20200505214156148.png)\n\n### 环境初始化\n\n1)    检查操作系统的版本\n\n~~~powershell\n# 此方式下安装kubernetes集群要求Centos版本要在7.5或之上\n[root@master ~]# cat /etc/redhat-release\nCentOS Linux release 7.5.1804 (Core)\n~~~\n\n2） 主机名解析\n\n为了方便后面集群节点间的直接调用，在这配置一下主机名解析，企业中推荐使用内部DNS服务器\n\n```powershell\n# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容\n192.168.109.100  master\n192.168.109.101  node1\n192.168.109.102  node2\n```\n3） 时间同步\n\nkubernetes要求集群中的节点时间必须精确一致，这里直接使用chronyd服务从网络同步时间。\n\n企业中建议配置内部的时间同步服务器\n\n~~~powershell\n# 启动chronyd服务\n[root@master ~]# systemctl start chronyd\n# 设置chronyd服务开机自启\n[root@master ~]# systemctl enable chronyd\n# chronyd服务启动稍等几秒钟，就可以使用date命令验证时间了\n[root@master ~]# date\n~~~\n\n4） 禁用iptables和firewalld服务\n\nkubernetes和docker在运行中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则\n\n~~~powershell\n# 1 关闭firewalld服务\n[root@master ~]# systemctl stop firewalld\n[root@master ~]# systemctl disable firewalld\n# 2 关闭iptables服务\n[root@master ~]# systemctl stop iptables\n[root@master ~]# systemctl disable iptables\n~~~\n\n5） 禁用selinux\n\n selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题\n\n~~~powershell\n# 编辑 /etc/selinux/config 文件，修改SELINUX的值为disabled\n# 注意修改完毕之后需要重启linux服务\nSELINUX=disabled\n~~~\n\n6） 禁用swap分区\n\nswap分区指的是虚拟内存分区，它的作用是在物理内存使用完之后，将磁盘空间虚拟成内存来使用\n\n启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备\n\n但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明\n\n~~~powershell\n# 编辑分区配置文件/etc/fstab，注释掉swap分区一行\n# 注意修改完毕之后需要重启linux服务\n UUID=455cc753-7a60-4c17-a424-7741728c44a1 /boot    xfs     defaults        0 0\n /dev/mapper/centos-home /home                      xfs     defaults        0 0\n# /dev/mapper/centos-swap swap                      swap    defaults        0 0\n~~~\n\n7）修改linux的内核参数\n\n~~~powershell\n# 修改linux的内核参数，添加网桥过滤和地址转发功能\n# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置:\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n\n# 重新加载配置\n[root@master ~]# sysctl -p\n\n# 加载网桥过滤模块\n[root@master ~]# modprobe br_netfilter\n\n# 查看网桥过滤模块是否加载成功\n[root@master ~]# lsmod | grep br_netfilter\n~~~\n\n8）配置ipvs功能\n\n在kubernetes中service有两种代理模型，一种是基于iptables的，一种是基于ipvs的\n\n两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块\n\n~~~powershell\n# 1 安装ipset和ipvsadm\n[root@master ~]# yum install ipset ipvsadmin -y\n\n# 2 添加需要加载的模块写入脚本文件\n[root@master ~]# cat <<EOF >  /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n\n# 3 为脚本文件添加执行权限\n[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules\n\n# 4 执行脚本文件\n[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules\n\n# 5 查看对应的模块是否加载成功\n[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n~~~\n\n9） 重启服务器\n\n上面步骤完成之后，需要重新启动linux系统\n\n~~~powershell\n[root@master ~]# reboot\n~~~\n\n### 安装docker\n\n~~~powershell\n# 1 切换镜像源\n[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n# 2 查看当前镜像源中支持的docker版本\n[root@master ~]# yum list docker-ce --showduplicates\n\n# 3 安装特定版本的docker-ce\n# 必须指定--setopt=obsoletes=0，否则yum会自动安装更高版本\n[root@master ~]# yum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 -y\n\n# 4 添加一个配置文件\n# Docker在默认情况下使用的Cgroup Driver为cgroupfs，而kubernetes推荐使用systemd来代替cgroupfs\n[root@master ~]# mkdir /etc/docker\n[root@master ~]# cat <<EOF >  /etc/docker/daemon.json\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"registry-mirrors\": [\"https://kn0t2bca.mirror.aliyuncs.com\"]\n}\nEOF\n\n# 5 启动docker\n[root@master ~]# systemctl restart docker\n[root@master ~]# systemctl enable docker\n\n# 6 检查docker状态和版本\n[root@master ~]# docker version\n~~~\n\n### 安装kubernetes组件\n\n~~~powershell\n# 由于kubernetes的镜像源在国外，速度比较慢，这里切换成国内的镜像源\n# 编辑/etc/yum.repos.d/kubernetes.repo，添加下面的配置 \n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\n       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n# 安装kubeadm、kubelet和kubectl\n[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y\n\n# 配置kubelet的cgroup\n# 编辑/etc/sysconfig/kubelet，添加下面的配置\nKUBELET_CGROUP_ARGS=\"--cgroup-driver=systemd\"\nKUBE_PROXY_MODE=\"ipvs\"\n\n# 4 设置kubelet开机自启\n[root@master ~]# systemctl enable kubelet\n~~~\n\n### 准备集群镜像\n\n~~~powershell\n# 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看\n[root@master ~]# kubeadm config images list\n\n# 下载镜像\n# 此镜像在kubernetes的仓库中,由于网络原因,无法连接，下面提供了一种替代方案\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\nfor imageName in ${images[@]} ; do\n\tdocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n\tdocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName \t\tk8s.gcr.io/$imageName\n\tdocker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\ndone\n~~~\n\n### 集群初始化\n\n下面开始对集群进行初始化，并将node节点加入到集群中\n\n> 下面的操作只需要在`master`节点上执行即可\n>\n\n~~~powershell\n# 创建集群\n[root@master ~]# kubeadm init \\\n\t--kubernetes-version=v1.17.4 \\\n    --pod-network-cidr=10.244.0.0/16 \\\n    --service-cidr=10.96.0.0/12 \\\n    --apiserver-advertise-address=192.168.109.100\n\n# 创建必要文件\n[root@master ~]# mkdir -p $HOME/.kube\n[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config\n~~~\n\n> 下面的操作只需要在`node`节点上执行即可\n>\n\n~~~powershell\n# 将node节点加入集群\n[root@master ~]# kubeadm join 192.168.109.100:6443 \\ \n\t--token 8507uc.o0knircuri8etnw2 \\\n\t--discovery-token-ca-cert-hash \\\n\tsha256:acc37967fb5b0acf39d7598f8a439cc7dc88f439a3f4d0c9cae88e7901b9d3f\n\t\n# 查看集群状态 此时的集群状态为NotReady，这是因为还没有配置网络插件\n[root@master ~]# kubectl get nodes\nNAME     STATUS     ROLES    AGE     VERSION\nmaster   NotReady   master   6m43s   v1.17.4\nnode1    NotReady   <none>   22s     v1.17.4\nnode2    NotReady   <none>   19s     v1.17.4\n~~~\n\n### 安装网络插件\n\nkubernetes支持多种网络插件，比如flannel、calico、canal等等，任选一种使用即可，本次选择flannel\n\n> 下面操作依旧只在`master`节点执行即可，插件使用的是DaemonSet的控制器，它会在每个节点上都运行\n>\n\n~~~powershell\n# 获取fannel的配置文件\n[root@master ~]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\n# 修改文件中quay.io仓库为quay-mirror.qiniu.com\n\n# 使用配置文件启动fannel\n[root@master ~]# kubectl apply -f kube-flannel.yml\n\n# 稍等片刻，再次查看集群节点的状态\n[root@master ~]# kubectl get nodes\nNAME     STATUS   ROLES    AGE     VERSION\nmaster   Ready    master   15m     v1.17.4\nnode1    Ready    <none>   8m53s   v1.17.4\nnode2    Ready    <none>   8m50s   v1.17.4\n~~~\n\n至此，kubernetes的集群环境搭建完成\n\n## 服务部署\n\n接下来在kubernetes集群中部署一个nginx程序，测试下集群是否在正常工作。\n\n~~~powershell\n# 部署nginx\n[root@master ~]# kubectl create deployment nginx --image=nginx:1.14-alpine\n\n# 暴露端口\n[root@master ~]# kubectl expose deployment nginx --port=80 --type=NodePort\n\n# 查看服务状态\n[root@master ~]# kubectl get pods,service\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-86c57db685-fdc2k   1/1     Running   0          18m\n\nNAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        82m\nservice/nginx        NodePort    10.104.121.45   <none>        80:30073/TCP   17m\n\n# 4 最后在电脑上访问下部署的nginx服务\n~~~\n\n<img src=\"k8s_day1/image-20200405142656921.png\" alt=\"image-20200405142656921\" style=\"zoom:80%; border:1px solid\" />\n\n\n\n# 第三章 资源管理\n\n本章节主要介绍yaml语法和kubernetes的资源管理方式\n\n## 资源管理介绍\n\n在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。\n\n>​    kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。\n>\n>​    kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在`Pod`中，而kubernetes一般也不会直接管理Pod，而是通过`Pod控制器`来管理Pod的。\n>\n>​    Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了`Service`资源实现这个功能。\n>\n>​    当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种`存储`系统。\n\n<img src=\"k8s_day1/image-20200406225334627.png\" alt=\"image-20200406225334627\" style=\"zoom:200%;\" />\n\n>  学习kubernetes的核心，就是学习如何对集群上的`Pod、Pod控制器、Service、存储`等各种资源进行操作\n\n## YAML语言介绍\n\n​    YAML是一个类似 XML、JSON 的标记性语言。它强调以**数据**为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称\"一种人性化的数据格式语言\"。\n\n~~~xml\n<heima>\n\t<age>15</age>\n    <address>Beijing</address>\n</heima>\n~~~\n\n~~~yaml\nheima:\n  age: 15\n  address: Beijing\n~~~\n\nYAML的语法比较简单，主要有下面几个：\n\n- 大小写敏感\n- 使用缩进表示层级关系\n- 缩进不允许使用tab，只允许空格( 低版本限制 )\n- 缩进的空格数不重要，只要相同层级的元素左对齐即可\n- '#'表示注释\n\nYAML支持以下几种数据类型：\n\n- 纯量：单个的、不可再分的值\n- 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary）\n- 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list）\n\n~~~yaml\n# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期\n# 1 布尔类型\nc1: true (或者True)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用ISO 8601格式，即yyyy-MM-dd\n# 6 时间类型\nc6: 2018-02-17T15:02:31+08:00  # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n~~~\n\n~~~yaml\n# 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: Beijing\n# 形式二(了解):\nheima: {age: 15,address: Beijing}\n~~~\n\n~~~yaml\n# 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平\t\n# 形式二(了解):\naddress: [顺义,昌平]\n~~~\n\n> 小提示：\n>\n> ​\t1  书写yaml切记`:` 后面要加一个空格\n>\n> ​\t2  如果需要将多段yaml配置放在一个文件中，中间要使用`---`分隔\n>\n> ​    3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确\n>\n> ​       https://www.json2yaml.com/convert-yaml-to-json\n\n## 资源管理方式\n\n- 命令式对象管理：直接使用命令去操作kubernetes资源\n\n  `kubectl run nginx-pod --image=nginx:1.17.1 --port=80`\n\n- 命令式对象配置：通过命令配置和配置文件去操作kubernetes资源\n\n  `kubectl create/patch -f nginx-pod.yaml`\n\n- 声明式对象配置：通过apply命令和配置文件去操作kubernetes资源\n\n  `kubectl apply -f nginx-pod.yaml`\n\n| 类型           | 操作对象 | 适用环境 | 优点           | 缺点                             |\n| -------------- | -------- | -------- | -------------- | -------------------------------- |\n| 命令式对象管理 | 对象     | 测试     | 简单           | 只能操作活动对象，无法审计、跟踪 |\n| 命令式对象配置 | 文件     | 开发     | 可以审计、跟踪 | 项目大时，配置文件多，操作麻烦   |\n| 声明式对象配置 | 目录     | 开发     | 支持目录操作   | 意外情况下难以调试               |\n\n### 命令式对象管理\n\n**kubectl命令**\n\n​    kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下：\n\n~~~md\nkubectl [command] [type] [name] [flags]\n~~~\n\n**comand**：指定要对资源执行的操作，例如create、get、delete\n\n**type**：指定资源类型，比如deployment、pod、service\n\n**name**：指定资源的名称，名称大小写敏感\n\n**flags**：指定额外的可选参数\n\n~~~powershell\n# 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n~~~\n\n**资源类型**\n\nkubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看:\n\n~~~powershell\nkubectl api-resources\n~~~\n\n经常使用的资源有下面这些：\n\n<table>\n\t<tr>\n\t    <th>资源分类</th>\n\t    <th>资源名称</th>\n\t\t<th>缩写</th>\n\t\t<th>资源作用</th>\n\t</tr>\n\t<tr>\n\t    <td rowspan=\"2\">集群级别资源</td>\n        <td>nodes</td>\n\t    <td>no</td>\n\t\t<td>集群组成部分</td>\n\t</tr>\n\t<tr>\n\t\t<td>namespaces</td>\n\t    <td>ns</td>\n\t\t<td>隔离Pod</td>\n\t</tr>\n\t<tr>\n\t\t<td>pod资源</td>\n\t    <td>pods</td>\n\t    <td>po</td>\n\t\t<td>装载容器</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"8\">pod资源控制器</td>\n\t    <td>replicationcontrollers</td>\n\t    <td>rc</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\n\t<tr>\n\t    <td>replicasets</td>\n\t    <td>rs</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\n\t<tr>\n\t    <td>deployments</td>\n\t    <td>deploy</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\n\t<tr>\n\t    <td>daemonsets</td>\n\t    <td>ds</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\n\t<tr>\n\t    <td>jobs</td>\n\t    <td></td>\n\t\t<td>控制pod资源</td>\n\t</tr>\t\n\t<tr>\n\t    <td>cronjobs</td>\n\t    <td>cj</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\t\n\t<tr>\n\t    <td>horizontalpodautoscalers</td>\n\t    <td>hpa</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\t\n\t<tr>\n\t    <td>statefulsets</td>\n\t    <td>sts</td>\n\t\t<td>控制pod资源</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"2\">服务发现资源</td>\n\t    <td>services</td>\n\t    <td>svc</td>\n\t\t<td>统一pod对外接口</td>\n\t</tr>\n    <tr>\n\t    <td>ingress</td>\n\t    <td>ing</td>\n\t\t<td>统一pod对外接口</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"3\">存储资源</td>\n\t    <td>volumeattachments</td>\n\t    <td></td>\n\t\t<td>存储</td>\n\t</tr>\n\t<tr>\n\t    <td>persistentvolumes</td>\n\t    <td>pv</td>\n\t\t<td>存储</td>\n\t</tr>\n\t<tr>\n\t    <td>persistentvolumeclaims</td>\n\t    <td>pvc</td>\n\t\t<td>存储</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"2\">配置资源</td>\n\t    <td>configmaps</td>\n\t    <td>cm</td>\n\t\t<td>配置</td>\n\t</tr>\n\t<tr>\n\t    <td>secrets</td>\n\t    <td></td>\n\t\t<td>配置</td>\n\t</tr>\n</table>\n\n**操作**\n\nkubernetes允许对资源进行多种操作，可以通过--help查看详细的操作命令\n\n~~~powershell\nkubectl --help\n~~~\n\n经常使用的操作有下面这些：\n\n<table>\n\t<tr>\n\t    <th>命令分类</th>\n\t    <th>命令</th>\n\t\t<th>翻译</th>\n\t\t<th>命令作用</th>\n\t</tr>\n\t<tr>\n\t    <td rowspan=\"6\">基本命令</td>\n\t    <td>create</td>\n\t    <td>创建</td>\n\t\t<td>创建一个资源</td>\n\t</tr>\n\t<tr>\n\t\t<td>edit</td>\n\t    <td>编辑</td>\n\t\t<td>编辑一个资源</td>\n\t</tr>\n\t<tr>\n\t\t<td>get</td>\n\t    <td>获取</td>\n\t    <td>获取一个资源</td>\n\t</tr>\n   <tr>\n\t\t<td>patch</td>\n\t    <td>更新</td>\n\t    <td>更新一个资源</td>\n\t</tr>\n\t<tr>\n\t    <td>delete</td>\n\t    <td>删除</td>\n\t\t<td>删除一个资源</td>\n\t</tr>\n\t<tr>\n\t    <td>explain</td>\n\t    <td>解释</td>\n\t\t<td>展示资源文档</td>\n\t</tr>\n\t<tr>\n\t    <td rowspan=\"10\">运行和调试</td>\n\t    <td>run</td>\n\t    <td>运行</td>\n\t\t<td>在集群中运行一个指定的镜像</td>\n\t</tr>\n\t<tr>\n\t    <td>expose</td>\n\t    <td>暴露</td>\n\t\t<td>暴露资源为Service</td>\n\t</tr>\n\t<tr>\n\t    <td>describe</td>\n\t    <td>描述</td>\n\t\t<td>显示资源内部信息</td>\n\t</tr>\n\t<tr>\n\t    <td>logs</td>\n\t    <td>日志</td>\n\t\t<td>输出容器在 pod 中的日志</td>\n\t</tr>\t\n\t<tr>\n\t    <td>attach</td>\n\t    <td>缠绕</td>\n\t\t<td>进入运行中的容器</td>\n\t</tr>\t\n\t<tr>\n\t    <td>exec</td>\n\t    <td>执行</td>\n\t\t<td>执行容器中的一个命令</td>\n\t</tr>\t\n\t<tr>\n\t    <td>cp</td>\n\t    <td>复制</td>\n\t\t<td>在Pod内外复制文件</td>\n\t</tr>\n\t\t<tr>\n\t\t<td>rollout</td>\n\t    <td>首次展示</td>\n\t\t<td>管理资源的发布</td>\n\t</tr>\n\t<tr>\n\t\t<td>scale</td>\n\t    <td>规模</td>\n\t\t<td>扩(缩)容Pod的数量</td>\n\t</tr>\n\t<tr>\n\t\t<td>autoscale</td>\n\t    <td>自动调整</td>\n\t\t<td>自动调整Pod的数量</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"2\">高级命令</td>\n\t    <td>apply</td>\n\t    <td>rc</td>\n\t\t<td>通过文件对资源进行配置</td>\n\t</tr>\n\t<tr>\n\t    <td>label</td>\n\t    <td>标签</td>\n\t\t<td>更新资源上的标签</td>\n\t</tr>\n\t<tr>\n\t\t<td rowspan=\"2\">其他命令</td>\n\t    <td>cluster-info</td>\n\t    <td>集群信息</td>\n\t\t<td>显示集群信息</td>\n\t</tr>\n\t<tr>\n\t    <td>version</td>\n\t    <td>版本</td>\n\t\t<td>显示当前Server和Client的版本</td>\n\t</tr>\n</table>\n\n下面以一个namespace / pod的创建和删除简单演示下命令的使用：\n\n~~~powershell\n# 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   21h\ndev               Active   21s\nkube-node-lease   Active   21h\nkube-public       Active   21h\nkube-system       Active   21h\n\n# 在此namespace下创建并运行一个nginx的Pod\n[root@master ~]# kubectl run pod --image=nginx -n dev\nkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-864f9875b9-pcw7x   1/1     Running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod \"pod-864f9875b9-pcw7x\" deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace \"dev\" deleted\n~~~\n\n###  命令式对象配置\n\n命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。\n\n1） 创建一个nginxpod.yaml，内容如下：\n\n~~~yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginxpod\n  namespace: dev\nspec:\n  containers:\n  - name: nginx-containers\n    image: nginx:1.17.1\n~~~\n\n2）执行create命令，创建资源：\n\n~~~powershell\n[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n~~~\n\n此时发现创建了两个资源对象，分别是namespace和pod\n\n3）执行get命令，查看资源：\n\n~~~powershell\n[root@master ~]#  kubectl get -f nginxpod.yaml\nNAME            STATUS   AGE\nnamespace/dev   Active   18s\n\nNAME            READY   STATUS    RESTARTS   AGE\npod/nginxpod    1/1     Running   0          17s\n~~~\n\n这样就显示了两个资源对象的信息\n\n4）执行delete命令，删除资源：\n\n~~~powershell\n[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace \"dev\" deleted\npod \"nginxpod\" deleted\n~~~\n\n此时发现两个资源对象被删除了\n\n~~~md\n总结:\n\t命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n~~~\n\n###  声明式对象配置\n\n声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。\n\n~~~powershell\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n~~~\n\n~~~md\n总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n\t使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n\t\t如果资源已存在，就更新，相当于 kubectl patch\n~~~\n\n> 扩展：kubectl可以在node节点上运行吗 ?\n\n​    kubectl的运行是需要进行配置的，它的配置文件是\\$HOME/.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作：\n\n~~~powershell\nscp  -r  HOME/.kube   node1: HOME/\n~~~\n\n> 使用推荐:  三种方式应该怎么用 ?\n\n 创建/更新资源      使用声明式对象配置 kubectl apply -f  XXX.yaml\n\n 删除资源              使用命令式对象配置 kubectl delete -f  XXX.yaml\n\n 查询资源              使用命令式对象管理 kubectl get(describe) 资源名称\n\n","tags":["k8s 教程"],"categories":["k8s"]},{"title":"mysql同步报错","url":"/2021/11/03/数据库/mysql 同步报错/","content":"\n### 报错1： Error 'Operation CREATE USER failed for 'zhangsan'@'%'' on query\n\n在给用户授权时，mysql出现如下报错：\n```\nLast_SQL_Error: Error 'Operation CREATE USER failed for 'zahngsan'@'%'' on query. Default database: ''. Query: 'CREATE USER 'zhangsan'@'%' IDENTIFIED BY PASSWORD '*2F7A17C3E76FB561456B2111C0E78CFB5E5030A5''\n```\n这个报错的原因可能是由于：（1）已创建过该用户 （2）已有该用户的授权信息\n\n解决方法：\n\n删除此用户，刷新，并重启slave\n\n```\n#删除该用户\nmysql> drop user 'zhangsan'@'%';\n\nmysql> FLUSH PRIVILEGES;\n\n# 重启同步\nmysql> stop slave;\n\nmysql> start slave;\n\n# 最后查看是否同步正常\nmysql> show slave status\\G\n```\n","tags":["mysql"],"categories":["数据库"]},{"title":"linux下df -hT和du -sh 显示的数据非常不一致解决方法","url":"/2021/09/24/Linux/linux下df -hT和du -sh 显示的数据非常不一致解决方法/","content":"\n\n使用lsof|grep delete命令查看正在使用的已删除的文件，发现存在多个这样的文件，kill掉进程，问题解决\n```\nfor i in `lsof |grep deleted |awk '{ print $2 }'`; do kill -9 $i ;done\n```\n","tags":["df","du"],"categories":["Linux"]},{"title":"mysql 配置详解","url":"/2021/09/22/数据库/mysql 配置详解/","content":"```\n\n[client]\nport = 3306\nsocket = /tmp/mysql.sock\n\n[mysqld]\nport = 3306\nsocket = /tmp/mysql.sock\n\nbasedir = /usr/local/mysql\ndatadir = /data/mysql\npid-file = /data/mysql/mysql.pid\nuser = mysql\nbind-address = 0.0.0.0\nserver-id = 1 #表示是本机的序号为1,一般来讲就是master的意思\n\nskip-name-resolve\n# 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，\n# 则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求\n\n#skip-networking\n\nback_log = 600\n# MySQL能有的连接数量。当主要MySQL线程在一个很短时间内得到非常多的连接请求，这就起作用，\n# 然后主线程花些时间(尽管很短)检查连接并且启动一个新线程。back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。\n# 如果期望在一个短时间内有很多连接，你需要增加它。也就是说，如果MySQL的连接数据达到max_connections时，新来的请求将会被存在堆栈中，\n# 以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。\n# 另外，这值（back_log）限于您的操作系统对到来的TCP/IP连接的侦听队列的大小。\n# 你的操作系统在这个队列大小上有它自己的限制（可以检查你的OS文档找出这个变量的最大值），试图设定back_log高于你的操作系统的限制将是无效的。\n\nmax_connections = 1000\n# MySQL的最大连接数，如果服务器的并发连接请求量比较大，建议调高此值，以增加并行连接数量，当然这建立在机器能支撑的情况下，因为如果连接数越多，介于MySQL会为每个连接提供连接缓冲区，就会开销越多的内存，所以要适当调整该值，不能盲目提高设值。可以过'conn%'通配符查看当前状态的连接数量，以定夺该值的大小。\n\nmax_connect_errors = 6000\n# 对于同一主机，如果有超出该参数值个数的中断错误连接，则该主机将被禁止连接。如需对该主机进行解禁，执行：FLUSH HOST。\n\nopen_files_limit = 65535\n# MySQL打开的文件描述符限制，默认最小1024;当open_files_limit没有被配置的时候，比较max_connections*5和ulimit -n的值，哪个大用哪个，\n# 当open_file_limit被配置的时候，比较open_files_limit和max_connections*5的值，哪个大用哪个。\n\ntable_open_cache = 128\n# MySQL每打开一个表，都会读入一些数据到table_open_cache缓存中，当MySQL在这个缓存中找不到相应信息时，才会去磁盘上读取。默认值64\n# 假定系统有200个并发连接，则需将此参数设置为200*N(N为每个连接所需的文件描述符数目)；\n# 当把table_open_cache设置为很大时，如果系统处理不了那么多文件描述符，那么就会出现客户端失效，连接不上\n\nmax_allowed_packet = 4M\n# 接受的数据包大小；增加该变量的值十分安全，这是因为仅当需要时才会分配额外内存。例如，仅当你发出长查询或MySQLd必须返回大的结果行时MySQLd才会分配更多内存。\n# 该变量之所以取较小默认值是一种预防措施，以捕获客户端和服务器之间的错误信息包，并确保不会因偶然使用大的信息包而导致内存溢出。\n\nbinlog_cache_size = 1M\n# 一个事务，在没有提交的时候，产生的日志，记录到Cache中；等到事务提交需要提交的时候，则把日志持久化到磁盘。默认binlog_cache_size大小32K\n\nmax_heap_table_size = 8M\n# 定义了用户可以创建的内存表(memory table)的大小。这个值用来计算内存表的最大行数值。这个变量支持动态改变\n\ntmp_table_size = 16M\n# MySQL的heap（堆积）表缓冲大小。所有联合在一个DML指令内完成，并且大多数联合甚至可以不用临时表即可以完成。\n# 大多数临时表是基于内存的(HEAP)表。具有大的记录长度的临时表 (所有列的长度的和)或包含BLOB列的表存储在硬盘上。\n# 如果某个内部heap（堆积）表大小超过tmp_table_size，MySQL可以根据需要自动将内存中的heap表改为基于硬盘的MyISAM表。还可以通过设置tmp_table_size选项来增加临时表的大小。也就是说，如果调高该值，MySQL同时将增加heap表的大小，可达到提高联接查询速度的效果\n\nread_buffer_size = 2M\n# MySQL读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySQL会为它分配一段内存缓冲区。read_buffer_size变量控制这一缓冲区的大小。\n# 如果对表的顺序扫描请求非常频繁，并且你认为频繁扫描进行得太慢，可以通过增加该变量值以及内存缓冲区大小提高其性能\n\nread_rnd_buffer_size = 8M\n# MySQL的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，\n# MySQL会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySQL会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大\n\nsort_buffer_size = 8M\n# MySQL执行排序使用的缓冲大小。如果想要增加ORDER BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。\n# 如果不能，可以尝试增加sort_buffer_size变量的大小\n\njoin_buffer_size = 8M\n# 联合查询操作所能使用的缓冲区大小，和sort_buffer_size一样，该参数对应的分配内存也是每连接独享\n\nthread_cache_size = 8\n# 这个值（默认8）表示可以重新利用保存在缓存中线程的数量，当断开连接时如果缓存中还有空间，那么客户端的线程将被放到缓存中，\n# 如果线程重新被请求，那么请求将从缓存中读取,如果缓存中是空的或者是新的请求，那么这个线程将被重新创建,如果有很多新的线程，\n# 增加这个值可以改善系统性能.通过比较Connections和Threads_created状态的变量，可以看到这个变量的作用。(–>表示要调整的值)\n# 根据物理内存设置规则如下：\n# 1G  —> 8\n# 2G  —> 16\n# 3G  —> 32\n# 大于3G  —> 64\n\nquery_cache_size = 8M\n#MySQL的查询缓冲大小（从4.0.1开始，MySQL提供了查询缓冲机制）使用查询缓冲，MySQL将SELECT语句和查询结果存放在缓冲区中，\n# 今后对于同样的SELECT语句（区分大小写），将直接从缓冲区中读取结果。根据MySQL用户手册，使用查询缓冲最多可以达到238%的效率。\n# 通过检查状态值'Qcache_%'，可以知道query_cache_size设置是否合理：如果Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，\n# 如果Qcache_hits的值也非常大，则表明查询缓冲使用非常频繁，此时需要增加缓冲大小；如果Qcache_hits的值不大，则表明你的查询重复率很低，\n# 这种情况下使用查询缓冲反而会影响效率，那么可以考虑不用查询缓冲。此外，在SELECT语句中加入SQL_NO_CACHE可以明确表示不使用查询缓冲\n\nquery_cache_limit = 2M\n#指定单个查询能够使用的缓冲区大小，默认1M\n\nkey_buffer_size = 4M\n#指定用于索引的缓冲区大小，增加它可得到更好处理的索引(对所有读和多重写)，到你能负担得起那样多。如果你使它太大，\n# 系统将开始换页并且真的变慢了。对于内存在4GB左右的服务器该参数可设置为384M或512M。通过检查状态值Key_read_requests和Key_reads，\n# 可以知道key_buffer_size设置是否合理。比例key_reads/key_read_requests应该尽可能的低，\n# 至少是1:100，1:1000更好(上述状态值可以使用SHOW STATUS LIKE 'key_read%'获得)。注意：该参数值设置的过大反而会是服务器整体效率降低\n\nft_min_word_len = 4\n# 分词词汇最小长度，默认4\n\ntransaction_isolation = REPEATABLE-READ\n# MySQL支持4种事务隔离级别，他们分别是：\n# READ-UNCOMMITTED, READ-COMMITTED, REPEATABLE-READ, SERIALIZABLE.\n# 如没有指定，MySQL默认采用的是REPEATABLE-READ，ORACLE默认的是READ-COMMITTED\n\nlog_bin = mysql-bin\nbinlog_format = mixed\nexpire_logs_days = 30 #超过30天的binlog删除\n\nlog_error = /data/mysql/mysql-error.log #错误日志路径\nslow_query_log = 1\nlong_query_time = 1 #慢查询时间 超过1秒则为慢查询\nslow_query_log_file = /data/mysql/mysql-slow.log\n\nperformance_schema = 0\nexplicit_defaults_for_timestamp\n\n#lower_case_table_names = 1 #不区分大小写\n\nskip-external-locking #MySQL选项以避免外部锁定。该选项默认开启\n\ndefault-storage-engine = InnoDB #默认存储引擎\n\ninnodb_file_per_table = 1\n# InnoDB为独立表空间模式，每个数据库的每个表都会生成一个数据空间\n# 独立表空间优点：\n# 1．每个表都有自已独立的表空间。\n# 2．每个表的数据和索引都会存在自已的表空间中。\n# 3．可以实现单表在不同的数据库中移动。\n# 4．空间可以回收（除drop table操作处，表空不能自已回收）\n# 缺点：\n# 单表增加过大，如超过100G\n# 结论：\n# 共享表空间在Insert操作上少有优势。其它都没独立表空间表现好。当启用独立表空间时，请合理调整：innodb_open_files\n\ninnodb_open_files = 500\n# 限制Innodb能打开的表的数据，如果库里的表特别多的情况，请增加这个。这个值默认是300\n\ninnodb_buffer_pool_size = 64M\n# InnoDB使用一个缓冲池来保存索引和原始数据, 不像MyISAM.\n# 这里你设置越大,你在存取表里面数据时所需要的磁盘I/O越少.\n# 在一个独立使用的数据库服务器上,你可以设置这个变量到服务器物理内存大小的80%\n# 不要设置过大,否则,由于物理内存的竞争可能导致操作系统的换页颠簸.\n# 注意在32位系统上你每个进程可能被限制在 2-3.5G 用户层面内存限制,\n# 所以不要设置的太高.\n\ninnodb_write_io_threads = 4\ninnodb_read_io_threads = 4\n# innodb使用后台线程处理数据页上的读写 I/O(输入输出)请求,根据你的 CPU 核数来更改,默认是4\n# 注:这两个参数不支持动态改变,需要把该参数加入到my.cnf里，修改完后重启MySQL服务,允许值的范围从 1-64\n\ninnodb_thread_concurrency = 0\n# 默认设置为 0,表示不限制并发数，这里推荐设置为0，更好去发挥CPU多核处理能力，提高并发量\n\ninnodb_purge_threads = 1\n# InnoDB中的清除操作是一类定期回收无用数据的操作。在之前的几个版本中，清除操作是主线程的一部分，这意味着运行时它可能会堵塞其它的数据库操作。\n# 从MySQL5.5.X版本开始，该操作运行于独立的线程中,并支持更多的并发数。用户可通过设置innodb_purge_threads配置参数来选择清除操作是否使用单\n# 独线程,默认情况下参数设置为0(不使用单独线程),设置为 1 时表示使用单独的清除线程。建议为1\n\ninnodb_flush_log_at_trx_commit = 2\n# 0：如果innodb_flush_log_at_trx_commit的值为0,log buffer每秒就会被刷写日志文件到磁盘，提交事务的时候不做任何操作（执行是由mysql的master thread线程来执行的。\n# 主线程中每秒会将重做日志缓冲写入磁盘的重做日志文件(REDO LOG)中。不论事务是否已经提交）默认的日志文件是ib_logfile0,ib_logfile1\n# 1：当设为默认值1的时候，每次提交事务的时候，都会将log buffer刷写到日志。\n# 2：如果设为2,每次提交事务都会写日志，但并不会执行刷的操作。每秒定时会刷到日志文件。要注意的是，并不能保证100%每秒一定都会刷到磁盘，这要取决于进程的调度。\n# 每次事务提交的时候将数据写入事务日志，而这里的写入仅是调用了文件系统的写入操作，而文件系统是有 缓存的，所以这个写入并不能保证数据已经写入到物理磁盘\n# 默认值1是为了保证完整的ACID。当然，你可以将这个配置项设为1以外的值来换取更高的性能，但是在系统崩溃的时候，你将会丢失1秒的数据。\n# 设为0的话，mysqld进程崩溃的时候，就会丢失最后1秒的事务。设为2,只有在操作系统崩溃或者断电的时候才会丢失最后1秒的数据。InnoDB在做恢复的时候会忽略这个值。\n# 总结\n# 设为1当然是最安全的，但性能页是最差的（相对其他两个参数而言，但不是不能接受）。如果对数据一致性和完整性要求不高，完全可以设为2，如果只最求性能，例如高并发写的日志服务器，设为0来获得更高性能\n\ninnodb_log_buffer_size = 2M\n# 此参数确定些日志文件所用的内存大小，以M为单位。缓冲区更大能提高性能，但意外的故障将会丢失数据。MySQL开发人员建议设置为1－8M之间\n\ninnodb_log_file_size = 32M\n# 此参数确定数据日志文件的大小，更大的设置可以提高性能，但也会增加恢复故障数据库所需的时间\n\ninnodb_log_files_in_group = 3\n# 为提高性能，MySQL可以以循环方式将日志文件写到多个文件。推荐设置为3\n\ninnodb_max_dirty_pages_pct = 90\n# innodb主线程刷新缓存池中的数据，使脏数据比例小于90%\n\ninnodb_lock_wait_timeout = 120\n# InnoDB事务在被回滚之前可以等待一个锁定的超时秒数。InnoDB在它自己的锁定表中自动检测事务死锁并且回滚事务。InnoDB用LOCK TABLES语句注意到锁定设置。默认值是50秒\n\nbulk_insert_buffer_size = 8M\n# 批量插入缓存大小， 这个参数是针对MyISAM存储引擎来说的。适用于在一次性插入100-1000+条记录时， 提高效率。默认值是8M。可以针对数据量的大小，翻倍增加。\n\nmyisam_sort_buffer_size = 8M\n# MyISAM设置恢复表之时使用的缓冲区的尺寸，当在REPAIR TABLE或用CREATE INDEX创建索引或ALTER TABLE过程中排序 MyISAM索引分配的缓冲区\n\nmyisam_max_sort_file_size = 10G\n# 如果临时文件会变得超过索引，不要使用快速排序索引方法来创建一个索引。注释：这个参数以字节的形式给出\n\nmyisam_repair_threads = 1\n# 如果该值大于1，在Repair by sorting过程中并行创建MyISAM表索引(每个索引在自己的线程内)\n\ninteractive_timeout = 28800\n# 服务器关闭交互式连接前等待活动的秒数。交互式客户端定义为在mysql_real_connect()中使用CLIENT_INTERACTIVE选项的客户端。默认值：28800秒（8小时）\n\nwait_timeout = 28800\n# 服务器关闭非交互连接之前等待活动的秒数。在线程启动时，根据全局wait_timeout值或全局interactive_timeout值初始化会话wait_timeout值，\n# 取决于客户端类型(由mysql_real_connect()的连接选项CLIENT_INTERACTIVE定义)。参数默认值：28800秒（8小时）\n# MySQL服务器所支持的最大连接数是有上限的，因为每个连接的建立都会消耗内存，因此我们希望客户端在连接到MySQL Server处理完相应的操作后，\n# 应该断开连接并释放占用的内存。如果你的MySQL Server有大量的闲置连接，他们不仅会白白消耗内存，而且如果连接一直在累加而不断开，\n# 最终肯定会达到MySQL Server的连接上限数，这会报'too many connections'的错误。对于wait_timeout的值设定，应该根据系统的运行情况来判断。\n# 在系统运行一段时间后，可以通过show processlist命令查看当前系统的连接状态，如果发现有大量的sleep状态的连接进程，则说明该参数设置的过大，\n# 可以进行适当的调整小些。要同时设置interactive_timeout和wait_timeout才会生效。\n\n[mysqldump]\nquick\nmax_allowed_packet = 16M #服务器发送和接受的最大包长度\n\n[myisamchk]\nkey_buffer_size = 8M\nsort_buffer_size = 8M\nread_buffer = 4M\nwrite_buffer = 4M\n\n```\n","tags":["mysql"],"categories":["数据库"]},{"title":"docker-compose 配置参数详解","url":"/2021/08/16/docker/docker-compose 配置参数详解/","content":"\n\n\n### Compose和Docker兼容性：\n    Compose 文件格式有3个版本,分别为1, 2.x 和 3.x\n    目前主流的为 3.x 其支持 docker 1.13.0 及其以上的版本\n\n<!--more-->\n### 常用参数：\n    version           # 指定 compose 文件的版本\n    services          # 定义所有的 service 信息, services 下面的第一级别的 key 既是一个 service 的名称\n\n        build                 # 指定包含构建上下文的路径, 或作为一个对象，该对象具有 context 和指定的 dockerfile 文件以及 args 参数值\n            context               # context: 指定 Dockerfile 文件所在的路径\n            dockerfile            # dockerfile: 指定 context 指定的目录下面的 Dockerfile 的名称(默认为 Dockerfile)\n            args                  # args: Dockerfile 在 build 过程中需要的参数 (等同于 docker container build --build-arg 的作用)\n            cache_from            # v3.2中新增的参数, 指定缓存的镜像列表 (等同于 docker container build --cache_from 的作用)\n            labels                # v3.3中新增的参数, 设置镜像的元数据 (等同于 docker container build --labels 的作用)\n            shm_size              # v3.5中新增的参数, 设置容器 /dev/shm 分区的大小 (等同于 docker container build --shm-size 的作用)\n\n        command               # 覆盖容器启动后默认执行的命令, 支持 shell 格式和 [] 格式\n\n        configs               # 不知道怎么用\n\n        cgroup_parent         # 不知道怎么用\n\n        container_name        # 指定容器的名称 (等同于 docker run --name 的作用)\n\n        credential_spec       # 不知道怎么用\n\n        deploy                # v3 版本以上, 指定与部署和运行服务相关的配置, deploy 部分是 docker stack 使用的, docker stack 依赖 docker swarm\n            endpoint_mode         # v3.3 版本中新增的功能, 指定服务暴露的方式\n                vip                   # Docker 为该服务分配了一个虚拟 IP(VIP), 作为客户端的访问服务的地址\n                dnsrr                 # DNS轮询, Docker 为该服务设置 DNS 条目, 使得服务名称的 DNS 查询返回一个 IP 地址列表, 客户端直接访问其中的一个地址\n            labels                # 指定服务的标签，这些标签仅在服务上设置\n            mode                  # 指定 deploy 的模式\n                global                # 每个集群节点都只有一个容器\n                replicated            # 用户可以指定集群中容器的数量(默认)\n            placement             # 不知道怎么用\n            replicas              # deploy 的 mode 为 replicated 时, 指定容器副本的数量\n            resources             # 资源限制\n                limits                # 设置容器的资源限制\n                    cpus: \"0.5\"           # 设置该容器最多只能使用 50% 的 CPU\n                    memory: 50M           # 设置该容器最多只能使用 50M 的内存空间\n                reservations          # 设置为容器预留的系统资源(随时可用)\n                    cpus: \"0.2\"           # 为该容器保留 20% 的 CPU\n                    memory: 20M           # 为该容器保留 20M 的内存空间\n            restart_policy        # 定义容器重启策略, 用于代替 restart 参数\n                condition             # 定义容器重启策略(接受三个参数)\n                    none                  # 不尝试重启\n                    on-failure            # 只有当容器内部应用程序出现问题才会重启\n                    any                   # 无论如何都会尝试重启(默认)\n                delay                 # 尝试重启的间隔时间(默认为 0s)\n                max_attempts          # 尝试重启次数(默认一直尝试重启)\n                window                # 检查重启是否成功之前的等待时间(即如果容器启动了, 隔多少秒之后去检测容器是否正常, 默认 0s)\n            update_config         # 用于配置滚动更新配置\n                parallelism           # 一次性更新的容器数量\n                delay                 # 更新一组容器之间的间隔时间\n                failure_action        # 定义更新失败的策略\n                    continue              # 继续更新\n                    rollback              # 回滚更新\n                    pause                 # 暂停更新(默认)\n                monitor               # 每次更新后的持续时间以监视更新是否失败(单位: ns|us|ms|s|m|h) (默认为0)\n                max_failure_ratio     # 回滚期间容忍的失败率(默认值为0)\n                order                 # v3.4 版本中新增的参数, 回滚期间的操作顺序\n                    stop-first            #旧任务在启动新任务之前停止(默认)\n                    start-first           #首先启动新任务, 并且正在运行的任务暂时重叠\n            rollback_config       # v3.7 版本中新增的参数, 用于定义在 update_config 更新失败的回滚策略\n                parallelism           # 一次回滚的容器数, 如果设置为0, 则所有容器同时回滚\n                delay                 # 每个组回滚之间的时间间隔(默认为0)\n                failure_action        # 定义回滚失败的策略\n                    continue              # 继续回滚\n                    pause                 # 暂停回滚\n                monitor               # 每次回滚任务后的持续时间以监视失败(单位: ns|us|ms|s|m|h) (默认为0)\n                max_failure_ratio     # 回滚期间容忍的失败率(默认值0)\n                order                 # 回滚期间的操作顺序\n                    stop-first            # 旧任务在启动新任务之前停止(默认)\n                    start-first           # 首先启动新任务, 并且正在运行的任务暂时重叠\n\n            注意：\n                支持 docker-compose up 和 docker-compose run 但不支持 docker stack deploy 的子选项\n                security_opt  container_name  devices  tmpfs  stop_signal  links    cgroup_parent\n                network_mode  external_links  restart  build  userns_mode  sysctls\n\n        devices               # 指定设备映射列表 (等同于 docker run --device 的作用)\n\n        depends_on            # 定义容器启动顺序 (此选项解决了容器之间的依赖关系， 此选项在 v3 版本中 使用 swarm 部署时将忽略该选项)\n            示例：\n                docker-compose up 以依赖顺序启动服务，下面例子中 redis 和 db 服务在 web 启动前启动\n                默认情况下使用 docker-compose up web 这样的方式启动 web 服务时，也会启动 redis 和 db 两个服务，因为在配置文件中定义了依赖关系\n                version: '3'\n                services:\n                    web:\n                        build: .\n                        depends_on:\n                            - db\n                            - redis\n                    redis:\n                        image: redis\n                    db:\n                        image: postgres\n\n        dns                   # 设置 DNS 地址(等同于 docker run --dns 的作用)\n\n        dns_search            # 设置 DNS 搜索域(等同于 docker run --dns-search 的作用)\n\n        tmpfs                 # v2 版本以上, 挂载目录到容器中, 作为容器的临时文件系统(等同于 docker run --tmpfs 的作用, 在使用 swarm 部署时将忽略该选项)\n\n        entrypoint            # 覆盖容器的默认 entrypoint 指令 (等同于 docker run --entrypoint 的作用)\n\n        env_file              # 从指定文件中读取变量设置为容器中的环境变量, 可以是单个值或者一个文件列表, 如果多个文件中的变量重名则后面的变量覆盖前面的变量, environment 的值覆盖 env_file 的值\n            文件格式：\n                RACK_ENV=development\n\n        environment           # 设置环境变量， environment 的值可以覆盖 env_file 的值 (等同于 docker run --env 的作用)\n\n        expose                # 暴露端口, 但是不能和宿主机建立映射关系, 类似于 Dockerfile 的 EXPOSE 指令\n\n        external_links        # 连接不在 docker-compose.yml 中定义的容器或者不在 compose 管理的容器(docker run 启动的容器, 在 v3 版本中使用 swarm 部署时将忽略该选项)\n\n        extra_hosts           # 添加 host 记录到容器中的 /etc/hosts 中 (等同于 docker run --add-host 的作用)\n\n        healthcheck           # v2.1 以上版本, 定义容器健康状态检查, 类似于 Dockerfile 的 HEALTHCHECK 指令\n            test                  # 检查容器检查状态的命令, 该选项必须是一个字符串或者列表, 第一项必须是 NONE, CMD 或 CMD-SHELL, 如果其是一个字符串则相当于 CMD-SHELL 加该字符串\n                NONE                  # 禁用容器的健康状态检测\n                CMD                   # test: [\"CMD\", \"curl\", \"-f\", \"http://localhost\"]\n                CMD-SHELL             # test: [\"CMD-SHELL\", \"curl -f http://localhost || exit 1\"] 或者　test: curl -f https://localhost || exit 1\n            interval: 1m30s       # 每次检查之间的间隔时间\n            timeout: 10s          # 运行命令的超时时间\n            retries: 3            # 重试次数\n            start_period: 40s     # v3.4 以上新增的选项, 定义容器启动时间间隔\n            disable: true         # true 或 false, 表示是否禁用健康状态检测和　test: NONE 相同\n\n        image                 # 指定 docker 镜像, 可以是远程仓库镜像、本地镜像\n\n        init                  # v3.7 中新增的参数, true 或 false 表示是否在容器中运行一个 init, 它接收信号并传递给进程\n\n        isolation             # 隔离容器技术, 在 Linux 中仅支持 default 值\n\n        labels                # 使用 Docker 标签将元数据添加到容器, 与 Dockerfile 中的 LABELS 类似\n\n        links                 # 链接到其它服务中的容器, 该选项是 docker 历史遗留的选项, 目前已被用户自定义网络名称空间取代, 最终有可能被废弃 (在使用 swarm 部署时将忽略该选项)\n\n        logging               # 设置容器日志服务\n            driver                # 指定日志记录驱动程序, 默认 json-file (等同于 docker run --log-driver 的作用)\n            options               # 指定日志的相关参数 (等同于 docker run --log-opt 的作用)\n                max-size              # 设置单个日志文件的大小, 当到达这个值后会进行日志滚动操作\n                max-file              # 日志文件保留的数量\n\n        network_mode          # 指定网络模式 (等同于 docker run --net 的作用, 在使用 swarm 部署时将忽略该选项)\n\n        networks              # 将容器加入指定网络 (等同于 docker network connect 的作用), networks 可以位于 compose 文件顶级键和 services 键的二级键\n            aliases               # 同一网络上的容器可以使用服务名称或别名连接到其中一个服务的容器\n            ipv4_address      # IP V4 格式\n            ipv6_address      # IP V6 格式\n\n            示例:\n                version: '3.7'\n                services:\n                    test:\n                        image: nginx:1.14-alpine\n                        container_name: mynginx\n                        command: ifconfig\n                        networks:\n                            app_net:                                # 调用下面 networks 定义的 app_net 网络\n                            ipv4_address: 172.16.238.10\n                networks:\n                    app_net:\n                        driver: bridge\n                        ipam:\n                            driver: default\n                            config:\n                                - subnet: 172.16.238.0/24\n\n        pid: 'host'           # 共享宿主机的 进程空间(PID)\n\n        ports                 # 建立宿主机和容器之间的端口映射关系, ports 支持两种语法格式\n            SHORT 语法格式示例:\n                - \"3000\"                            # 暴露容器的 3000 端口, 宿主机的端口由 docker 随机映射一个没有被占用的端口\n                - \"3000-3005\"                       # 暴露容器的 3000 到 3005 端口, 宿主机的端口由 docker 随机映射没有被占用的端口\n                - \"8000:8000\"                       # 容器的 8000 端口和宿主机的 8000 端口建立映射关系\n                - \"9090-9091:8080-8081\"\n                - \"127.0.0.1:8001:8001\"             # 指定映射宿主机的指定地址的\n                - \"127.0.0.1:5000-5010:5000-5010\"\n                - \"6060:6060/udp\"                   # 指定协议\n\n            LONG 语法格式示例:(v3.2 新增的语法格式)\n                ports:\n                    - target: 80                    # 容器端口\n                      published: 8080               # 宿主机端口\n                      protocol: tcp                 # 协议类型\n                      mode: host                    # host 在每个节点上发布主机端口,  ingress 对于群模式端口进行负载均衡\n\n        secrets               # 不知道怎么用\n\n        security_opt          # 为每个容器覆盖默认的标签 (在使用 swarm 部署时将忽略该选项)\n\n        stop_grace_period     # 指定在发送了 SIGTERM 信号之后, 容器等待多少秒之后退出(默认 10s)\n\n        stop_signal           # 指定停止容器发送的信号 (默认为 SIGTERM 相当于 kill PID; SIGKILL 相当于 kill -9 PID; 在使用 swarm 部署时将忽略该选项)\n\n        sysctls               # 设置容器中的内核参数 (在使用 swarm 部署时将忽略该选项)\n\n        ulimits               # 设置容器的 limit\n\n        userns_mode           # 如果Docker守护程序配置了用户名称空间, 则禁用此服务的用户名称空间 (在使用 swarm 部署时将忽略该选项)\n\n        volumes               # 定义容器和宿主机的卷映射关系, 其和 networks 一样可以位于 services 键的二级键和 compose 顶级键, 如果需要跨服务间使用则在顶级键定义, 在 services 中引用\n            SHORT 语法格式示例:\n                volumes:\n                    - /var/lib/mysql                # 映射容器内的 /var/lib/mysql 到宿主机的一个随机目录中\n                    - /opt/data:/var/lib/mysql      # 映射容器内的 /var/lib/mysql 到宿主机的 /opt/data\n                    - ./cache:/tmp/cache            # 映射容器内的 /var/lib/mysql 到宿主机 compose 文件所在的位置\n                    - ~/configs:/etc/configs/:ro    # 映射容器宿主机的目录到容器中去, 权限只读\n                    - datavolume:/var/lib/mysql     # datavolume 为 volumes 顶级键定义的目录, 在此处直接调用\n\n            LONG 语法格式示例:(v3.2 新增的语法格式)\n                version: \"3.2\"\n                services:\n                    web:\n                        image: nginx:alpine\n                        ports:\n                            - \"80:80\"\n                        volumes:\n                            - type: volume                  # mount 的类型, 必须是 bind、volume 或 tmpfs\n                                source: mydata              # 宿主机目录\n                                target: /data               # 容器目录\n                                volume:                     # 配置额外的选项, 其 key 必须和 type 的值相同\n                                    nocopy: true                # volume 额外的选项, 在创建卷时禁用从容器复制数据\n                            - type: bind                    # volume 模式只指定容器路径即可, 宿主机路径随机生成; bind 需要指定容器和数据机的映射路径\n                                source: ./static\n                                target: /opt/app/static\n                                read_only: true             # 设置文件系统为只读文件系统\n                volumes:\n                    mydata:                                 # 定义在 volume, 可在所有服务中调用\n\n        restart               # 定义容器重启策略(在使用 swarm 部署时将忽略该选项, 在 swarm 使用 restart_policy 代替 restart)\n            no                    # 禁止自动重启容器(默认)\n            always                # 无论如何容器都会重启\n            on-failure            # 当出现 on-failure 报错时, 容器重新启动\n\n        其他选项：\n            domainname, hostname, ipc, mac_address, privileged, read_only, shm_size, stdin_open, tty, user, working_dir\n            上面这些选项都只接受单个值和 docker run 的对应参数类似\n\n        对于值为时间的可接受的值：\n            2.5s\n            10s\n            1m30s\n            2h32m\n            5h34m56s\n            时间单位: us, ms, s, m， h\n        对于值为大小的可接受的值：\n            2b\n            1024kb\n            2048k\n            300m\n            1gb\n            单位: b, k, m, g 或者 kb, mb, gb\n    networks          # 定义 networks 信息\n        driver                # 指定网络模式, 大多数情况下, 它 bridge 于单个主机和 overlay Swarm 上\n            bridge                # Docker 默认使用 bridge 连接单个主机上的网络\n            overlay               # overlay 驱动程序创建一个跨多个节点命名的网络\n            host                  # 共享主机网络名称空间(等同于 docker run --net=host)\n            none                  # 等同于 docker run --net=none\n        driver_opts           # v3.2以上版本, 传递给驱动程序的参数, 这些参数取决于驱动程序\n        attachable            # driver 为 overlay 时使用, 如果设置为 true 则除了服务之外，独立容器也可以附加到该网络; 如果独立容器连接到该网络，则它可以与其他 Docker 守护进程连接到的该网络的服务和独立容器进行通信\n        ipam                  # 自定义 IPAM 配置. 这是一个具有多个属性的对象, 每个属性都是可选的\n            driver                # IPAM 驱动程序, bridge 或者 default\n            config                # 配置项\n                subnet                # CIDR格式的子网，表示该网络的网段\n        external              # 外部网络, 如果设置为 true 则 docker-compose up 不会尝试创建它, 如果它不存在则引发错误\n        name                  # v3.5 以上版本, 为此网络设置名称\n文件格式示例：\n```\nversion: \"3\"\nservices:\n    redis:\n    image: redis:alpine\n    ports:\n        - \"6379\"\n    networks:\n        - frontend\n    deploy:\n        replicas: 2\n        update_config:\n        parallelism: 2\n        delay: 10s\n        restart_policy:\n        condition: on-failure\n    db:\n    image: postgres:9.4\n    volumes:\n        - db-data:/var/lib/postgresql/data\n    networks:\n        - backend\n    deploy:\n        placement:\n        constraints: [node.role == manager]\n\n```\n\n\n\n#### 原文链接：http://blog.baidu120.cc/archives/2019_10_27_840.html\n","tags":["docker-compose","配置参数"],"categories":["docker"]},{"title":"服务器挂载大于2T的硬盘","url":"/2021/08/16/Linux/服务器挂载大于2T的硬盘/","content":"### 背景:\n- 服务器购买的时候带了一个5T硬盘，当使用fdisk进行格式化挂载到文件夹下，发现只有2T！！！后来网上查询发现fdisk只能处理2T以下的硬盘,需要使用parted处理大于2T  以上的硬盘!!\n![image](https://user-images.githubusercontent.com/28568478/117395390-60a75b80-af2a-11eb-9acd-ac5bdc4e0ee8.png)\n![image](https://user-images.githubusercontent.com/28568478/117395399-69982d00-af2a-11eb-9bfc-9308c3cb3f0b.png)\n\n\n<!--more-->\n\n### 命令说明:\n - 磁盘分区的命令有fdisk和parted，但fdisk只能处理2T以下的硬盘，parted能处理大于2T以上的硬盘，由于服务的硬盘是5T，所以我们采用parted作为磁盘分区。\n\n\n### 步骤：\n- 一块硬盘要想被使用，要经过分区——>格式化——>挂载。这三个步骤\n#### 1、查看硬盘情况\n    fdisk -l\n![image](https://user-images.githubusercontent.com/28568478/117395429-7ae13980-af2a-11eb-8224-5c6240ad3401.png)\n\n#### 2、挂载2T以上的硬盘需要GPT格式，使用parted命令，细节如下:\n```\n   parted /dev/vdc\n  挂载2T以上的硬盘需要GPT格式，使用parted命令，细节如下\n  （1）parted /dev/vdc （视具体情况盘符编号）\n  （2）print （查看当前分区情况）\n  （3）mklabel gpt （设置分区类型为gpt）\n  （4）mkpart extended 0% 100% （扩展分区extended ,主分区primary ,并使用整个硬盘）\n  （5）print （查看一下）\n  （6）quit\n  （7）mkfs.ext4 /dev/vdc 或者 mkfs.xfs -f /dev/vdc（格式化新硬盘，格式化需要比较长的时间，具体根据磁盘读写速度和大小来确定。下面有节点数完成对应的数量即可）\n\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117395473-90eefa00-af2a-11eb-8be4-a7b565266022.png)\n![image](https://user-images.githubusercontent.com/28568478/117395479-951b1780-af2a-11eb-8b1d-226524a7935a.png)\n\n\n\n\n#### 3、挂载目录\n```\nmkdir  /data2 (在根目录下，建一个文件夹，待会将分区挂载在这个文件夹上，以后要往新硬盘存东西就存在新建文件夹下就可以了。)\nmount /dev/vdc /data2 （挂载，之后即可使用了，第二个参数是挂载到哪个目录）\n```\n![image](https://user-images.githubusercontent.com/28568478/117395486-9ba98f00-af2a-11eb-84ef-12793057ee14.png)\n\n#### 3、自动挂载\n还没有完事，这样挂载的重启整个服务器后不会自动挂载\n需要修改/etc/fstab文件，在文件最后追加\n/dev/vdc /data ext4 defaults 0 0\n```\nsudo echo \"/dev/vdc /data2 ext4 defaults 0 1\" >> /etc/fstab       让系统开机自动挂载这块硬盘\n或者\nsudo echo \"/dev/vdc /data2 xfs defaults 0 1\" >> /etc/fstab\n```\n这样在重启后会自动挂载\n![image](https://user-images.githubusercontent.com/28568478/117395551-bbd94e00-af2a-11eb-8007-4bd9d388179a.png)\n\n### 挂载完成后 一定要reboot重启服务器  df-h 确认应该在服务器重启后依然可以正常挂载！！！\n\n## 参考目录：\nhttps://blog.51cto.com/devin223/2175078\nhttps://blog.csdn.net/glongljl/article/details/80104569\n[fdisk和parted磁盘分区工具命令](https://blog.csdn.net/reliveIT/article/details/44602959)\n[磁盘挂载问题：Fdisk最大只能创建2T分区的盘，超过2T使用parted](https://blog.csdn.net/xianjuke008/article/details/88354466)\n[linux下（fdisk,gdisk,parted）三种分区工具比较](https://www.cnblogs.com/zhaojiedi1992/p/zhaojiedi_linux_039_fdisk_gdisk_parted.html)\n","tags":["硬盘挂载"],"categories":["Linux"]},{"title":"kafka基本命令","url":"/2021/07/01/异步任务/kafka基本命令 copy/","content":"\n**一：Homebrew是神马**\n\n　　先介绍下mac 神器 Homebrew, 类似linux系统下的yum和 apt-get，Homebrew简称brew，是Mac OSX上的软件包管理工具，能在Mac中方便的安装软件或者卸载软件\n\n**二：Homebrew官网** https://brew.sh/\n\n**三：Homebrew安装** \n\n打开终端，直接输入即可完成安装\n\n```\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n```\n<!--more-->\n**四：Homebrew使用**\n\nHomebrew使用没啥好说的了，常用的\n\n搜索软件：brew search 软件名，如brew search mysql\n\n安装软件：brew install 软件名，如brew install mysql\n\n卸载软件：brew remove 软件名，如brew remove mysql\n\n列出软件：brew list 软件名，如brew list mysql\n\n### 五. 安装kafka\n\n$   brew install kafka\n\n(1)  安装过程将依赖安装 zookeeper\n\n(2)  软件位置\n\n/usr/local/Cellar/zookeeper\n\n/usr/local/Cellar/kafka\n\n(3)  配置文件位置\n\n/usr/local/etc/kafka/zookeeper.properties\n\n/usr/local/etc/kafka/server.properties\n\n备注：后续操作均需进入 /usr/local/Cellar/kafka/2.0.0/bin 目录下。\n\n### 六、安装\n\nkafka依赖Java，因此要确保先安装好java\n\n```\n// mac 环境使用brew直接安装kafkabrew install kafka  // 卸载brew uninstall kafka\n```\n\n### 七、启动\n\n```\n// 启动brew services start kafkabrew services start zookeeper // 重启brew services restart kafkabrew services restart zookeeper\n```\n\n### 八、创建主题\n\n```\nkafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n```\n\n###  九、删除主题\n\n```\nkafka-topics  --delete --zookeeper localhost:2181 --topic 【topic name】\n\n```\n\n### 十、查看主题\n\n```\nkafka-topics --list --zookeeper localhost:2181\n\n```\n\n### 十一、生产消息\n\n```\nkafka-console-producer --broker-list localhost:9092 --topic test\n\n```\n\n### 十二、消费消息\n\n```\nkafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning\n```\n\n","tags":["macOS","kafka"],"categories":["异步任务"]},{"title":"Docker volume 挂载时文件或文件夹不存在","url":"/2021/06/11/docker/Docker volume 挂载时文件或文件夹不存在/","content":"\n\n### 一、背景介绍\ndocker volume 可以使我们在启动docker容器时，动态的挂载一些文件（如配置文件）, 以覆盖镜像中原有的文件，但是，挂载一个主机上尚不存在的文件夹或者文件到容器中会怎样呢？LZ在工作中就遇到了这样的问题，故自己实践了一下，记录实验结果如下：\n\n### 二、文件夹挂载\ndocker在文件夹挂载上的行为是统一的，具体表现为：\n\n- 若文件夹不存在，则先创建出文件夹（若为多层文件夹，则递归创建）\n- 用host上的文件夹内容覆盖container中的文件夹内容\n\n`docker run -v /path-to-folder/A:/path-to-folder/B test-image`\n\n#### 详细说明如下：\n<!--more-->\n##### 1、host上文件夹存在，且非空\n|host|container|mount result|\n|:----    |:---|:----- |\n| 存在的非空文件夹A\t  |不存在的文件夹B\t | 先在contanier中创建文件夹B，再将A文件夹中的所有文件copy到B中|\n|存在的非空文件夹A\t|存在的非空文件夹B\t|先将container中文件夹B的原有内容清空，再将A中文件copy到B中|\n>无论container中的文件夹B是否存在， A都会完全覆盖B的内容\n\n\n\n##### 2、host上文件夹存在，但为空\n\n|host|container|mount result|\n|:----    |:---|:----- |\n| 存在的空文件夹A |  存在的非空文件夹B | container中文件夹B的内容被清空  |\n> container中对应的文件夹内容被清空\n\n##### 3、host上文件夹不存在\n|host|container|mount result|\n|:----    |:---|:----- |\n| 不存在的文件夹A |  存在的非空文件夹B | 在host上创建文件夹A，container中文件夹B的内容被清空  |\n| 不存在的文件夹A/B/C |  存在的非空文件夹B | 在host上创建文件夹A/B/C，container中文件夹B的内容被清空  |\n\n>container中对应的文件夹内容被清空\n\n### 总结:\n    host上文件夹一定会覆盖container中文件夹：\n|host|container|mount result|\n|:----    |:---|:----- |\n| 文件夹不存在/文件夹存在但为空 | 文件夹不存在/存在但为空/存在且不为空  | container中文件被覆盖（清空）  |\n| 文件夹存在且不为空 |文件夹不存在/存在但为空/存在且不为空   |  container中文件夹内容被覆盖（原内容清空， 覆盖为host上文件夹内容） |\n\n\n\n### 三、文件挂载\n\n文件挂载与文件夹挂载最大的不同点在于：\n\n- docker 禁止用主机上不存在的文件挂载到container中已经存在的文件\n- 文件挂载不会对同一文件夹下的其他文件产生任何影响\n\n除此之外， 其覆盖行为与文件夹挂载一致，即：\n\n- 用host上的文件的内容覆盖container中的文件的内容\n\n`docker run -v /path-to-folder/non-existent-config.js:/path-to-folder/config.js test-image # forbidden`\n\n##### 详细说明如下：\n\n##### 1、host上文件不存在\n\n|host|container|mount result|\n|:----    |:---|:----- |\n|  |   |   |\n\n|host|container|mount result|\n|:----    |:---|:----- |\n| 不存在的文件configA.js | 已经存在的文件congfigB.js  |  报错，Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type. 同时会在host上生成两个空目录 configA.js 和 configB.js, 但是container无法启动 |\n\n\n##### 2、host上文件存在\n\n|host|container|mount result|\n|:----    |:---|:----- |\n|  存在的文件configA.js| 存在的文件congfigB.js  |  container中文件名configB.js保持不变,但是文件内容被congfigA.js的内容覆盖了 |\n|存在的文件configA.js | 不存在的文件congfigB.js  |  container中新建一个文件configB.js，其内容为configA.js的文件内容， configB.js所在文件下的所有其他文件维持不变 |\n\n\n### 总结:\n    host上文件一定会覆盖container中文件夹\n\n|host|container|mount result|\n|:----    |:---|:----- |\n| 不存在的文件 |  已经存在的文件 | 禁止行为  |\n| 存在的文件 |  不存在的文件/已经存在的文件 | 新增/覆盖 （若目录不存在则会创建目录）  |\n\n\n\n### 四、结论:\n### 文件夹挂载\n- 允许不存在的文件夹或者存在的空文件夹挂载进container, container中对应的文件夹将被清空\n- 非空文件夹挂载进container将会覆盖container中原有文件夹\n### 文件挂载\n- 禁止将不存在的文件挂载进container中已经存在的文件上\n- 存在的文件挂载进container中将会覆盖container中对应的文件， 若文件不存在则新建\n### 应用场景\n- 从上面的分析可知，文件夹挂载以整个文件夹为单位进行文件覆盖，故可在需要将大量文件挂载进container时使用，另外，如果挂载一个空文件夹或者不存在的文件夹，一般是做逆向使用： 即容器启动后，可能会在容器内挂载点的文件夹下生成一些文件（如日志），此时，在对应的host上的文件夹内就能直接看到。\n- 文件挂载由于只会覆盖单个文件而不会影响container中同一文件夹下的其他文件，常常被用来挂载配置文件，以在运行时，动态的修改默认配置。\n\n参考：\n[docker-compose:数据卷volumes挂载规则](https://blog.csdn.net/viafcccy/article/details/115514431)\n[Docker volume 挂载时文件或文件夹不存在](https://blog.csdn.net/weixin_33953249/article/details/88759709)\n","tags":["docker","volumes"],"categories":["docker"]},{"title":"创建python SDK 并上传至pypi","url":"/2021/05/28/python/创建python SDK 并上传至pypi/","content":"\n# 一、python打包创建SDK\n\n### 1、首先，我们需要一个工具包来协助我们完成python打包的任务\n```\npip3 install setuptools\n```\n\n### 2、之后封装好你的项目api\n目录结构\n\n├─python-sdk\n│  │  `README.md`\n│  │  `setup.py`\n│  └─`Demo`\n|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |  `demo.py`\n|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;│ `__init__.py`\n|\n\nDemo/demo.py.py中只有一个输出函数demo（），这里可以自行封装自己的api\n<!--more-->\n### 3、编写setup.py文件，用于安装Demo包\n\n```\nsetup.py各参数介绍：\n    --name 包名称\n    --version (-V) 包版本\n    --author 程序的作者\n    --author_email 程序的作者的邮箱地址\n    --maintainer 维护者\n    --maintainer_email 维护者的邮箱地址\n    --url 程序的官网地址\n    --license 程序的授权信息\n    --description 程序的简单描述\n    --long_description 程序的详细描述\n    --platforms 程序适用的软件平台列表\n    --classifiers 程序的所属分类列表\n    --keywords 程序的关键字列表\n    --packages 需要处理的包目录（包含__init__.py的文件夹）\n    --py_modules 需要打包的python文件列表\n    --download_url 程序的下载地址\n    --cmdclass\n    --data_files 打包时需要打包的数据文件，如图片，配置文件等\n    --scripts 安装时需要执行的脚步列表\n    --package_dir 告诉setuptools哪些目录下的文件被映射到哪个源码包。一个例子：package_dir = {'': 'lib'}，表示“root package”中的模块都在lib 目录中。\n    --requires 定义依赖哪些模块\n    --provides定义可以为哪些模块提供依赖\n    --find_packages() 对于简单工程来说，手动增加packages参数很容易，刚刚我们用到了这个函数，它默认在和setup.py同一目录下搜索各个含有 __init__.py的包。\n\n                        其实我们可以将包统一放在一个src目录中，另外，这个包内可能还有aaa.txt文件和data数据文件夹。另外，也可以排除一些特定的包\n\n                            find_packages(exclude=[\"*.tests\", \"*.tests.*\", \"tests.*\", \"tests\"])\n\n    --install_requires = [\"requests\"] 需要安装的依赖包\n    --entry_points 动态发现服务和插件，下面详细讲\n```\n下列entry_points中： console_scripts 指明了命令行工具的名称；在“redis_run = RedisRun.redis_run:main”中，等号前面指明了工具包的名称，等号后面的内容指明了程序的入口地址。\n```\n1 entry_points={'console_scripts': [\n2         'redis_run = RedisRun.redis_run:main',\n3 ]}\n```\n\n这里可以有多条记录，这样一个项目就可以制作多个命令行工具了，比如：\n```\n1 setup(\n2     entry_points = {\n3         'console_scripts': [\n4             'foo = demo:test',\n5             'bar = demo:test',\n6         ]}\n7 ）\n```\n\n#### 1、setup.py的项目示例代码\n```\n1 #!/usr/bin/env python\n2 # coding=utf-8\n3\n4 from setuptools import setup\n5\n6 '''\n7 把redis服务打包成C:\\Python27\\Scripts下的exe文件\n8 '''\n9\n10 setup(\n11     name=\"RedisRun\",  #pypi中的名称，pip或者easy_install安装时使用的名称，或生成egg文件的名称\n12     version=\"1.0\",\n13     author=\"Andreas Schroeder\",\n14     author_email=\"andreas@drqueue.org\",\n15     description=(\"This is a service of redis subscripe\"),\n16     license=\"GPLv3\",\n17     keywords=\"redis subscripe\",\n18     url=\"https://ssl.xxx.org/redmine/projects/RedisRun\",\n19     packages=['RedisRun'],  # 需要打包的目录列表\n20\n21     # 需要安装的依赖\n22     install_requires=[\n23         'redis>=2.10.5',\n24         'setuptools>=16.0',\n25     ],\n26\n27     # 添加这个选项，在windows下Python目录的scripts下生成exe文件\n28     # 注意：模块与函数之间是冒号:\n29     entry_points={'console_scripts': [\n30         'redis_run = RedisRun.redis_run:main',\n31     ]},\n32\n33     # long_description=read('README.md'),\n34     classifiers=[  # 程序的所属分类列表\n35         \"Development Status :: 3 - Alpha\",\n36         \"Topic :: Utilities\",\n37         \"License :: OSI Approved :: GNU General Public License (GPL)\",\n38     ],\n39     # 此项需要，否则卸载时报windows error\n40     zip_safe=False\n\n```\n#### 2、简单setup.py的项目示例代码\n上边的setup.py比较复杂，参数较多，可根据自己的实际情况做删减，\n```\nimport setuptools\n\n#  打包: python setup.py sdist\n#  对包进行检查: twine check dist/*\n#  运行上传: twine upload dist/* --config-file .pypirc\n\n\n# 读取项目的readme介绍\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\nsetuptools.setup(\n    name=\"******SDK\",\n    version=\"1.0.2\",\n    author=\"******\",  # 项目作者\n    author_email=\"******@qq.com\",\n    description=\"This is the official Python SDK for Niffler Analytics.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"******\",\n    packages=setuptools.find_packages(),\n)\n\n```\n\n\n# 二、用Python将库打包发布到pypi\n\n如果需要将自己写好的python打包，并发布到pypi，这样其他人就可以直接通过pip install来安装对应的包，可以参考如下教程\n\n### 1. 注册pypi账号并创建token\n![](https://img.jbzj.com/file_images/article/202104/2021041314454555.png)\n然后选择API token->Add API token\n![](https://img.jbzj.com/file_images/article/202104/2021041314454556.png)\n\n输入token name并在Scope中选择Entire account（第一次需要选择Entire account）\n![](https://img.jbzj.com/file_images/article/202104/2021041314454557.png)\n然后在本地，修改`.pypirc`文件\n输入的内容为：\n```\n[pypi]\nusername = __token__\npassword = {token}\n```\n只需要修改`{token}`为自己的token即可\n\n### 2、打包\n打包命令为`python setup.py cmd`\ncmd可以取值为\n```\nbdist_wheel : create a wheel distribution\nbdist_egg : create an “egg” distribution\nsdist : create a source distribution (tarball, zip file, etc.)\nbdist : create a built (binary) distribution\nbdist_dumb : create a “dumb” built distribution\nbdist_rpm : create an RPM distribution\nbdist_wininst : create an executable installer for MS Windows\n```\n打包为tar.gz\n```\npython setup.py sdist\n```\n\n### 3、上传\n安装twine ` pip install twine`\n可以首先使用`twine`对包进行检查\n```\ntwine check dist/*\n```\n输出如下\n![](https://img.jbzj.com/file_images/article/202104/2021041314454560.png)\n\n再运行上传命令\n```\ntwine upload dist/*\n```\n数据注册好的pypi的用户名密码即可，如果在`.pypirc`文件中配置了用户名密码，也可以直接指定这个文件为配置文件\n\n可以用Makefile来自动运行打包上传\n```\nupload: ## 编译dockerbase\n\t@echo \"===========start...===========\"\n\trm -rf ./dist\n\trm -rf ./NifflerAnalyticsSDK.egg-info\n\t@echo \"===========python setup.py sdist...===========\"\n\tpython setup.py sdist\n\t@echo \"===========check...===========\"\n\ttwine check dist/*\n\t@echo \"===========upload...===========\"\n\ttwine upload dist/* --config-file .pypirc\n\t@echo \"===========finished...===========\"\n```\n","tags":["SDK","pypi"],"categories":["python"]},{"title":"docker的ulimit配置","url":"/2021/05/20/docker/docker ulimit 配置/","content":"\n### 突发报警\n\n半夜，业务服务器监控突然报警，报500，查看报警日志后发现是api中连接kafka报错了，查看kafka，发现docker容器在不断重启，报警内容是\n```\nERROR Error while accepting connection (kafka.network.Acceptor)\njava.io.IOException: No file descriptors available\n```\n\n发觉可能和 连接数 超过ulimit 限制\n\n查看服务器和容器和ulimit配置 ulimit -aH\n- 服务器参数\n```\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 30446\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 65535\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) unlimited\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n```\n- 容器参数\n```\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 30446\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 4096\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 10240\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) unlimited\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n```\n\n发现服务器的open files， 是65535，按理说是够用的，  但是容器的open files是 4096，觉得问题应该是出在这里了，因为最近kafka对接了新的日志输入，导致了再某个时间点多项业务链接叠加，导致连接数过大\n\n因为用的是aws服务器，aws服务器自己对容器的ulimit有限制，\n具体文档：https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/APIReference/API_Ulimit.html\n> The ulimit settings to pass to the container.\n\n> Amazon ECS tasks hosted on Fargate use the default resource limit values set by the operating system with the exception of the nofile resource limit parameter which Fargate overrides. The nofile resource limit sets a restriction on the number of open files that a container can use. The default nofile soft limit is 1024 and hard limit is 4096.\n\n查看docker 的配置\n* sudo vi /etc/sysconfig/docker\n发现默认ulimit和文档中写的一样\n```\n# The max number of open files for the daemon itself, and all\n# running containers.  The default value of 1048576 mirrors the value\n# used by the systemd service unit.\nDAEMON_MAXFILES=1048576\n\n# Additional startup options for the Docker daemon, for example:\n# OPTIONS=\"--ip-forward=true --iptables=true\"\n# By default we limit the number of open files per container\nOPTIONS=\"--default-ulimit nofile=1024:4096\"\n\n# How many seconds the sysvinit script waits for the pidfile to appear\n# when starting the daemon.\nDAEMON_PIDFILE_TIMEOUT=10\n```\n\n\n那么找到问题的所在，需要对容器的ulimit 进行修改\n\n```\nOPTIONS=\"--default-ulimit nofile=65535:65535\"\n```\n\n修改配置文件后需要加载然后重启docker\n```\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart docker\n```\n\n查看配置是否应用 systemctl status docker.service\n```\n● docker.service - Docker Application Container Engine\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled)\n   Active: active (running) since Thu 2021-05-20 10:07:40 UTC; 45s ago\n     Docs: https://docs.docker.com\n  Process: 23357 ExecStartPre=/usr/libexec/docker/docker-setup-runtimes.sh (code=exited, status=0/SUCCESS)\n  Process: 23345 ExecStartPre=/bin/mkdir -p /run/docker (code=exited, status=0/SUCCESS)\n Main PID: 23364 (dockerd)\n    Tasks: 29\n   Memory: 45.0M\n   CGroup: /system.slice/docker.service\n           ├─23364 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --default-ulimit nofile=65535:65535\n\n```\n确认成功！\n\n**这里是修改docker全局的配置，也可以只针对某个容器去修改，这样则需要在docker容器的配置文件中添加ulimits参数**\n\n### 我踩的坑\n最开始认为是服务器的ulimt导致的，所以想将 65535 设置成 unlimited\n```\n open files (-n) 65535\n```\n\n\n```\n# sudo vi /etc/security/limits.conf\n*    soft    nofile          unlimited\n*    hard    nofile          unlimited\n```\n\n结果发现，设置完成后  各种报错，sudo 无法使用,报警如下\n```\nsudo: pam_open_session: Permission denied\nsudo: policy plugin failed session\n```\n也无法登录服务器,报警如下\n```\nPermission denied (publickey,gssapi-keyex,gssapi-with-mic).\n```\n\n因为aws默认用户权限是ec2-user，不是root，什么也无法操作，没办法，只能重置服务器\naws 重置服务器的方法，重置后重新部署kafka服务\n\n```\naws重置服务器的方法\n实例-选择实例-操作-监控和故障排除-替换根卷\n\n```\n\n\n\n参考：\n[kafka集群连接不上问题解决过程](https://www.jianshu.com/p/627b29f2236f)\n[docker daemon 配置文件](https://www.jianshu.com/p/2556a1c5d45d)\n[一次修改limits.conf 引发的血案](https://blog.csdn.net/zimuKobby/article/details/97242965)\n","tags":["docker 配置","ulimit","容器"],"categories":["docker"]},{"title":"抓包工具-charles","url":"/2021/05/18/爬虫/Charles的使用/","content":"\n## 一 、Charles 的安装与使用\n\n### Charles官网：https://www.charlesproxy.com/\n\nCharles的作用：\n![image](https://user-images.githubusercontent.com/28568478/118580794-1728fd00-b7c3-11eb-810b-64204319df1d.png)\n\n### 1. 对请求抓包\n保证iPhone和PC工作在同一局域网内（即同一个WIFI下）。\n\n#### 1.1 获取PC端的IP地址\n![image](https://user-images.githubusercontent.com/28568478/118580840-2c059080-b7c3-11eb-925e-fcf375981e38.png)\n\n打开网络偏好设置...\n\n![image](https://user-images.githubusercontent.com/28568478/118580852-33c53500-b7c3-11eb-94ca-ae08bfd9227d.png)\n获取IP地址。\n\n#### 1.2 配置手机代理\n设置->无线局域网\n![image](https://user-images.githubusercontent.com/28568478/118580927-59523e80-b7c3-11eb-8b5f-0103a15cc260.png)\nHTTP代理->配置代理（手动）\n![image](https://user-images.githubusercontent.com/28568478/118580939-5ce5c580-b7c3-11eb-9876-39186efb944c.png)\n\n\n#### 1.3 PC端的Charles设置\n具体的端口可以从Charles菜单栏 -> Proxy -> Proxy Settings中查看。图示如下：\n![image](https://user-images.githubusercontent.com/28568478/118581127-a9310580-b7c3-11eb-85ee-32165789ea92.png)\n代理配置完成后，Charles会弹出连接提示框，点击Allow之后即可使用Charles对从该手机发出的请求进行抓包了。图示如下：\n![image](https://user-images.githubusercontent.com/28568478/118581179-c36ae380-b7c3-11eb-90ed-4f5dae97304f.png)\n\n\n### 2. 抓取HTTPS请求\n配置iPhone代理完毕后，就可以通过Structure或者Sequence窗口预览HTTP请求的数据了。但由于HTTPS请求被加密过，预览请求时只能预览到乱码数据。要预览到HTTPS请求的数据明文，需要PC和iPhone安装证书授权。\n\n#### 2.1 PC端安装证书\n在Charles菜单栏 -> Help -> SSL Proxying -> Install Charles Root Certificate中可以为PC安装证书，证书可以在Mac的钥匙串中查看。图示如下：\n![image](https://user-images.githubusercontent.com/28568478/118581241-e1384880-b7c3-11eb-8999-47c18149561c.png)\n\n\n证书可以在Mac的钥匙串中查看，双击证书，将信任权限设定为始终信任。图示如下：\n![image](https://user-images.githubusercontent.com/28568478/118581258-e7c6c000-b7c3-11eb-9c44-f8dfd864606b.png)\n\n#### 2.2 在手机端配置根证书\n##### 2.2.1 iPhone手机安装证书\n\n- 1、打开Charles，选择help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser\n    ![image](https://user-images.githubusercontent.com/28568478/118582584-2bbac480-b7c6-11eb-8ea9-3352629687aa.png)\n\n- 2、手机连接电脑代理，打开safari，输入网址：chls.pro/ssl\n选择后会显示IP与端口号，用于手机设置http代理：\n![image](https://user-images.githubusercontent.com/28568478/118582629-3d9c6780-b7c6-11eb-9bd6-7b6ccd44f28e.png)\n手机的网络上设置成电脑的http代理：\n此时必须保证手机和电脑在同一网络，并且手机可以访问电脑的ip与端口\n\n![image](https://user-images.githubusercontent.com/28568478/118582754-789e9b00-b7c6-11eb-9f79-44b426cda1ae.png)\n- 3、手机弹出提示：此网站正尝试打开“设置”已向您显示一个配置描述文件。您要允许吗？忽略|允许，选择允许，安装描述文件，并信任\n\n![image](https://user-images.githubusercontent.com/28568478/118582765-7d634f00-b7c6-11eb-94ff-81428d1e1573.png)\n- 4.1、iOS10.3以上的手机需要在：设置→ 通用 → 关于本机 → 证书信任设置→ 找到charles proxy CA证书，打开信任即可\n- 4.2、 手机浏览器（笔者使用Chrome）访问chls.pro/ssl，下载证书并安装(证书名任意)：\n![image](https://user-images.githubusercontent.com/28568478/118582782-8522f380-b7c6-11eb-8979-4b74a651fc99.png)\n\n在iPhone浏览器中访问http://charlesproxy.com/getssl可以下载证书并安装。\n基于iOS的证书信任机制，在安装完成并信任证书后，需要到iPhone设置 ->通用-> 关于本机 -> 证书信任设置中启用根证书。\n\n\n##### 2.2.2 安卓手机安装证书\n\n- 方法一：\n\n        1、打开Charles，选择help→SSL Proxying→Install Charles Root Certificate on a Mobile Device or Remote Browser\n\n        2、手机连接电脑代理，打开浏览器，输入网址：chls.pro/ssl\n\n        3、手机弹出提示：安装配置描述文件。您要允许吗？忽略|允许，选择允许，即可\n\n- 方法二：\n\n        1、打开Charles，选择help→SSL Proxying→Save Charles Certificate，将证书导入到手机中\n\n        2、导入后直接点击安装证书即可\n\n- 方法三：\n\n        1、打开Charles，选择help→SSL Proxying→Save Charles Certificate，将证书导入到手机中\n\n        2、导入后直接点击安装证书，提示无法打开\n\n        3、进入手机设置 → 更多设置 → 系统安全 → 从存储设备安装 → 选择charles.pem，点击高级，安装证书即可\n\n常见手机：小米手机，华为手机，需要设置手机锁屏密码\n\n##### 2.2.3 如果证书下载不下来怎么办？\n可根据安卓我手机安装证书的方法 将证书下载并导入手机进行安装\n\n\n#### 2.4 启动抓包的SSL权限\n在Charles菜单栏 -> Proxy -> SSL Proxying Settings中勾选Enable SSL Proxying，然后在Locations中添加需要抓包的Host和Port即可。图示如下：\n![image](https://user-images.githubusercontent.com/28568478/118581370-180e5e80-b7c4-11eb-9979-a322b1a9f39b.png)\n\n\n即对所有的https都进行抓包。\n\n\n## 二、Charles的破解方法\n### 破解网站：https://www.zzzmode.com/mytools/charles/\n\n#### 此工具用于计算Charles激活码\n输入RegisterName(此名称随意，用于显示 Registered to xxx)，点击生成计算出注册码，打开Charles输入注册码即可。\n![image](https://user-images.githubusercontent.com/28568478/118581701-a5ea4980-b7c4-11eb-85ee-31e28e3e9264.png)\n打开Charles菜单栏 -> help -> Register Charles\n填入上述产生的Registered Name和License Key，即可破解成功！\n![image](https://user-images.githubusercontent.com/28568478/118582091-5bb59800-b7c5-11eb-8c29-a556dd717469.png)\n破解成功\n![image](https://user-images.githubusercontent.com/28568478/118583990-8b19d400-b7c8-11eb-96ff-a152b36ea1ec.png)\n\n## 三、Android 7.0以上手机抓包方式（抓取https，亲测有用）\n安卓7.0有安全保护，用户级别的证书在大部分app中无法通过验证。（charles安装的证书就是用户级别的）\n\nAndroid7.0 之后默认不信任用户添加到系统的CA证书：\n>   To provide a more consistent and more secure experience across the Android ecosystem, beginning with Android Nougat, compatible devices trust only the standardized system CAs maintained in AOSP.（[文档链接](https://android-developers.googleblog.com/2016/07/changes-to-trusted-certificate.html)）\n\n也就是说对基于 SDK24 及以上的APP来说，即使你在手机上安装了抓包工具的证书也无法抓取 https 请求\n\n### 1、下载VritualXposed apk包安装到android手机\n\nhttps://github.com/android-hacker/VirtualXposed\n\n### 2、下载JustTrustMe apk包安装到android手机\n\nhttps://github.com/Fuzion24/JustTrustMe\n\n### 3、安装完成进入VirtualXposed apk应用，点击6个小点进入设置页面\n![image](https://user-images.githubusercontent.com/28568478/118583816-3fffc100-b7c8-11eb-91c3-7745b4d09681.png)\n### 4、进入设置页面，点击模块管理，勾选JustTrustMe（如果没有找到它，可能是这个版本不需要在手动选择了，安装之后自动识别到了）\n\n### 5、重启VirtualXposed\n\n### 6、重启之后我们重新进去设置页面，添加我们需要抓包的应用即可，比如网速管家APP\n\n![image](https://user-images.githubusercontent.com/28568478/118583877-5a399f00-b7c8-11eb-8e3c-0db4541e3516.png)\n![image](https://user-images.githubusercontent.com/28568478/118583888-6160ad00-b7c8-11eb-8d83-fbef18212e8b.png)\n\n\n### 7、设置Fiddler/charles代理，手机设置wifi里面代理改成手动。输入ip，端口：默认8888，注意手机和电脑在一个wifi下就可以了\n\n### 8、回到VirtualXposed 上滑解锁，打开我们安装的应用进行操作，则发现Fiddler/Charles已经成功抓取到安居客的HTTPS的数据包\n![image](https://user-images.githubusercontent.com/28568478/118584941-8a823d00-b7ca-11eb-8252-ba1e28dc6c1a.png)\n\n","tags":["爬虫","抓包","手机抓包","Charles"],"categories":["爬虫"]},{"title":"driver failed programming external connectivity on endpoint nginx","url":"/2021/05/16/Linux/driver failed programming external connectivity on endpoint nginx/","content":"\n今天在阿里云部署服务的时候报如下错：\n```\nroot@iZ2zeagh7rzys1hl53collZ:~/data/csh_bk/docker# docker-compose -f nginx/docker-compose.yml up\nCreating nginx ...\nCreating nginx ... error\n\nERROR: for nginx  Cannot start service nginx: driver failed programming external connectivity on endpoint nginx (84a2d44102b6fd874d94c52c199958bbb88214bf31890d9a66e264336f2a9794):  (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 80 -j DNAT --to-destination 172.20.0.2:80 ! -i br-90c5e5593694: iptables: No chain/target/match by that name.\n (exit status 1))\n\nERROR: for nginx  Cannot start service nginx: driver failed programming external connectivity on endpoint nginx (84a2d44102b6fd874d94c52c199958bbb88214bf31890d9a66e264336f2a9794):  (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 80 -j DNAT --to-destination 172.20.0.2:80 ! -i br-90c5e5593694: iptables: No chain/target/match by that name.\n (exit status 1))\nERROR: Encountered errors while bringing up the project.\n```\n\n1、本身部署服务需要按照Nginx  mysql redis\n2、安装完成后去阿里云设置安全组，打开对应的端口\n3、打开后发现mysql和redis都可以访问  但是Nginx无法访问\n4、后来发现是不小心动了防火墙，导致docker异常，重启docker服务，然后再重启对应服务即可\n\n```\nsystemctl restart docker\n```\n#### 原因:\ndocker服务启动时定义的自定义链DOCKER由于 centos7 firewall 被清掉\n\nfirewall的底层是使用iptables进行数据过滤，建立在iptables之上，这可能会与 Docker 产生冲突。\n\n当 firewalld 启动或者重启的时候，将会从 iptables 中移除 DOCKER 的规则，从而影响了 Docker 的正常工作。\n\n当你使用的是 Systemd 的时候， firewalld 会在 Docker 之前启动，但是如果你在 Docker 启动之后再启动 或者重启 firewalld ，你就需要重启 Docker 进程了。\n\n重启docker服务及可重新生成自定义链DOCKER\n\n\n参考：https://blog.csdn.net/liyanhui1001/article/details/107507847\n","tags":["nginx","docker","aliyun"],"categories":["Linux"]},{"title":"查找日志","url":"/2021/05/07/Linux web/查找日志/","content":"\n```\nfunction latest_log(){\n    for i in $(ls -th $1); do\n        if [[ -f $1$i ]]; then\n            log=$i\n            break;\n        fi\n    done\n    echo $log\n    tail -f $1$log\n}\n```\n","tags":["shell","日志"],"categories":["Linux web"]},{"title":"Unable to read additional data from client, it probably closed the socket","url":"/2021/05/07/异常问题/Unable to read additional data from client, it probab/","content":"今天在用docker-compose 部署Apache DolphinScheduler的时候在我的mac上报如下的错，但是服务器上是正常的，正常来讲用docker部署项目不应该出现这种问题，上网查了很多，说是初始化Zookeeper连接时，将接收超时参数值调整大一些即可（tickTime2000改为10000），默认是毫秒（ms），但是还是不行，后来看到一篇文章说可能是docker内存不够的关系，后来将docker内存从2G跳到了4G好了，但感觉不太像是这个问题，所以记录一下\n\n```\n2021-02-27 19:21:26,435 [myid:1] - WARN  [NIOWorkerThread-1:NIOServerCnxn@364] - Unexpected exception\nEndOfStreamException: Unable to read additional data from client, it probably closed the socket: address = /172.21.0.5:39431, session = 0x0\n\tat org.apache.zookeeper.server.NIOServerCnxn.handleFailedRead(NIOServerCnxn.java:163)\n\tat org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:326)\n\tat org.apache.zookeeper.server.NIOServerCnxnFactory$IOWorkRequest.doWork(NIOServerCnxnFactory.java:522)\n\tat org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:154)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n```\n","tags":["docker"],"categories":["异常问题"]},{"title":"nginx an upstream response is buffered to a temporary file","url":"/2021/05/07/异常问题/nginx an upstream response is buffered to a temporary file/","content":"查看Nginx日志，发现有很多报错\n```\n2021/02/27 17:00:54 [warn] 28#28: *49939\nan upstream response is buffered to a temporary file /var/cache/nginx/proxy_temp/6/14/0000000146 while reading upstream, client: xxx.xxx.xxx.x,\nserver: airflow.yidianshihui.com, request: \"GET /home HTTP/1.1\", upstream: \"http://xxx.xxx.xxx.xxx:xxxx/home\", host: \"airflow.yidianshihui.com\",\nreferrer: \"http://airflow.yidianshihui.com/home?tags=%E6%AF%8F%E6%97%A5ic_sku%E5%90%8C%E6%AD%A5\"\n```\n\n\n\n### 1.错误日志：warn：an upstream response is buffered to a temporary file\n\n    解决办法：增加fastcgi_buffers      8 4K;     fastcgi_buffer_size  4K;\n#### 2. a client request body is buffered to a temporary file\n\n解决办法：增加client_max_body_size 2050m;     client_body_buffer_size 1024k;\n<!--more-->\n\n###Nginx 的 buffer 机制：\n\n\n对于来自 FastCGI Server 的 Response，Nginx 将其缓冲到内存中，然后依次发送到客户端浏览器。缓冲区的大小由 fastcgi_buffers 和 fastcgi_buffer_size 两个值控制。\n\n比如如下配置：\n ```\nfastcgi_buffers      8 4K;\nfastcgi_buffer_size  4K;\n```\nfastcgi_buffers 控制 nginx 最多创建 8 个大小为 4K 的缓冲区，而 fastcgi_buffer_size 则是处理 Response 时第一个缓冲区的大小，不包含在前者中。所以总计能创建的最大内存缓冲区大小是 8*4K+4K = 36k。而这些缓冲区是根据实际的 Response 大小动态生成的，并不是一次性创建的。比如一个 8K 的页面，Nginx 会创建 2*4K 共 2 个 buffers。\n\n当 Response 小于等于 36k 时，所有数据当然全部在内存中处理。如果 Response 大于 36k 呢？fastcgi_temp 的作用就在于此。多出来的数据会被临时写入到文件中，放在这个目录下面。同时你会在 error.log 中看到一条类似 warning：\n```\n2010/03/13 03:42:22 [warn] 3994#0: *1 an upstream response is buffered to a temporary file\n/usr/local/nginx/fastcgi_temp/1/00/0000000001 while reading upstream,\nclient: 192.168.1.111,\nserver: www.xxx.cn,\nrequest: \"POST /test.php HTTP/1.1\",\nupstream: \"fastcgi://127.0.0.1:9000\",\nhost: \"xxx.cn\",\nreferrer: \"http://xxx.cn/test.php\"\n```\n显然，缓冲区设置的太小的话，Nginx 会频繁读写硬盘，对性能有很大的影响，但也不是越大越好，没意义\n\n官方文档：\n[http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size](http://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size) \n\n修改Nginx配置：\n```\nlocation /api {\n        proxy_set_header  Host  $http_host;\n        proxy_set_header  X-Real-IP  $remote_addr;\n        proxy_set_header  X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_pass   http://elk_server/api;\n        client_body_buffer_size 1024k;\n        client_max_body_size 2050m;\n        fastcgi_buffers 8 4K;\n        fastcgi_buffer_size 4K;\n    }\n```\n\n参考：\n[Nginx性能调优之buffer参数设置](https://blog.csdn.net/zhouyan8603/article/details/89410119)\n [nginx常见问题](https://my.oschina.net/linland/blog/373315)\n","tags":["nginx"],"categories":["异常问题"]},{"title":"docker mysql Could not write unix socket lock file /var/run/mysqld/mysqld.sock.lock","url":"/2021/05/07/数据库/docker mysql Could not write unix socket lock file/","content":"昨天在用docker部署mysql的时候一直报错\n```\n2020-06-23T09:59:47.339758Z 0 [Note] Skipping generation of SSL certificates as certificate files are present in data directory.\n2020-06-23T09:59:47.340593Z 0 [Warning] CA certificate ca.pem is self signed.\n2020-06-23T09:59:47.340636Z 0 [Note] Skipping generation of RSA key pair as key files are present in data directory.\n2020-06-23T09:59:47.340852Z 0 [ERROR] Could not create unix socket lock file /var/run/mysqld/mysqld.sock.lock.\n2020-06-23T09:59:47.340867Z 0 [ERROR] Unable to setup unix socket lock file.\n2020-06-23T09:59:47.340872Z 0 [ERROR] Aborting\n```\n\n解决方法：\n这里可以看到mysqld.sock的目录是在/var/run/mysqld目录下，但是这个目录，我们并没有挂载主机目录，下面我们重新运行mysql容器，挂载相应的容器\n```\n volumes:\n      - /data/mysql-data:/var/lib/mysql\n      - ./init:/docker-entrypoint-initdb.d/\n      - /data/mysql-data/mysqld:/var/run/mysqld\n```\n\n参考：\n[docker安装mysql及相关配置、运行细节和常见报错解决方案](https://zhangxueliang.blog.csdn.net/article/details/111266278)\n","tags":["docker","mysql"],"categories":["数据库"]},{"title":"mac 相关命令","url":"/2021/05/07/mac/mac 相关命令/","content":"### 复制\nditto\n```\n ditto -V -x -k --sequesterRsrc --rsrc xxxx-img-1.zip xxxx-img-1\n```\n\n### 切割\nsplit\n```\nsplit -b 102400000   Joker.小丑.2019.中英字幕.WEBrip.720P-人人影视.mp4\n```\n","tags":["命令"],"categories":["mac"]},{"title":"mac 命令行终端 设置代理","url":"/2021/05/07/mac/mac 命令行终端 设置代理/","content":"## 环境：\n\n*   macOS Mojave 10.14.3\n*   iTrem 2 3.2.8\n*   酸酸乳1.1.4.4-R8\n\n查看自己命令行的状态\n\n```\ncurl ip.gs\n```\n\n## 正式开始\n\n### 一、首先检查自己的酸酸乳是否正常，并在高级设置页面，找到本地 Sock5 监听端口，我自己是 1086，注意这里还有一个 HTTP 代理监听端口，不是这个\n\n![image](https://user-images.githubusercontent.com/28568478/117410597-3adb8000-af45-11eb-96e1-0fde94cea9b3.png)\n\n###  二、安装需要的软件（没有 brew 就先安装 brew，具体自行搜索哈）\n\n```\nbrew install privoxy\n```\n### 三、配置 privoxy\n\n```\nvim /usr/local/etc/privoxy/config\n```\n\n在里面插入两行，\n\n```\nlisten-address 0.0.0.0:8118\nforward-socks5 / localhost:1086 .\n```\n\n 注意！！！ 这里的 1086 就是刚才上面圈出来的端口，千万不要弄错了，弄错了就用不了的\n\n### 四、启动 prioxy\n```\nsudo /usr/local/sbin/privoxy /usr/local/etc/privoxy/config\n```\n\n查看是否成功，输入命令\n\n\n```\nnetstat -na | grep 8118\n```\n\n看到有类似如下信息就表示启动成功了 \n\n```\ntcp4 0 0 *.8118 *.* LISTEN\n```\n\n### 五、最后一步，方便使用\n\n`在 ~/.zshrc` 里加入开关函数，使用起来更方便\n\n```\nvim ~/.zshrc\n```\n\n在里面插入\n\n```\nfunction proxy_off(){\n    unset http_proxy\n    unset https_proxy\n    echo -e \"已关闭代理\"\n}\n\nfunction proxy_on() {\n    export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com\"\n    export http_proxy=\"http://127.0.0.1:8118\"\n    export https_proxy=$http_proxy\n    echo -e \"已开启代理\"\n}\n```\n再 source 一下\n\n```\nsource  ~/.zshrc\n```\n\n然后就可以通过在命令行输入 proxy_on 和 proxy_off 来开启和关闭代理啦\n\n### 六、日常开启 prioxy 软件\n\n```\nbrew services start privoxy\n```\n### 七、日常使用步骤\n```\n先启动 provixy 软件\nbrew services start privoxy\n然后 启动代理\nproxy_on\n```\n\n最后在看看自己的命令行状态吧\n\n```\ncurl ip.gs\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117410679-53e43100-af45-11eb-8193-cda84662c914.png)\n\n\n well done！\n","tags":["代理"],"categories":["mac"]},{"title":"mac rz sz 在服务器中上传下载文件","url":"/2021/05/07/mac/mac rz sz 在服务器中上传下载文件/","content":"\n# [mac rz sz 在服务器中上传下载文件](https://www.cnblogs.com/jellyabd/p/11388041.html)\n\n主要参考：[https://segmentfault.com/a/1190000012166969](https://segmentfault.com/a/1190000012166969)\n\n安装步骤参考：[https://github.com/aikuyun/iterm2-zmodem](https://github.com/aikuyun/iterm2-zmodem)\n\n**安装步骤**\n\n1.安装支持rz和sz命令的lrzsz：brew install lrzsz\n\n> 等了挺长时间的。\n\n2.在本地/usr/local/bin/目录下保存iterm2-send-zmodem.sh 和iterm2-recv-zmodem.sh两个脚本\n\niterm2-send-zmodem.sh\n<!--more-->\n\n```\n\n#!/bin/bash\n# Author: Matt Mastracci (matthew@mastracci.com)\n# AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script\n# licensed under cc-wiki with attribution required\n# Remainder of script public domain\n\nosascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm\nif [[ $NAME = \"iTerm\" ]]; then\n    FILE=`osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"`\nelse\n    FILE=`osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose file with prompt \"Choose a file to send\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"`\nfi\nif [[ $FILE = \"\" ]]; then\n    echo Cancelled.\n    # Send ZModem cancel\n    echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18\n    sleep 1\n    echo\n    echo \\# Cancelled transfer\nelse\n    /usr/local/bin/sz \"$FILE\" -e -b\n    sleep 1\n    echo\n    echo \\# Received $FILE\nfi\n```\n\n\niterm2-recv-zmodem.sh\n\n```\n#!/bin/bash\n# Author: Matt Mastracci (matthew@mastracci.com)\n# AppleScript from http://stackoverflow.com/questions/4309087/cancel-button-on-osascript-in-a-bash-script\n# licensed under cc-wiki with attribution required\n# Remainder of script public domain\n\nosascript -e 'tell application \"iTerm2\" to version' > /dev/null 2>&1 && NAME=iTerm2 || NAME=iTerm\nif [[ $NAME = \"iTerm\" ]]; then\n    FILE=`osascript -e 'tell application \"iTerm\" to activate' -e 'tell application \"iTerm\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"`\nelse\n    FILE=`osascript -e 'tell application \"iTerm2\" to activate' -e 'tell application \"iTerm2\" to set thefile to choose folder with prompt \"Choose a folder to place received files in\"' -e \"do shell script (\\\"echo \\\"&(quoted form of POSIX path of thefile as Unicode text)&\\\"\\\")\"`\nfi\n\nif [[ $FILE = \"\" ]]; then\n    echo Cancelled.\n    # Send ZModem cancel\n    echo -e \\\\x18\\\\x18\\\\x18\\\\x18\\\\x18\n    sleep 1\n    echo\n    echo \\# Cancelled transfer\nelse\n    cd \"$FILE\"\n    /usr/local/bin/rz -E -e -b\n    sleep 1\n    echo\n    echo\n    echo \\# Sent \\-\\> $FILE\nfi\n```\n\n3.设置一下两个脚本的权限，一般 chmod 777 就行了\n\n```\nchmod 777 /usr/local/bin/iterm2-*\n```\n\n4.设置Iterm2的Tirgger特性，profiles->default->editProfiles->Advanced中的Tirgger\n\n添加两条trigger，分别设置 Regular expression，Action，Parameters，Instant如下：\n\n1.第一条\n\n        Regular expression: \\*\\*B0100\n\n        Action: Run Silent Coprocess\n\n        Parameters: /usr/local/bin/iterm2-send-zmodem.sh\n\n2.第二条\n\n        Regular expression: \\*\\*B00000000000000\n\n        Action: Run Silent Coprocess\n\n        Parameters: /usr/local/bin/iterm2-recv-zmodem.sh\n\n**需要注意的点：**\n\n# 使用expect自动登录后，不能使用rz和sz命令问题\n\n    脚本开头可以加入：export LC_CTYPE=en_US\n```\n对/usr/local/bin的\n *   iterm2-recv-zmodem.sh\n *  iterm2-send-zmodem.sh\n脚本添加权限\n```\n\n            chmod 777 iterm2-*\n\n*   iterm2 的设置路径为：\n\n            点击 iTerm2 的设置界面 Perference-> Profiles -> Default -> Advanced -> Triggers 的 Edit 按钮\n\n*   iterm2的triggers截图\n\n            1.第一行receive，只需要“**B0100”的内容，不要前面的那一串说明。\n\n            2.“**B0100”对应的是send\n\n![image](https://user-images.githubusercontent.com/28568478/117410212-b5f06680-af44-11eb-9282-2eb8dbea1b44.png)\n","tags":["macOS","rz sz"],"categories":["mac"]},{"title":"docker 命令","url":"/2021/05/07/docker/docker 命令/","content":"\n# 一、关于镜像\n\n### 1、删除所有镜像\n```\ndocker rmi `docker images -q`\n```\n\n### 2、按条件删除镜像\n\n- 没有打标签\n```\ndocker rmi `docker images -q | awk '/^<none>/ { print $3 }'`\n```\n- 镜像名包含关键字\n```\ndocker rmi --force `docker images | grep doss-api | awk '{print $3}'`    //其中doss-api为关键字\n```\n# 二、关于容器\n\n ### 1、查看运行容器\n```\ndocker ps\n```\n\n### 2、查看所有容器 和 列出所有容器ID\n```\n# 查看所有容器\ndocker ps -a\n\n# 列出所有容器ID\ndocker ps -aq\n\n```\n\n### 3、进入容器\n\n- 其中字符串为容器ID:\n\n```\ndocker exec -it d27bd3008ad9 /bin/bash\n```\n\n### 4、停用全部运行中的容器:\n\n```\ndocker stop $(docker ps -q)\n```\n\n### 5、删除全部容器：\n\n```\ndocker rm $(docker ps -aq)\n```\n\n### 6、一条命令实现停用并删除容器：\n\n```\ndocker stop $(docker ps -q) & docker rm $(docker ps -aq)\n```\n# 三、其他命令\n具体详见参考文档\n```\n查看 docker 占用的资源\n在进行资源清理之前我们有必要搞清楚 docker 都占用了哪些系统的资源。这需要综合使用不同的命令来完成。\ndocker container ls：默认只列出正在运行的容器，-a 选项会列出包括停止的所有容器。\ndocker image ls：列出镜像信息，-a 选项会列出 intermediate 镜像(就是其它镜像依赖的层)。\ndocker volume ls：列出数据卷。\ndocker network ls：列出 network。\ndocker info：显示系统级别的信息，比如容器和镜像的数量等。\n\n\n\nDocker 提供了方便的 docker system prune 命令来删除那些已停止的容器、dangling 镜像、未被容器引用的 network 和构建过程中的 cache：\ndocker system prune\n安全起见，这个命令默认不会删除那些未被任何容器引用的数据卷，如果需要同时删除这些数据卷，你需要显式的指定 --volumns 参数。比如你可能想要执行下面的命令：\n\n$ docker system prune --all --force --volumes\n这次不仅会删除数据卷，而且连确认的过程都没有了！注意，使用 --all 参数后会删除所有未被引用的镜像而不仅仅是 dangling 镜像。\n\n\n```\n\n\n参考：https://www.cnblogs.com/sparkdev/p/9177283.html\n","tags":["docker 命令"],"categories":["docker"]},{"title":"/var/lib/docker/overlay2 占用很大，清理Docker占用的磁盘空间，迁移 /var/lib/docker 目录","url":"/2021/05/07/docker/清理Docker占用的磁盘空间/","content":"#### 0、  du -hs /var/lib/docker/ 命令查看磁盘使用情况。\n```\nlinlf@dacent:~$ sudo du -hs /var/lib/docker/\n237G\t/var/lib/docker/\n```\n#### 1 docker system df命令，类似于Linux上的df命令，用于查看Docker的磁盘使用情况:\n```\nlinlf@dacent:~$ docker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              7                   2                   122.2GB             79.07GB (64%)\nContainers          2                   2                   61.96GB             0B (0%)\nLocal Volumes       0                   0                   0B                  0B\nBuild Cache         0                   0                   0B                  0B\n```\n#### 2 docker system prune命令可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)。\n```\nlinlf@dacent:~$ docker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N] y\nTotal reclaimed space: 0B\n```\n#### 3 docker system prune -a命令清理得更加彻底，可以将没有容器使用Docker镜像都删掉。注意，这两个命令会把你暂时关闭的容器，以及暂时没有用到的Docker镜像都删掉了…所以使用之前一定要想清楚.。我没用过，因为会清理 没有开启的  Docker 镜像。\n\n \n\n#### 4 迁移 /var/lib/docker 目录。\n\n- 4.1 停止docker服务。\n```\nsystemctl stop docker\n```\n- 4.2 创建新的docker目录，执行命令df -h,找一个大的磁盘。 我在 /home目录下面建了 /home/docker/lib目录，执行的命令是：\n```\nmkdir -p /home/docker/lib\n```\n- 4.3 迁移/var/lib/docker目录下面的文件到 /home/docker/lib：\n```\nrsync -avz /var/lib/docker /home/docker/lib/\n```\n- 4.4 配置 /etc/systemd/system/docker.service.d/devicemapper.conf。查看 devicemapper.conf 是否存在。如果不存在，就新建。\n```\nsudo mkdir -p /etc/systemd/system/docker.service.d/\nsudo vi /etc/systemd/system/docker.service.d/devicemapper.conf\n```\n- 4.5 然后在 devicemapper.conf 写入：（同步的时候把父文件夹一并同步过来，实际上的目录应在 /home/docker/lib/docker ）\n```\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd  --graph=/home/docker/lib/docker\n```\n- 4.6 重新加载 docker\n```\nsystemctl daemon-reload\n\nsystemctl restart docker\n\nsystemctl enable docker\n```\n- 4.7 为了确认一切顺利，运行\n```\n# docker info\n```\n命令检查Docker 的根目录.它将被更改为 /home/docker/lib/docker\n\n```\nDocker Root Dir: /home/docker/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\n\n```\n\n- 4.8 启动成功后，再确认之前的镜像还在：\n```\nlinlf@dacent:~$ docker images\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\nAAA/AAA               v2                  7331b8651bcc        27 hours ago        3.85GB\nBBB/BBB               v1                  da4a80dd8424        28 hours ago        3.47GB\n```\n- 4.9 确定容器没问题后删除/var/lib/docker/目录中的文件。\n\n \n\n参考链接：\nhttps://blog.csdn.net/qq_37674858/article/details/79976751\n\nhttps://blog.csdn.net/cmrsautomation/article/details/52857791\n\n\n原文链接：https://blog.csdn.net/weixin_32820767/article/details/81196250\n","tags":["docker","overlay2","清理"],"categories":["docker"]},{"title":"aws 安装docker","url":"/2021/05/07/docker/aws 安装docker/","content":"\n1、[aws官方教程](https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/docker-basics.html)\n\n2、\n```\nsudo yum update -y\n```\n发现报错\n![image](https://user-images.githubusercontent.com/28568478/117409227-8a20b100-af43-11eb-977b-29b69dc8895e.png)\n\n\n安装提示进行解决\n```\nsudo yum-config-manager --disable docker-ce-stable\n```\n<!--more-->\n\n3、安装官方文档进行安装docker 启动docker\n```\nsudo yum update -y\n\nsudo amazon-linux-extras install docker\n\n/bin/systemctl start docker.service\n# 原本使用命令 sudo service docker start   实际提示  Redirecting to /bin/systemctl start docker.service\n\nsudo usermod -a -G docker ec2-user 或者 sudo gpasswd -a ${USER} docker  都可以\n\n# 然后退出服务器重新登录服务器\n\ndocker info # 发现docker安装完成\n```\n\n\n4、 docker-compose 安装\n[docker-compose github](https://github.com/docker/compose)\n```\nsudo curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose\n\n# 安装最新版本\nsudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose\n\n# 修改权限\nsudo chmod +x /usr/local/bin/docker-compose\n\n# 验证是否安装成功\ndocker-compose version\n\n```\n\n#### 注意 ：\n          如果不退出服务器再次登录  docker info 会报错无权限\n![](https://upload-images.jianshu.io/upload_images/5818745-630d6164d4278a6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n\n\n### 参考文档：\nhttps://gist.github.com/npearce/6f3c7826c7499587f00957fee62f8ee9\n","tags":["docker","aws"],"categories":["docker"]},{"title":"mac docker","url":"/2021/05/07/docker/mac docker/","content":"### 安装命令\n```\nbrew cask install docker\n```\n![image](https://user-images.githubusercontent.com/28568478/117409037-4b8af680-af43-11eb-8584-074f05051f51.png)\n\n\n在启动台点击Docker app, 在载入 Docker app 后，点击 Next，可能会询问你的 macOS 登陆密码，你输入即可。之后会弹出一个 Docker 运行的提示窗口，状态栏上也有有个小鲸鱼的图标\n（![image](https://user-images.githubusercontent.com/28568478/117409069-56de2200-af43-11eb-94bf-a3abda66bea4.png)）。\n\n\n###  Docker app 启动命令\n安装完成后会自启动 如果没有启动  可以用这个命令\n```\ndocker run -d -p 80:80 docker/getting-started\n```\n![image](https://user-images.githubusercontent.com/28568478/117409079-59d91280-af43-11eb-960b-0fcc22502921.png)\n\n","tags":["docker","macOS"],"categories":["docker"]},{"title":"Go struct 类型的 map 结构体成员不能修改的问题","url":"/2021/05/07/golang/Go struct 类型的 map 结构体成员不能修改的问题/","content":"\n\n\n引入： 错误 Reports assignments directly to a struct field of a map\n\n![image](https://user-images.githubusercontent.com/28568478/117408509-a3752d80-af42-11eb-9267-d8146f2ff28b.png)\n\n\n### 1. 问题的产生\n\n这个问题在github上可以追溯到2012年提交的一个issue，链接为 [https://github.com/golang/go/issues/3117](https://github.com/golang/go/issues/3117) ；如上图，结构体作为map的元素时，不能够直接赋值给结构体的某个字段，也就是map中的struct中的字段不能够直接寻址。\n\n<!--more-->\n\n### 2. 问题产生的原因\n\n关于golang中map的这种古怪的特性有这样几个观点：\n\n1）map作为一个封装好的数据结构，由于它底层可能会由于数据扩张而进行迁移，所以拒绝直接寻址，避免产生野指针；\n\n2）map中的key在不存在的时候，赋值语句其实会进行新的k-v值的插入，所以拒绝直接寻址结构体内的字段，以防结构体不存在的时候可能造成的错误；\n\n3）这可能和map的并发不安全性相关\n\n\n- x = y 这种赋值的方式，你必须知道 x的地址，然后才能把值 y 赋给 x。\n- 但 go 中的 map 的 value 本身是不可寻址的，因为 map 的扩容的时候，可能要做 key/val pair迁移\n- value 本身地址是会改变的\n- 不支持寻址的话又怎么能赋值呢\n\n### 3. 问题的解决\n\n1）迂回方式一：整体更新map的value部分\n\n```\npackage main\n\nimport \"fmt\"\n\ntype Person struct{\n\tname string\n\tsex string\n\tage int\n}\n\nfunc main(){\n\tm := map[uint]Person{\n\t\t0 : Person{\"张无忌\", \"男\", 18},\n\t\t1 : Person{\"周芷若\", \"女\", 17},\n\t}\n\n\t//m[0].age += 1\n\t//整体更新结构体\n\ttemp := m[0]\n\ttemp.age += 1\n\tm[0] = temp\n\tfmt.Println(m)\n}\n```\n\n运行结果：\n\n![image](https://user-images.githubusercontent.com/28568478/117408510-a3752d80-af42-11eb-8f10-e860eb41f4d0.png)\n\n\n2） 迂回方式二：把map的value部分定义为对应类型的指针类型或是slice或是map时，这样是可以更新v的内部字段的\n\n```\npackage main\n\nimport \"fmt\"\n\ntype Person struct{\n\tname string\n\tsex string\n\tage int\n}\n\nfunc main() {\n\t//定义map的value类型为指针类型\n\tm := map[uint]*Person{\n\t\t0: &Person{\"张无忌\", \"男\", 18},\n\t\t1: &Person{\"周芷若\", \"女\", 17},\n\t}\n\n\tm[0].age += 1\n\n\tfmt.Println(*m[0])\n}\n```\n\n运行结果：\n\n![image](https://user-images.githubusercontent.com/28568478/117408536-aa03a500-af42-11eb-9737-32d0b54feb84.png)\n\n\n\n原文链接：[golang 结构体作为map的元素时，不能够直接赋值给结构体的某个字段](https://blog.csdn.net/zhngcho/article/details/82424962)\n","tags":["struct"],"categories":["golang"]},{"title":"MAC brew update镜像源切换","url":"/2021/05/07/mac/MAC brew update镜像源切换/","content":"\n### 一、查看当前Homebrew 镜像源\n\n- 方法1\n```\nbrew config\n# 查看现在镜像源\n# ORIGIN:https://github.com/Homebrew/brew.git 之前未操作过则会显示这个链接，也就是官方镜像源\n```\n<!--more-->\n- 方法2\n```\n# brew.git镜像源\ngit -C \"$(brew --repo)\" remote -v\n\n# homebrew-core.git镜像源\ngit -C \"$(brew --repo homebrew/core)\" remote -v\n\n# homebrew-cask.git镜像源\ngit -C \"$(brew --repo homebrew/cask)\" remote -v\n```\n\n### 二、替换源\n\n#####  国内镜像地址\n\n*   科大: [https://mirrors.ustc.edu.cn](https://link.zhihu.com/?target=https%3A//links.jianshu.com/go%3Fto%3Dhttps%253A%252F%252Fmirrors.ustc.edu.cn)\n*   阿里: [https://mirrors.aliyun.com/homebrew/](https://link.zhihu.com/?target=https%3A//links.jianshu.com/go%3Fto%3Dhttps%253A%252F%252Fmirrors.aliyun.com%252Fhomebrew%252F)\n- 方法1\n```\n# 替换 brew.git\ncd \"$(brew --repo)\"\ngit remote set-url origin https://mirrors.ustc.edu.cn/brew.git\n\n# 替换 homebrew-core.git\ncd \"$(brew --repo)/Library/Taps/homebrew/homebrew-core\"\ngit remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git\n```\n- 方法2\n```\ngit -C \"$(brew --repo)\" remote set-url origin https://mirrors.ustc.edu.cn/brew.git\n\ngit -C \"$(brew --repo homebrew/core)\" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.git\n\ngit -C \"$(brew --repo homebrew/cask)\" remote set-url origin https://mirrors.ustc.edu.cn/homebrew-cask.git\n\n```\n- 方法3\n```\nif [ $SHELL = \"/bin/bash\" ] # 如果你的是bash\nthen\n    echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/' >> ~/.bash_profile\n    source ~/.bash_profile\nelif [ $SHELL = \"/bin/zsh\" ] # 如果用的shell 是zsh 的话\nthen\n    echo 'export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.ustc.edu.cn/homebrew-bottles/' >> ~/.zshrc\n    source ~/.zshrc\nfi\n```\n\n\n### 三、查看是否替换成功\n```\nbrew config\n#查看更换后的镜像源(ORIGIN: https://mirrors.ustc.edu.cn/brew.git 说明更换成功)\n```\n如果替换成功后，进行brew update就可以了\n\n\n### 四、恢复原有镜像源的方法\n- 如果需要恢复原有镜像源的话（国内镜像源突然不能用了或版本不够新）\n```\ngit -C \"$(brew --repo)\" remote set-url origin https://github.com/Homebrew/brew.git\n\ngit -C \"$(brew --repo homebrew/core)\" remote set-url origin https://github.com/Homebrew/homebrew-core.git\n\ngit -C \"$(brew --repo homebrew/cask)\" remote set-url origin https://github.com/Homebrew/homebrew-cask.git\n\n# 找到 ~/.bash_profile 或者 ~/.zshrc 中的HOMEBREW_BOTTLE_DOMAIN 一行删除\n\nbrew update\n```\n\n参考：\n-  [mac brew update 卡着不动的问题](https://www.cnblogs.com/demingblog/p/11436602.html)\n\n- [Mac Homebrew 国内镜像源替换或重置（brew update 没反应）](https://zhuanlan.zhihu.com/p/102760018)\n\n- [Mac安装Homebrew并更换国内镜像源](https://www.cnblogs.com/StivenYang/p/12546605.html)\n","tags":["macOS","镜像源"],"categories":["mac"]},{"title":"解决MAC删除应用程序后依然残留的图标","url":"/2021/05/07/mac/解决MAC删除应用程序后依然残留的图标/","content":"在MAC，有时候删除应用程序后，发现 应用台 仍有该应用的图标\n\n## 删除方法：\n\n在终端输入\n```\nsqlite3 $(find /private/var/folders \\( -name com.apple.dock.launchpad -a -user $USER \\) 2> /dev/null)/db/db \"DELETE FROM apps WHERE title='TeamViewer';\" && killall Dock\n```\n我这个是删除TeamViewer,如果要删除其他的，改成相应应用程序的名字即可\n注意大小写\n","tags":["macOS","图标"],"categories":["mac"]},{"title":"证书的生成","url":"/2021/05/07/Linux web/证书的生成/","content":"我们通过-in参数指定传入的文件名称,而-out文件指定输出的文件名称,而-nodes参数表示不对私钥进行加密\n\n### 方法1：\n\n生成cert:\n```\nopenssl pkcs12 -clcerts -nokeys -out cert.pem  -in  cert.p12\n```\n生成key:\n\n```\nopenssl pkcs12 -nocerts -out key.pem  -in  key.p12\n```\n\n取消设置key时的密码:\n```\nopenssl rsa -in  key.pem  -out key.unencrypted.pem\n```\n\n### 方法2（常用方法）：\n\n默认没有密码生成cert:\n```\nopenssl pkcs12 -in   证书.p12    -nokeys  -out   cert.pem  -nodes\n```\n生成key:\n```\nopenssl pkcs12   -in  证书.p12   -nocerts -out key.pem   -nodes\n```\n注意：解压证书时万不可设置密码\n\n在终端测试证书是否可正常使用\n```\nopenssl   s_client  -connect   gateway.push.apple.com:2195  -cert  cert.pem -key   key.pem\n```\n","tags":["证书"],"categories":["Linux web"]},{"title":"pycharm如何配置pytest","url":"/2021/05/07/tools/pycharm如何配置pytest/","content":"\nMac下\nPyCharm Community Edition-->Preference-->Tools-->Python Integrated Tools， 把Default test runner换为pytest就可以了\n\n\n![image](https://user-images.githubusercontent.com/28568478/117407710-81c77680-af41-11eb-8ff3-29540ac4720e.png)\n\n正常情况下如下图\n![image](https://user-images.githubusercontent.com/28568478/117407724-8724c100-af41-11eb-8c2a-7d2f3426c78d.png)\n\n![image](https://user-images.githubusercontent.com/28568478/117407753-9441b000-af41-11eb-9ae6-4d1be9743971.png)\n\n\n\n如果无法正常使用，则可能你最开始用unittest进行了测试  则需要按照下图删除使用过的测试文件即可\n![image](https://user-images.githubusercontent.com/28568478/117407807-aa4f7080-af41-11eb-9a1c-5a09c623d1f6.png)\n![image](https://user-images.githubusercontent.com/28568478/117407815-ad4a6100-af41-11eb-84f4-7794dd0d8887.png)\n\n","tags":["python","pycharm"],"categories":["工具"]},{"title":"mac 安装 zookeeper kafka","url":"/2021/05/07/异步任务/mac 安装 zookeeper kafka/","content":"# 1、安装JSK\n\n# 2、安装zookeeper\n\n```\nbrew install zookeeper\n\n// 后台启动\nbrew services start zookeeper\n\n// 非后台启动\nzkServer start\n\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117407463-2c8b6500-af41-11eb-8d22-2eb8ea34bd70.png)\n\n\n# 3、安装kafka\n```\nbrew install kafka\n\n// 后台启动\nbrew services start kafka\n\n// 非后台启动\nzookeeper-server-start /usr/local/etc/kafka/zookeeper.properties & kafka-server-start /usr/local/etc/kafka/server.properties\n```\n![image](https://user-images.githubusercontent.com/28568478/117407483-33b27300-af41-11eb-8dc3-b34985a1527c.png)\n\n\n","tags":["macOS","kafka"],"categories":["异步任务"]},{"title":"kafka-python 使用","url":"/2021/05/07/异步任务/kafka-python/","content":"\n**一：kafka producer**\n\n　\n```\nfrom kafka import KafkaProducer\nfrom kafka.errors import kafka_errors\nimport traceback\n\njson_list = {\"test\": 1}\nproducer = KafkaProducer(\n    bootstrap_servers=['kafka.aws.bigdata.yidianshihui.com:9092'],\n    # key和value序列化\n    key_serializer=lambda k: json.dumps(k).encode(),\n    value_serializer=lambda v: json.dumps(v).encode()\n    )\n\nfuture = producer.send(\n    'custom_msg',\n    # key='test',  # 同一个key值，会被送至同一个分区\n    value=json_list,\n    # value=json.dumps(json_list).encode(\"utf-8\"),\n    partition=random.randint(0, 3)  # 向多分区发送数据（注意：分区需存在，如果不存在需要通过标题四的方法去创建分区）\n)  # 向分区1发送消息\n\ntry:\n    future.get(timeout=10) # 监控是否发送成功\nexcept kafka_errors:  # 发送失败抛出kafka_errors\n    traceback.format_exc()\n\n```\n<!--more-->\n**二：kafka consumer** \n```\n\nfrom kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(bootstrap_servers=['kafka.aws.bigdata.yidianshihui.com:9092'],\n                         # value_deserializer=msgpack.loads,\n                         # key_deserializer=msgpack.loads\n                         key_deserializer=lambda k: json.loads(k),\n                         value_deserializer=lambda v: json.loads(v),\n                         auto_offset_reset='latest',\n                         group_id='test1-group'\n                         )\n# 订阅消费的主题的分区\nconsumer.assign([TopicPartition('test1', 0),TopicPartition('test1', 1),TopicPartition('test1', 2)])\n\n\nwhile True:\n    time.sleep(1)\n    for message in consumer:\n        if message is not None:\n            print(message.offset, message.value)\n            print(msg)\n            value = msg.value\n            key = msg.key\n            print(type(value))\n            print(value)\n            print(type(key))\n            print(msg.partition)\n```\n参数详解\n```\nclass kafka.KafkaConsumer(*topics, **configs)\n\n*topics (str) – 可选，设置需要订阅的topic，如果未设置，需要在消费记录前调用subscribe或者assign。\n\n\n\nclient_id (str) – 客户端名称，默认值: ‘kafka-python-{version}’\n\n\n\ngroup_id (str or None) – 消费组名称。如果为None，则通过group coordinator auto-partition分区分配，offset提交被禁用。默认为None\n\n\n\nauto_offset_reset (str) – 重置offset策略: 'earliest'将移动到最老的可用消息, 'latest'将移动到最近消息。 设置为其它任何值将抛出异常。默认值：'latest'。\n\n\n\nenable_auto_commit (bool) –  如果为True，将自动定时提交消费者offset。默认为True。\n\n\n\nauto_commit_interval_ms (int) – 自动提交offset之间的间隔毫秒数。如果enable_auto_commit 为true，默认值为： 5000。\n\n\n\nvalue_deserializer(可调用对象) - 携带原始消息value并返回反序列化后的value\n\n\n\nsubscribe(topics=(), pattern=None, listener=None)\n\n订阅需要的主题\n\ntopics (list) – 需要订阅的主题列表\n\npattern (str) – 用于匹配可用主题的模式，即正则表达式。注意：必须提供topics、pattern两者参数之一，但不能同时提供两者。\n\n\n ```\n**三：kafka 创建topic并创建分区** \n\n```\nfrom kafka.admin import KafkaAdminClient, NewTopic, NewPartitions\nc = KafkaAdminClient(bootstrap_servers=\"kafka.aws.bigdata.yidianshihui.com:9092\")\n# 创建topic同时创建分区\ntopic_list = []\ntopic_list.append(NewTopic(name=\"test1\", num_partitions=2,replication_factor=1))\nc.create_topics(new_topics=topic_list, validate_only=True)\n\n```\n\n**四：kafka 已有的topic并创建分区**\n有时候，向指定分区发送数据时，会报如下错误，这个是因为topic里不存在此分区，需手动创建\n```\nassert partition in self._metadata.partitions_for_topic(topic), 'Unrecognized partition'\nAssertionError: Unrecognized partition\n```\n创建方法\n```\nfrom kafka.admin import KafkaAdminClient, NewTopic, NewPartitions\nc = KafkaAdminClient(bootstrap_servers=\"kafka.aws.bigdata.yidianshihui.com:9092\")\n\n# 在已有的topic中创建分区\nnew_partitions = NewPartitions(3)\nc.create_partitions({\"custom_msg\": new_partitions})\n```\n\n参考文档：\nhttps://zhuanlan.zhihu.com/p/103915834\nhttps://www.cnblogs.com/xiaozengzeng/p/13621045.html\nhttps://kafka-python.readthedocs.io/en/master/usage.html\n[Python生产者和消费者API使用](https://www.cnblogs.com/rexcheny/articles/9463979.html)\n[kafka 随笔](https://www.cnblogs.com/rexcheny/tag/Kafka/)\n\n","tags":["kafka","kafka-python"],"categories":["异步任务"]},{"title":"mac etcd","url":"/2021/05/07/异步任务/mac etcd/","content":"\n\n##一、使用 brew 安装\n#### 1、 确定 brew 是否有 etcd 包：\n\n```\nbrew search etcd\n```\n当然肯定有这个包，这样做的好处是养成一个好的习惯，避免盲目使用 brew install balabala\n<!--more-->\n####2、安装\n```\nbrew install etcd\n```\n![image](https://user-images.githubusercontent.com/28568478/117406890-51330d00-af40-11eb-9d77-cee34e5d064d.png)\n\n\n####3、运行 etcd\n安装完后，会有相关提示，告知我们怎么使用，推荐使用 brew services 来管理这些应用。\n运行 **brew services list**, 可以看到相关应用的状况，很方便。哎，真香！\n\n![image](https://user-images.githubusercontent.com/28568478/117406896-542dfd80-af40-11eb-858d-9a8334e523fb.png)\n\n\n\nbrew services 常用的操作\n\n```\n# 启动某个应用，这里用 etcd 做演示\nbrew services start etcd\n\n# 停止某个应用\nbrew services stop etcd\n\n# 查看当前应用列表\nbrew services list\n```\n![image](https://user-images.githubusercontent.com/28568478/117406908-58f2b180-af40-11eb-9b81-fd34bffa0fca.png)\n\n从执行结果中可以看出：\n\n- etcdserver: name = default， name表示节点名称，默认为default。\n- etcdserver: data dir = default.etcd，data-dir保存日志和快照的目录，默认为当前工作目录“./default.etcd/”。\n- etcdserver: initial advertise peer URLs = http://localhost:2380，通过http://localhost:2380，和集群中其他节点通信。\n- etcdserver: advertise client URLs = http://localhost:2379，通过http://localhost:2379，对外提供HTTP API服务，供客户端交互。如果配置webui，就使用这个地址。\n- etcdserver: heartbeat = 100ms leader发送心跳到followers的间隔时间。\n- etcdserver: election = 1000ms 重新投票的超时时间，如果follow在该时间间隔没有收到心跳包，会触发重新投票，默认为1000ms\n- 集群和每个节点都会生成一个 uuid。\n- 启动的时候，会运行 raft协议，选举出 leader。\n\n好了， etcd 已经启动了，现在验证下，是否正确的启动：\n```\netcdctl endpoint health\n```\n正常情况会输出：\n![image](https://user-images.githubusercontent.com/28568478/117406997-7e7fbb00-af40-11eb-951f-54cfe745af03.png)\n\n\n至此，etcd 已经安装完毕。\n\n## 二、安装etcd webui\n\n在安装etcd webui之前，请确保已安装node工具。使用brew search node命令，可以查看候选安装包；使用**brew install node**命令，即可安装node工具。\n\n使用git命令下载etcd webui代码，并修改配置文件：\n\n```\n$ git clone https://github.com/henszey/etcd-browser.git\n$ cd etcd-browser/\n$ vim server.js\n```\n\n编辑server.js，修改内容如下：\n\n```\nvar etcdHost = process.env.ETCD_HOST || '127.0.0.1'; // || '172.17.42.1';\nvar etcdPort = process.env.ETCD_PORT || 2379; // 4001\nvar serverPort = process.env.SERVER_PORT || 8000;\n```\n\n将etcd host修改为本机，将etcd port修改为2379（对于旧版etcd，修改为4001）。\n\n在安装etcd webui之前，务必先启动etcd。\n\n启动etcd webui，执行命令：\n\n```\nnode server.js\n```\n\n执行结果如下所示：\n\n```\nproxy /api requests to etcd on 127.0.0.1:2379etc-browser listening on port 8000\n```\n\n在浏览器中，直接访问：[http://127.0.0.1:8000/](http://127.0.0.1:8000/)，响应页面如下：\n\n![image](https://user-images.githubusercontent.com/28568478/117407010-82abd880-af40-11eb-83f7-840fac4b4a06.png)\n\n至此，mac下安装etcd成功，配置etcd可视化页面etcd webui成功。\n\n##三、操作\n最常见的就是put、get和del命令。\n  ```\n# 放入一个 键值对\n ~  etcdctl put \"name\" \"zyq1\"\nOK\n#取出一个 键值对\n ~  etcdctl get  \"name\"\nname\nzyq1\n# 删除一个 键值对\n ~  etcdctl del  \"name\"\n1\n# 放入一个 键值对\n```\nwatch\nwatch命令用来监测key的变化，会建立长连接，一直监听。\n\n```\n ~  etcdctl watch \"name\"\nPUT\nname\nzyq1\nDELETE\nname\n```\n\n##四、租约\n租约是一段时间，可以为etcd的key授予租约。当key被附加到租约时，它的生存时间被绑定到租约的生存时间，一旦租约的TTL到期，租约就过期并且所有附带的key都将被删除。\n\n一个租约可以绑定不止一个key。\n\n```\n# 创建一个20s的租约\n$ ./etcdctl lease grant 20\nlease 694d673115905e37 granted with TTL(20s)\n\n# 使用租约的 id 进行 put 操作\n$ ./etcdctl put --lease=694d673115905e37 \"name\" \"zyq\"\n\n# 20s后get发现 key被删除了\n$ ./etcdctl get \"name\"\n# 空应答\n```\n\n租约可以被删除\n\n```\n# 创建一个20s的租约\n$ ./etcdctl lease grant 1000\nlease 694d673115905e49 granted with TTL(1000s)\n\n# 使用租约的 id 进行 put 操作\n$ ./etcdctl put --lease=694d673115905e49 \"name\" \"zyq\"\nOK\n# 删除租约\n$ ./etcdctl lease revoke 694d673115905e49\nlease 694d673115905e49 revoked\n# 发现key也被删除了\n$ ./etcdctl get \"name\"\n# 空应答\n```\n\n租约可以自动续租\n\n```\n# 创建一个20s的租约\n$ ./etcdctl lease grant 20\nlease 694d673115905e4f granted with TTL(20s)\n# 自动续租\n$ ./etcdctl lease keep-alive 694d673115905e4f\nlease 694d673115905e4f keepalived with TTL(20)\nlease 694d673115905e4f keepalived with TTL(20)\n```\n\n————————————————\n转自链接：\nhttps://learnku.com/articles/42515\nhttps://blog.csdn.net/chinawangfei/article/details/94555155\n","tags":["macOS","etcd"],"categories":["异步任务"]},{"title":"在 Mac 下面安装 cryptography 依赖包，始终报错，出现 'openssl/opensslv.h' file not found 的错误。","url":"/2021/05/07/mac/在 Mac下面安装 cryptography 依赖包始终报错/","content":"\n在 Mac 下面安装 cryptography 依赖包，始终报错，出现 'openssl/opensslv.h' file not found 的错误。\n```\n$ pip install cryptography\n```\n```\nbuilding '_openssl' extension\ncc -fno-strict-aliasing -fno-common -dynamic -arch i386 -arch x86_64 -g -Os -pipe -fno-common -fno-strict-aliasing -fwrapv -DENABLE_DTRACE -DMACOSX -DNDEBUG -Wall -Wstrict-prototypes -Wshorten-64-to-32 -DNDEBUG -g -fwrapv -Os -Wall -Wstrict-prototypes -DENABLE_DTRACE -arch i386 -arch x86_64 -pipe -I/System/Library/Frameworks/Python.framework/Versions/2.7/include/python2.7 -c build/temp.macosx-10.12-intel-2.7/_openssl.c -o build/temp.macosx-10.12-intel-2.7/build/temp.macosx-10.12-intel-2.7/_openssl.o\nbuild/temp.macosx-10.12-intel-2.7/_openssl.c:434:10: fatal error: 'openssl/opensslv.h' file not found\n#include <openssl/opensslv.h>\n         ^\n1 error generated.\nerror: command 'cc' failed with exit status 1\n```\n这是因为找不到 openssl 的头文件，可以使用 brew 命令进行安装:\n```\n$ brew install openssl\n```\n如果安装完成以后还是会出现上面的错误的话，就是环境变量的问题了，需要重新指定 openssl 的路径安装：\n```\n$ env LDFLAGS=\"-L$(brew --prefix openssl)/lib\" CFLAGS=\"-I$(brew --prefix openssl)/include\" pip install cryptography\n```\n这样就可以搞定了~~~\n","tags":["macOS","cryptography"],"categories":["mac"]},{"title":"Linux访问远程服务器文件——NFS+mount挂载","url":"/2021/05/07/Linux/Linux访问远程服务器文件——NFS+mount挂载/","content":"## 前言\n有两台服务器，S1和S2，Linux操作系统，S1和S2IP不同，但是可以相互访问。\n\nS2需要访问S1的文件系统，换言之，S1中有文件夹被分享给S2使用。\n\n## 准备工作\n假设 S1的ip为 192.168.1.2，要分享为公共文件夹的目录为 /tmp/share\n\n假设 S2的ip为 192.168.1.3，接受上传文件的目录为 /usr/tomcat/here\n\n## 运行场景\n在实际使用中，直接向 192.168.1.3 的 /usr/tomcat/here 中保存文件，或者删除文件，都相当于在 192.168.1.2 /tmp/share中进行操作\n\n<!--more-->\n## 开始实施\n### 一、S1服务器的设置\n#### 1、检查 S1，即需要提供分享文件夹的服务器是否具有NFS服务\n$ rpm -qa |grep nfs\n\n如果安装了，则会打印一些信息，\n\n![image](https://user-images.githubusercontent.com/28568478/129548625-3bf0255e-f506-4651-a1a6-ee67e9f078b5.png)\n\n\n否则什么也不会发生，就需要安装了\n\n使用root权限\n\n```$ yum install nfs-utils```\n\n#### 2、修改 S1中的/etc/exports\n增加语句\n\n```/tmp/share 192.168.1.3(rw,no_root_squash,async)```\n\n看出来格式了吗？允许 Ip为192.168.1.3的服务器访问本服务器的/tmp/share文件夹，后面括号里的内容是必须的，设定了一些操作规则\n\n关于exports 的内容可以参考 http://blog.chinaunix.NET/uid-21089721-id-2327441.html\n\n#### 3、重启NFS服务\n\n```\n$ service portmap start( service rpcbind start)\n\n$ service nfs start\n```\n\n或者\n\n```$ service nfs restart```\n\n以上 三步完成了S1的所有设置，S1又被叫做服务端\n\n##### 注意：每次配置完成/etc/exports后都要重启nfs服务\n\n### 二、S2服务器的设置\n#### 1、接下来是S2的设置，就比较简单了\n注意！S2中也必须安装nfs服务\n\n否则会报错：wrong fs type, bad option, bad superblock\n\n假设 S2的ip为 192.168.1.3，接受上传文件的目录为 /usr/tomcat/here\n\n这里 /usr/tomcat/here 是需要存在的，被称为挂载点\n\n如果不存在可以创建（-p 的意思是如果父目录不存在则创建）\n\n```mkdir -p /usr/tomcat/here```\n\n#### 2、然后运行\n\n```$ mount -t nfs 192.168.1.2:/tmp/share /usr/tomcat/here```\n\n格式上就是，mount -t nfs S1的IP：S1分享的目录 S2直接操作的目录\n\n这样操作S2的这个目录就相当于直接S1分享的目录了，当然，操作S1的分享的目录，这个S2里的内容也会跟着变\n\n#### 3.查看目前客户端的挂载情况\n\n```$ mount | grep nfs```\n\n![image](https://user-images.githubusercontent.com/28568478/129548589-24f517ef-2dcb-45a1-8556-4f47b75b745f.png)\n\n## 去除客户端的挂载\n\n```$ umount /var/tmp/share```\n\n或者\n\n```$ umount -l /var/tmp/share```\n\n/var/tmp/share 是客户端的目录，注意这个是我本地的实验数据，不要和上面的混淆\n\n其次，这里命令中加了 -l ，是强制执行的命令，对于出现device is busy 时才可以使用\n\n参考的文章\n\n1、http://zhuang13.blog.51cto.com/3044154/557879\n\n2、http://www.cnblogs.com/mchina/archive/2013/01/03/2840040.html\n\n3、http://blog.chinaunix.Net/uid-21089721-id-2327441.html\n\n## mount挂载永久设置（不建议，系统启动会很慢）\n按照上面的操作，客户端即S2的目录挂载是临时的，服务器重启后就失效了，如果需要永久设置另外需要操作。\n\nhttp://blog.csdn.net/a2683901/article/details/43274991\n\nvim /etc/fstab //在最后一行添加下面一行信息\n192.168.1.253:/testnfs /testnfs_client nfs defaults 0 0\n\n重启 mount -a\n\n## 自动挂载设置（推荐）\n自动挂载用到的软件包automount，一般系统都默认安装了的。如果没有安装再安装下：\n\n```yum install autofs```\n\n先在根目录创建一个用于自动挂载的目录\n\n```mkdir /u01```\n\n编辑配置文件\n\n```vim /etc/auto.master```\n\n最后一行添加（左边目录是指需要挂载的目录，右边目录是指关联到所需自动挂载路径\n\n```/u01 /etc/auto.nfs```\n\n新建我们刚刚设置的自动挂载路径，添加如下信息\n\n```vim /etc/auto.nfs```\n\n左边代表自动挂载目录(cd mount 会自动挂载)，中间权限，sync 代表同步，右边代表所需挂载的文件路径\n\n```mount -rw,sync 192.168.1.253:/testnfs```\n\n重置自动挂载map\n\n```service autofs reload```\n\n这时，我们去访问我们建立的自动挂载目录，去触发自动挂载\n\n```\n[root@xuan ~]# cd /u01/\n[root@xuan nfs_mount]# ls\n[root@xuan nfs_mount]# cd mount      //上面ls查看没有mount目录，但是我们可以cd访问mount目录去触发自动挂载\n[root@xuan nfs_client]# ls\ntestfile\n```\n\n\nnfs 服务器端的其他命令\n\n```$ service nfs {start|stop|status|restart|reload|force-reload|condrestart|try-restart|condstop}```\n\nnfs服务自动启动\n正常而言，nfs服务需在系统启动后手动启动，通过下面的设置可以使nfs服务在系统重启的时候自动启动\n\n```$ chkconfig –level 345 nfs on```\n\n检查结果\n\n```\n$ chkconfig –list nfs\n\n0:off1:off 2:off3:on 4:on5:on 6:off\n```\n\n命令格式参考\n\n![image](https://user-images.githubusercontent.com/28568478/129548520-f5bb0130-8bdf-49e8-a1a5-6a13db2eec8e.png)\n\n\n\n详解：http://blog.chinaunix.net/uid-22287947-id-1991563.html\n\n原文链接：https://blog.csdn.net/zwfmu/article/details/70300808\n","tags":["NFS+mount挂载","服务器之间目录软连"],"categories":["Linux"]},{"title":"linux 新添加的硬盘格式化并挂载到目录下","url":"/2021/05/07/Linux/linux 新添加的硬盘格式化并挂载到目录下/","content":"\n\n需求: 新增加一块硬盘sdb，将sdb分区，只分一个区，格式化，挂载到目录/ssd下。\n\n### 1、  查看现在已有的分区状态\n```\ndf –l\ndf -h\n```\n![image](https://user-images.githubusercontent.com/28568478/117395814-491ca280-af2b-11eb-8e34-b84381e67aa8.png)\n\n图中显示，没有看到sdb硬盘\n<!--more-->\n### 2、  查看服务器安装的硬盘状态（包括格式化和未格式化）\n```\nfdisk –l\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117395838-56399180-af2b-11eb-8651-47c79b52057f.png)\n\n图中显示，有sdb硬盘，但是没有分区。\n\n### 3、  添加新分区\n```\nfdisk /dev/sdb\n```\n![image](https://user-images.githubusercontent.com/28568478/117395863-6b162500-af2b-11eb-8a91-9e72f69c44fc.png)\n```\n按照以下红框输入\n\nN 回车\n\nP 回车\n\n1 回车\n\n两次回车\n\nW 回车\n```\n![image](https://user-images.githubusercontent.com/28568478/117395883-7701e700-af2b-11eb-88bb-20616afd029b.png)\n\n用以下命令查看分区\n```\nfdisk –l\n```\n![image](https://user-images.githubusercontent.com/28568478/117395904-8123e580-af2b-11eb-864c-57a14b6ece56.png)\n\n图中红框显示已多出了一个分区，但是还没有格式化。\n\n### 4、  格式化分区\n\ncentos6文件系统是ext4，因为设计较早，对于现今动辄上T的海量数据处理，性能较低。centos7文件系统是xfs，适用于海量数据。这两种文件系统都是日志文件系统。使用该文件系统的磁盘，空间包括两部分：日志空间和存储空间。写入的数据是先暂存在日志空间，然后刷入存储空间，这样有利于恢复数据。另外，xfs文件系统还支持将一块儿固态硬盘用作单独的日志空间盘，数据先写入固态硬盘，然后再刷入硬盘。对于操作系统来说，数据写入了日志空间盘，就算完成了I/O，因此这种方式提高了系统性能。\n```\n# mkfs -t ext4 -c /dev/sdb1\n# -t 制定要把磁盘格式化成什么类型\n# -c 在建立文件系统之前检查坏道，可能会很费时间，新硬盘一般不需要\n\n# 新硬盘可直接用此命令 格式化成不同类型的硬盘\nsudo mkfs.ext4 /dev/nvme1n1\n\nmkfs.xfs -f /dev/vdb\n\n# 查看硬盘类型\ndf -T\n```\n![image](https://user-images.githubusercontent.com/28568478/117395938-926cf200-af2b-11eb-9502-7b04a448c146.png)\n![image](https://user-images.githubusercontent.com/28568478/117395944-96007900-af2b-11eb-8797-4e08db65d265.png)\n\n### 5、  挂载新硬盘\n\n在根目录下，建一个文件夹，待会将分区挂载在这个文件夹上，以后要往新硬盘存东西就存在新建文件夹下就可以了。\n```\nmkdir /ssd\n```\n\n挂载硬盘\n\n```\nmount /dev/sdb1 /ssd\n```\n![image](https://user-images.githubusercontent.com/28568478/117395965-a153a480-af2b-11eb-9bed-22f5f33c9908.png)\n\n```\n sudo chown ec2-user:root /data1   用户权限\n```\n\n### 6、  让系统开机自动挂载这块硬盘\n\n```\necho \"/dev/sda1 /ssd ext4 defaults 0 0\" >> /etc/fstab\n```\n\n如果报错：\n```\n先更改权限\nsudo chown ec2-user:root  /etc/fstab\n```\n![image](https://user-images.githubusercontent.com/28568478/117396002-ae709380-af2b-11eb-8cb4-66d295993e02.png)\n\n\n## 注意：\n\n挂载硬盘\n```\nsudo mount  /dev/nvme1n1  /data1\n```\n\n取消挂载\n```\nsudo umount  /dev/nvme1n1\n```\n\n如果 /data1里原先有数据 他会自己保留 取消挂载后 会恢复回来\n\n## 查看文件夹大小\n```\ndu -h ./scheduler/\n\ndu -sh ./scheduler/\n\n参数解释\n\n-a ： 列出所有的文件与目录容量，因为默认仅统计目录的容量而已\n\n-h: 以人们较易读的容量格式呈现(G/M/K)显示，自动选择显示的单位大小\n\n-s : 列出总量而已，而不列出每个个别的目录占用容量\n\n-k ： 以KB为单位进行显示\n\n-m : 以MB为单位进行显示常用[命令](https://www.linuxcool.com/)参考  查看当前目录大小\n```\n\n\n参考文档：\n    [linux下查看硬盘信息、硬盘分区、格式化、挂载、及swap分区](https://blog.csdn.net/Ayhan_huang/article/details/72801647)\n    [linux 新添加的硬盘格式化并挂载到目录下](https://www.cnblogs.com/ddbear/p/7009736.html)\n","tags":["硬盘挂载"],"categories":["Linux"]},{"title":"Navicat Premium 15.0.21 强大的数据库管理工具(Big Sur 可用)","url":"/2021/05/07/tools/Navicat Premium 15.0.21 强大的数据库管理工具(Big Sur 可用)/","content":"Navicat Premium 15.0.21 强大的数据库管理工具(Big Sur 可用)\n链接： https://xclient.info/s/navicat-premium.html#versions\n","tags":["macOS","Navicat"],"categories":["工具"]},{"title":"Mysql5.7使用group by查询时order by无效问题","url":"/2021/05/07/数据库/Mysql5.7使用group by查询时order by无效问题/","content":"Mysql5.7使用group by查询时order by无效问题\n第一种写法：\n```\nSELECT\n\t*\nFROM\n\tuser_paper_relation\nWHERE\n\tpaper_id = \"\"\nGROUP BY\n\texamer_id\nORDER BY\n\ttotal_score DESC\n```\n问题：发现先执行的group by，后执行order by，我要的结果是先排序再分组。\n\n第二种写法：\n```\nSELECT\n\t*\nFROM\n\t( SELECT * FROM user_paper_relation WHERE paper_id = \"\"  ORDER BY consuming_time DESC ) u\nGROUP BY\n\tu.examer_id\n```\n问题：本以为将排序写成一个子查询应该会先排序后分组，执行后发现问题同第一种写法一样。\n\n第三种写法（最终解决的写法）：\n```\nSELECT\n\ta.*\nFROM\n\t(\n\tSELECT\n\t\t*\n\tFROM\n\t\tuser_paper_relation\n\tWHERE\n\t\tpaper_id = \"\"\n\tORDER BY\n\t\tconsuming_time\n\t\tLIMIT 0,\n\t\t300\n\t) a\nGROUP BY\n\ta.examer_id\n```\n解决方式：在子查询中添加LIMIT 0,300则可使子查询语句即排序执行完再进行分组。\n\n问题原因\n```\n原因：在mysql5.7中，如果不加limit，系统会把order by优化掉。\n在mysql5.7手册的8.2.2.1中有解释：\n子查询的优化是使用半连接的策略完成的(The optimizer uses semi-join strategies to improve subquery execution)\n使用半连接进行优化，子查询语句必须满足一些标准(In MySQL, a subquery must satisfy these criteria to be handled as a semi-join)。\n其中一个标准是:必须不是一个包含了limit和order by的语句(It must not have ORDER BY with LIMIT.)\n```\n原文链接：https://blog.csdn.net/weiwoyonzhe/article/details/82888281\n","tags":["mysql"],"categories":["数据库"]},{"title":"Can't connect to local MySQL server through socket'/var/run/mysqld/mysqld.sock' (2)","url":"/2021/05/07/数据库/Can't connect to local MySQL server through/","content":"启动命令mysql时报错：\n\nERROR2002 (HY000): Can't connect to local MySQL server through socket'/var/run/mysqld/mysqld.sock' (2)\n\n解决方法：\n\n找到配置文件/etc/mysql/conf.d/mysql.cnf添加\n\n[mysql]\n\nprotocol=tcp\n\n参考文献：https://www.cnblogs.com/zhao123h/p/5207622.html\n","tags":["mysql"],"categories":["数据库"]},{"title":"linux安装redis 完整步骤","url":"/2021/05/07/数据库/linux安装redis 完整步骤/","content":"## 安装：\n\n### 1.获取redis资源\n```\n　　wget http://download.redis.io/releases/redis-4.0.8.tar.gz\n```\n### 2.解压\n```\n　　tar xzvf redis-4.0.8.tar.gz\n```\n### 3.安装\n```\n　　cd redis-4.0.8\n\n　　make\n\n　　cd src\n\n　　make install PREFIX=/usr/local/redis\n```\n### 4.移动配置文件到安装目录下\n```\n　　cd ../\n\n　　mkdir /usr/local/redis/etc\n\n　　mv redis.conf /usr/local/redis/etc\n```\n### 5.配置redis为后台启动\n```\nvi /usr/local/redis/etc/redis.conf // 将daemonize no 改成daemonize yes\n```\n### 6.将redis加入到开机启动\n```\n　　vi /etc/rc.local // 在里面添加内容：/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf (意思就是开机调用这段开启redis的命令)\n```\n### 7.开启redis\n```\n　　/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf\n```\n\n```\n常用命令\n\n　　redis-server /usr/local/redis/etc/redis.conf //启动redis\n\n　　pkill redis  //停止redis\n\n卸载redis：\n\n　　rm -rf /usr/local/redis //删除安装目录\n\n　　rm -rf /usr/bin/redis-* //删除所有redis相关命令脚本\n\n　　rm -rf /root/download/redis-4.0.4 //删除redis解压文件夹\n```\n","tags":["redis"],"categories":["数据库"]},{"title":"Linux安装MySQL","url":"/2021/05/07/数据库/Linux安装MySQL/","content":"\n\n## 一、安装\n\nsudo apt update\n\nsudo apt-get install mysql-server  # 安装MySQL服务端\n\n以上两个命令 即安装成功\n\n<!--more-->\n\n## 二、安装后的密码问题\n\nsudo cat /etc/mysql/debian.cnf 查看默认的用户名和密码\n\n\n## 三、添加自己的密码\n\n- 1、use mysql; 然后敲回车\n\n- 2、update user set authentication_string=password(\"你的密码\") where user=\"root\"; 然后敲回车\n\n- 3、flush privileges; 然后敲回车\n\n用账户密码登录时发现报错\n\nERROR 1698 (28000): Access denied for user 'root'@'localhost'\n\n解决方法如下：\n\n* step1：在ubuntu的terminal（也即终端）上输入：\n\n        sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf,\n\n        进入到这个配置文件，\n\n        然后在这个配置文件中的[mysqld]这一块中加入skip-grant-tables这句话。\n\n        保存:wq，退出。输入：service mysql restart，重新启动mysql。\n\n* step2：在终端上输入mysql -u root -p，遇见输入密码的提示直接回车即可,进入mysql后，分别执行下面三句话：\n\n        1、use mysql; 然后敲回车\n\n        2、update user set authentication_string=password(\"你的密码\") where user=\"root\"; 然后敲回车\n\n        3、flush privileges; 然后敲回车\n\n\n\n* step3：重新进入到mysqld.cnf文件中去把刚开始加的skip-grant-tables这条语句给注释掉。\n\n            再返回终端输入mysql -u root -p，应该就可以进入数据库了。\n\n* step4：如果此时还是报出错误，那么就需要返回step3中，把注释掉的那条语句重新生效（就是删除#符号），重新进入mysql中，先选择一个数据库（use mysql;）,然后输入select user,plugin from user;，看下图：\n\n\n\n\n从图中可以看到在执行了select user,plugin from user;后，错误原因是因为plugin root的字段是auth_socket，那我们改掉它为下面的mysql_native_password就行了。输入：\n\n1、update user set authentication_string=password(\"你的密码\"),plugin='mysql_native_password' where user='root';\n\n\n\n最后quit退出。返回执行step3。\n\n那么这个问题就完全解决了\n\n\n\n\n\n参考链接：https://www.cnblogs.com/cpl9412290130/p/9583868.html\n","tags":["mysql"],"categories":["数据库"]},{"title":"Mac下MySQL-python安装及EnvironmentError mysql_config not found的解决办法","url":"/2021/05/07/数据库/Mac下MySQL-python安装及EnvironmentError: mysql_config not found的解决办法/","content":"今天  pip install MySQL-python==1.2.5  报如下错误：\n```\nEnvironmentError: mysql_config not found\n\nCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/6h/hchh3z9d6b33h11qh0wh__dh0000gn/T/pip-install-Rm43Qv/MySQL-python/\n\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117392434-a6f9bc00-af24-11eb-836c-f51c7a67ae47.png)\n\n解决方法如下：\n\n1）首先需要进行Python的安装\n\n（2）进行mysql的安装，多种方式，偷懒一些可以直接使用brew指令\n\nbrew install mysql\n\n（3）mysql_config not found的造成原因就是因为我们默认安装的路径没有被填加进去，所以使用export命令来设置下环境变量：\n\nexport PATH=$PATH:/usr/local/mysql/bin\n\n（4）进行MySQL-Python的安装：\n\npip install MySQL-Python\n\n原文：https://blog.csdn.net/Megustas_JJC/article/details/78955958\n","tags":["mysql"],"categories":["数据库"]},{"title":"mac 安装redis、mysql、MongoDB","url":"/2021/05/07/数据库/mac 安装redis、mysql、MongoDB/","content":"## redis 安装\n* brew install redis\n#### 启动\n* brew services start redis（后台启动）\n* redis-server /usr/local/etc/redis.conf （非后台启动）\n\n<!--more-->\n\n## mysql 安装\n* brew install mysql\n#### 启动\n* brew services start mysql（后台启动）\n* mysql.server start（非后台启动）\n* 没有详细的配置文件，可参考其他系统下的my.cnf进行自我配置\n\n![image](https://user-images.githubusercontent.com/28568478/117404953-6d817a80-af3d-11eb-8663-cb188d59b022.png)\n\n1、我们在没有root 密码的情况下安装了您的MySQL数据库。确保它运行：mysql_secure_installation\n2、登录mysql: mysql -uroot\n3、如果忘记密码：\n```\ncd /usr/local/bin/mysql/\n./mysqld_safe --skip-grant-tables &  # 禁止mysql验证功能\n./mysql  # 进入mysql\nFLUSH PRIVILEGES;\nSET PASSWORD FOR 'root'@'localhost' = '你的新密码';\n```\n\n4、![image](https://user-images.githubusercontent.com/28568478/117405005-825e0e00-af3d-11eb-8a6e-20ae85e72642.png)\n ```\nALTER USER root@localhost IDENTIFIED WITH mysql_native_password BY ‘你的密码’;\n```\n##  mongodb 安装\n* brew install mongodb\n\n#### 启动\n* brew services start mongodb （后台启动）\n* mongod --config /usr/local/etc/mongod.conf （非后台启动）\n","tags":["macOS","mysql","mongodb","redis"],"categories":["数据库"]},{"title":"Ubuntu18.04 安装mysql","url":"/2021/05/07/数据库/Ubuntu18.04 安装mysql/","content":"## 一、安装方法\n[Ubuntu18.04下安装MySQL](https://www.cnblogs.com/opsprobe/p/9126864.html)\n\n<!--more-->\n\n## 二、常用命令\n\n　　1.启动：/etc/init.d/mysql start\nsudo service mysql restart\n\n　　2.停止：/etc/init.d/mysql stop\n\n　　3.重启：/etc/init.d/mysql restart\n##  三、报错\n#### 1、[Can't connect to MySQL server on '<remote-ip>' (61)](https://my.oschina.net/Laily/blog/712958)\n第一次安装可能是3306 只监听了localhost ，修改配置文件/etc/mysql/mysql.conf.d/mysqld.cnf中的bind-address为0.0.0.0即可\n![image](https://user-images.githubusercontent.com/28568478/117405298-fef0ec80-af3d-11eb-9d77-3c2cb8079e64.png)\n\n#### 2、权限不够\n```\n（1）SSH登录root管理员账户\n\n（2）登录MySql\n\n  mysql -u root -p\nEnter password:\n（3）执行授权命令\n\nmysql> grant all privileges on *.* to root@'localhost' identified by '密码';\nmysql> flush privileges;\n或\n\nmysql> grant all privileges on *.* to root@'%' identified by '密码';\nmysql> flush privileges;\n（4）退出再试\n\nmysql> quit\nBye\n（5）再次登录\n```\n\n## 四、 supervisor 配置(可不用配置 默认后台自启动)\n```\n[program:mysql_3306]\ncommand=/usr/sbin/mysqld --daemonize --pid-file=/run/mysqld/mysqld.pid\nprocess_name=mysql_3306\nnumprocs=1\n;user=martin\nautostart=true\nautorestart=true\nstartsecs=10\nstartretries=7\nstopsignal=TERM\nstopwaitsecs=60\nredirect_stderr=false\nstdout_logfile=/root/log/mysql/mysql.log\nstdout_logfile_maxbytes=50MB\nstdout_logfile_backups=10\nstdout_capture_maxbytes=1MB\nstderr_logfile=/root/log/mysql/mysql_error.log\nstderr_logfile_maxbytes=50MB\nstderr_logfile_backups=10\nstderr_capture_maxbytes=1MB\n;directory=/home/martin/data/logagent\nserverurl=AUTO\n```\n","tags":["mysql"],"categories":["数据库"]},{"title":"Ubuntu18.04 安装redis","url":"/2021/05/07/数据库/Ubuntu18.04 安装redis/","content":"### 一、安装\n```\nsudo apt install redis   # 安装redis\n\ncd /etc/redis/redis.conf # 变更配置\n注释bind 127.0.0.1 ::1\n加 bind 0.0.0.0\n```\n\n### 二、命令\n```\nbin/sh /etc/init.d/redis-server start/stop/restart\nsystemctl start/stop/restart redis-server\n/usr/bin/redis-server # 只是启动\n```\n","tags":["redis"],"categories":["数据库"]},{"title":"Ubuntu 18.04 安装MongoDB","url":"/2021/05/07/数据库/Ubuntu 18.04 安装MongoDB/","content":"### 第1步 – 导入公钥\n```\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6\n```\n### 第2步 – 创建源列表文件MongoDB\n```\necho \"deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list\n```\n<!--more-->\n### 第3步 – 更新存储库\n```\nsudo apt update\n\n```\n### 第4步 – 安装MongoDB\n```\nsudo apt install -y mongodb-org\n```\n### 第5– 步修改配置\n```\nnet:\n  port: 27017         # 端口号\n  bindIp: 0.0.0.0    # 允许访问host\nsetParameter:\n   enableLocalhostAuthBypass: false # 大意为开启授权\n```\n### 新建目录\n\n为什么要创建这样这个文件夹呢？看其他人的说法是这个：\n\nMongoDB的数据存储在data目录的db目录下，但是这个目录在安装过程不会自动创建，所以你需要手动创建data目录，并在data目录中创建db目录。\n\n然后等待完成就可以使用MongoDB了，输入mongo进入命令行就可以操作了。\n```\nmkdir -p /data/db\n```\n### 命令\n```\n/usr/bin/mongod --config /etc/mongod.conf -dbpath /data/db/\n```\n","tags":["mongodb"],"categories":["数据库"]},{"title":"Mysql命令行查看数据库大小(数据库版本为5.7以上)","url":"/2021/05/07/数据库/Mysql命令行查看数据库大小(数据库版本为5.7以上)/","content":"\n数据库版本为5.7以上\n1、选择数据库\n```\nuse mydb1;\n```\n2、查看指定数据库表结构\n```\nselect * from information_schema.TABLES where information_schema.TABLES.TABLE_SCHEMA='mydb1';\n```\n3、查看指定数据库的大小\n比如说 数据库mydb1\n```\nselect concat(round(sum(DATA_LENGTH/1024/1024),2), 'MB') as data from information_schema.TABLES where information_schema.TABLES.TABLE_SCHEMA='mydb1';\n```\n4、查看指定数据库的表的大小\n比如说 数据库mydb1中b1表\n```\nselect concat(round(sum(DATA_LENGTH/1024/1024),2), 'MB') as data from information_schema.TABLES where information_schema.TABLES.TABLE_SCHEMA='mydb1' and TABLE_NAME='b1';\n```\n","tags":["mysql"],"categories":["数据库"]},{"title":"SQL的模糊匹配区别---like,rlike,regexpx","url":"/2021/05/07/数据库/SQL的模糊匹配区别---like,rlike,regexpx/","content":"\n\n\n## 一、主要区别\n- (1) like的内容不是正则，而是通配符。像mysql中的\"like\",但是建议使用高级函数\"instr\"效率更高。\n\n- (2) rlike的内容可以是正则，正则的写法与java一样。需要转义，例如’\\m’需要使用’\\m’\n\n- (3) regexp == rlike 同义词 not like not regexp\n\n\n<!--more-->\n\n## 二、Like常用方法\n### 1.like关键字\nlike有两个模式：_和%\n\n_：表示单个字符，用来查询定长的数据\n\n%：表示0个或多个任意字符\n\n### 2.示例\n```\n（1）SELECT * FROM Persons  WHERE City LIKE 'N%'     \"Persons\" 表中选取居住在以 \"N\" 开始的城市里的人\n（2）SELECT * FROM Persons  WHERE City LIKE '%g'     \"Persons\" 表中选取居住在以 \"g\" 结尾的城市里的人\n（3）SELECT * FROM Persons   WHERE City LIKE '%lon%'  从 \"Persons\" 表中选取居住在包含 \"lon\" 的城市里的人\n（4）SELECT * FROM Persons   WHERE City NOT LIKE '%lon%'  从 \"Persons\" 表中选取居住在不包含 \"lon\" 的城市里的人\n```\n## 三、Mysql中Regexp常见用法\n\n* 模糊匹配，包含特定字符串\n```\n  #查找content字段中包含“车友俱乐部”的记录\n select * from club_content where content regexp '车友俱乐部'\n\n# 此时的regexp与like的以下用法是等同的\nselect * from club_content where content like '%车友俱乐部%'\n```\n* 模糊匹配，以特定字符串开头\n```\n# 查找content字段中以“车友”开头的记录\nselect * from club_content where content regexp '^车友'\n\n# 此时的regexp与like的以下用法是等同的\nselect * from club_content where content like '车友%'\n```\n\n* 模糊匹配，以特定字符串结尾\n```\n# 查找content字段中以“车友”结尾的记录\nselect * from club_content where content regexp '车友$'\n\n# 此时的regexp与like的以下用法是等同的\nselect * from club_content where content like '%车友'\n```\n* 模糊匹配 或关系\n```\n# 查找content字段中包含“心得”、“分享”或“技术贴”\nselect * from club_content where content  REGEXP '心得|分享|技术贴'\n```\n* 模糊匹配，不包含单个字符\n```\n# 查找content字段中不包含“车”字、“友”字的记录\nselect * from club_content where content  REGEXP [^车友]\n```\n这个结果跑出来一看大吃一惊，竟然把所有记录给跑出来，这是为什么呢？\n因为一旦加了这个方括号\"[]\"，它就把里面的内容拆成单个的字符再匹配，它会逐个字符去匹配判断是不是等于“车”，或者是不是等于“友“，返回的结果是一组0、1的逻辑值。\n\n如果想匹配不包含特定字符串，该怎么实现呢？\n\n模糊匹配，不包含特定字符串\n```\n# 查找content字段不包含“车友”字符串的记录\nselect * from club_content where content not REGEXP '车友'\n```\n\n```\n交集\n表的字段就是\nname  no\na     2,9\nb     8,10\n字符串是str=\"0,1,2,3,4\"\n接下来就是查 no字段里跟str里有交集的记录\n查询的结果就是name=a的,no=2,9的\nselect * from table1 where concat(',',no,',') regexp concat(',0,|,1,|,2,|,3,|,4,');\n\n某字段中搜索\n可以使用FIND_IN_SET\nname  no\na     2,9\nb     8,10\n想查出no中包含2的记录\nselect * from table1 where FIND_IN_SET('2', no)\n\n替换某字段中的内容\nUPDATE `blog_iplimit` SET `ip` = REPLACE(`ip`, ',', '')\n```\n原文链接：https://blog.csdn.net/ZZQHELLO2018/java/article/details/92794555\n","tags":["mysql","sql"],"categories":["数据库"]},{"title":"macOS pip 安装 mysqlclient 报错","url":"/2021/05/07/数据库/macOS pip 安装 mysqlclient 报错/","content":"\n```\nld: library not found for -lssl\nclang: error: linker command failed with exit code 1 (use -v to see invocation)\nerror: command 'clang' failed with exit status 1\n```\n```\nexport LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/\n```\n然后再安装\n","tags":["macOS","mysql","mysqlclient"],"categories":["数据库"]},{"title":"mysql  show slave status 状态详解","url":"/2021/05/07/数据库/mysql  show slave status 状态详解/","content":"MySQL同步功能由3个线程(master上1个，slave上2个)来实现。执行 DE>START SLAVEDE> 语句后，slave就创建一个I/O线程。I/O线程连接到master上，并请求master发送二进制日志中的语句。master创建一个线程来把日志的内容发送到slave上。这个线程在master上执行 DE>SHOW PROCESSLISTDE> 语句后的结果中的 DE>Binlog DumpDE> 线程便是。slave上的I/O线程读取master的 DE>Binlog DumpDE> 线程发送的语句，并且把它们拷贝到其数据目录下的中继日志(relay logs)中。第三个是SQL线程，salve用它来读取中继日志，然后执行它们来更新数据。\n\n如上所述，每个master/slave上都有3个线程。每个master上有多个线程，它为每个slave连接都创建一个线程，每个slave只有I/O和SQL线程。\n\nshow slave master 用于提供有关从属服务器线程的关键参数的信息:\n<!--more-->\n```\nmysql> show slave status \\G;\n*************************** 1. row ***************************\n               Slave_IO_State: Waiting for master to send event\n                Master_Host: 172.17.2.40\n                Master_User: photorepl\n                Master_Port: 4331\n               Connect_Retry: 60\n               Master_Log_File: mysql-bin.005502\n                Read_Master_Log_Pos: 64401238\n               Relay_Log_File: mysqld-relay-bin.015418\n               Relay_Log_Pos: 13456757\n               Relay_Master_Log_File: mysql-bin.005152\n               Slave_IO_Running: Yes\n              Slave_SQL_Running: Yes\n               Replicate_Do_DB:\n               Replicate_Ignore_DB: mysql\n               Replicate_Do_Table:\n               Replicate_Ignore_Table:\n               Replicate_Wild_Do_Table: photo.%\n               Replicate_Wild_Ignore_Table: mysql.%\n                 Last_Errno: 0\n                 Last_Error:\n               Skip_Counter: 0\n               Exec_Master_Log_Pos: 13456620\n              Relay_Log_Space: 36764898503\n               Until_Condition: None\n              Until_Log_File:\n              Until_Log_Pos: 0\n              Master_SSL_Allowed: No\n             Master_SSL_CA_File:\n             Master_SSL_CA_Path:\n              Master_SSL_Cert:\n              Master_SSL_Cipher:\n             Master_SSL_Key:\n            Seconds_Behind_Master: 249904\n××××××××××××××××××××××××××××××××××××××××××××××××××××××××××\n```\n\nSHOW SLAVE STATUS会返回以下字段：\n```\n Slave_IO_State\n\nSHOW PROCESSLIST输出的State字段的拷贝。SHOW PROCESSLIST用于从属I/O线程。如果线程正在试图连接到主服务器，正在等待来自主服务器的时间或正在连接到主服务器等，本语句会通知您\n\n Master_User\n\n被用于连接主服务器的当前用户。\n\nMaster_Port\n\n当前的主服务器接口。\n\nConnect_Retry\n\n--master-connect-retry选项的当前值\n\nMaster_Log_File\n\nI/O线程当前正在读取的主服务器二进制日志文件的名称。\n\nRead_Master_Log_Pos\n\n在当前的主服务器二进制日志中，I/O线程已经读取的位置。\n\nRelay_Log_File\n\nSQL线程当前正在读取和执行的中继日志文件的名称。\n\nRelay_Log_Pos\n\n在当前的中继日志中，SQL线程已读取和执行的位置。\n\nRelay_Master_Log_File\n\n由SQL线程执行的包含多数近期事件的主服务器二进制日志文件的名称。\n\nSlave_IO_Running\n\nI/O线程是否被启动并成功地连接到主服务器上。\n\nSlave_SQL_Running\n\nSQL线程是否被启动。\n\nReplicate_Do_DB,Replicate_Ignore_DB\n\n使用--replicate-do-db和--replicate-ignore-db选项指定的数据库清单。\n\nReplicate_Do_Table,Replicate_Ignore_Table,Replicate_Wild_Do_Table,Replicate_Wild_Ignore_Table\n\n使用--replicate-do-table,--replicate-ignore-table,--replicate-wild-do-table和--replicate-wild-ignore_table选项指定的表清单。\n\nLast_Errno,Last_Error\n\n被多数最近被执行的查询返回的错误数量和错误消息。错误数量为0并且消息为空字符串意味着“没有错误”。如果Last_Error值不是空值，它也会在从属服务器的错误日志中作为消息显示。\n\n举例说明：\n\nLast_Errno: 1051\n\nLast_Error: error 'Unknown table 'z'' on query 'drop table z'\n\n该消息指示，表z曾经存在于在主服务器中并已被取消了，但是它没有在从属服务器中存在过，因此对于从属服务器，DROP TABLE失败。（举例说明，在设置复制时，如果您忘记了把此表拷贝到从属服务器中，则这有可能发生。）\n\nSkip_Counter\n\n最近被使用的用于SQL_SLAVE_SKIP_COUNTER的值。\n\nExec_Master_Log_Pos\n\n来自主服务器的二进制日志的由SQL线程执行的上一个时间的位置（Relay_Master_Log_File）。在主服务器的二进制日志中的(Relay_Master_Log_File,Exec_Master_Log_Pos)对应于在中继日志中的(Relay_Log_File,Relay_Log_Pos)。\n\nRelay_Log_Space\n\n所有原有的中继日志结合起来的总大小。\n\nUntil_Condition,Until_Log_File,Until_Log_Pos\n\n在START SLAVE语句的UNTIL子句中指定的值。\n\nUntil_Condition具有以下值：\n\n如果没有指定UNTIL子句，则没有值\n\n如果从属服务器正在读取，直到达到主服务器的二进制日志的给定位置为止，则值为Master\n\n如果从属服务器正在读取，直到达到其中继日志的给定位置为止，则值为Relay\n\nUntil_Log_File和Until_Log_Pos用于指示日志文件名和位置值。日志文件名和位置值定义了SQL线程在哪个点中止执行。\n\nMaster_SSL_Allowed,Master_SSL_CA_File,Master_SSL_CA_Path,Master_SSL_Cert,Master_SSL_Cipher,Master_SSL_Key\n\n这些字段显示了被从属服务器使用的参数。这些参数用于连接主服务器。\n\nMaster_SSL_Allowed具有以下值：\n\n如果允许对主服务器进行SSL连接，则值为Yes\n\n如果不允许对主服务器进行SSL连接，则值为No\n\n如果允许SSL连接，但是从属服务器没有让SSL支持被启用，则值为Ignored。\n\n与SSL有关的字段的值对应于--master-ca,--master-capath,--master-cert,--master-cipher和--master-key选项的值。\n\nSeconds_Behind_Master\n\n本字段是从属服务器“落后”多少的一个指示。当从属SQL线程正在运行时（处理更新），本字段为在主服务器上由此线程执行的最近的一个事件的时间标记开始，已经过的秒数。当此线程被从属服务器I/O线程赶上，并进入闲置状态，等待来自I/O线程的更多的事件时，本字段为零。总之，本字段测量从属服务器SQL线程和从属服务器I/O线程之间的时间差距，单位以秒计。\n\n如果主服务器和从属服务器之间的网络连接较快，则从属服务器I/O线程会非常接近主服务器，所以本字段能够十分近似地指示，从属服务器SQL线程比主服务器落后多少。如果网络较慢，则这种指示不准确；从属SQL线程经常会赶上读取速度较慢地从属服务器I/O线程，因此，Seconds_Behind_Master经常显示值为0。即使I/O线程落后于主服务器时，也是如此。换句话说，本列只对速度快的网络有用。\n\n即使主服务器和从属服务器不具有相同的时钟，时间差计算也会起作用（当从属服务器I/O线程启动时，计算时间差。并假定从此时以后，时间差保持不变）。如果从属SQL线程不运行，或者如果从属服务器I/O线程不运行或未与主服务器连接，则Seconds_Behind_Master为NULL（意义为“未知”）。举例说明，如果在重新连接之前，从属服务器I/O线程休眠了master-connect-retry秒，则显示NULL，因为从属服务器不知道主服务器正在做什么，也不能有把握地说落后多少。\n```\n","tags":["mysql"],"categories":["数据库"]},{"title":"percona-toolkit工具（数据一致性监测、延迟监控）使用梳理","url":"/2021/05/07/tools/percona-toolkit工具（数据一致性监测、延迟监控）使用梳理/","content":"[原文链接: MySQL 主从同步(3)-percona-toolkit工具（数据一致性监测、延迟监控）使用梳理\n](https://www.cnblogs.com/kevingrace/p/6261091.html)\n","tags":["percona-toolkit","监控","监测","一致性"],"categories":["工具"]},{"title":"Django的Model中不创建表格，并设为基类方法","url":"/2021/05/07/django/Django的Model中不创建表格，并设为基类方法/","content":"\n\n```\nclass UserInfo(models.Model):\n    \"\"\"用户表\"\"\"\n    name = models.CharField(verbose_name='用户名', max_length=32)\n    ...\n\n    class Meta:\n        # 此类可以当做父类，被其他model继承。字段自动过度给，继承的model\n        abstract = True  # 【django以后做数据库迁移时， 不再为UserInfo类创建相关的表以及表结构了】\n\n\n# app01/models.py\nclass UserInfo(RbacUserInfo):  # 继承上面那个userinfo\n    \"\"\"用户表\"\"\"\n    phone = models.CharField(verbose_name='联系方式', max_length=32)\n    depart = models.ForeignKey(verbose_name='部门', to='Department', on_delete=models.CASCADE)\n```\n首先介绍下django的模型有哪些属性：\n\n先看例子：\n\nDjango 模型类的Meta是一个内部类，它用于定义一些Django模型类的行为特性。以下对此作一总结：\n\nabstract\n 这个属性是定义当前的模型类是不是一个抽象类。所谓抽象类是不会对应数据库表的。一般我们用它来归纳一些公共属性字段，然后继承它的子类可以继承这些字段。比如下面的代码中Human是一个抽象类，Employee是一个继承了Human的子类，那么在运行syncdb命令时，不会生成Human表，但是会生成一个Employee表，它包含了Human中继承来的字段，以后如果再添加一个Customer模型类，它可以同样继承Human的公共属性：\n <!--more-->\n```\nclass Human(models.Model):\n    name=models.CharField(max_length=100)\n    GENDER_CHOICE=((u'M',u'Male'),(u'F',u'Female'),)\n    gender=models.CharField(max_length=2,choices=GENDER_CHOICE,null=True)\n    class Meta:\n        abstract=True\nclass Employee(Human):\n    joint_date=models.DateField()\nclass Customer(Human):\n    first_name=models.CharField(max_length=100)\n    birth_day=models.DateField()\n```\n\n上面的代码，执行python manage.py syncdb 后的输出结果入下，可以看出Human表并没有被创建:\n```\n$ python manage.py syncdb\nCreating tables ...\nCreating table myapp_employee\nCreating table myapp_customer\nInstalling custom SQL ...\nInstalling indexes ...\nNo fixtures found.\n•app_label\n```\n\napp_label这个选项只在一种情况下使用，就是你的模型类不在默认的应用程序包下的models.py文件中，这时候你需要指定你这个模型类是那个应用程序的。比如你在其他地方写了一个模型类，而这个模型类是属于myapp的，那么你这是需要指定为：\n```\napp_label='myapp'\n•db_table\n```\ndb_table是用于指定自定义数据库表名的。Django有一套默认的按照一定规则生成数据模型对应的数据库表名，如果你想使用自定义的表名，就通过这个属性指定，比如：\n```\ntable_name='my_owner_table'\n•db_tablespace\n```\n有些数据库有数据库表空间，比如Oracle。你可以通过db_tablespace来指定这个模型对应的数据库表放在哪个数据库表空间。\n```\n•get_latest_by\n```\n由于Django的管理方法中有个lastest()方法，就是得到最近一行记录。如果你的数据模型中有 DateField 或 DateTimeField 类型的字段，你可以通过这个选项来指定lastest()是按照哪个字段进行选取的。\n```\n•managed\n```\n由于Django会自动根据模型类生成映射的数据库表，如果你不希望Django这么做，可以把managed的值设置为False。\n```\n•order_with_respect_to\n```\n这个选项一般用于多对多的关系中，它指向一个关联对象。就是说关联对象找到这个对象后它是经过排序的。指定这个属性后你会得到一个get_XXX_order()和set_XXX_order（）的方法,通过它们你可以设置或者回去排序的对象。\n```\n•ordering\n```\n这个字段是告诉Django模型对象返回的记录结果集是按照哪个字段排序的。比如下面的代码：\n```\nordering=['order_date'] # 按订单升序排列\nordering=['-order_date'] # 按订单降序排列，-表示降序\nordering=['?order_date'] # 随机排序，？表示随机\n•permissions\npermissions主要是为了在Django Admin管理模块下使用的，如果你设置了这个属性可以让指定的方法权限描述更清晰可读。\n•proxy\n这是为了实现代理模型使用的，这里先不讲随后的文章介绍。\n•unique_together\n```\n```\nunique_together这个选项用于：当你需要通过两个字段保持唯一性时使用。比如假设你希望，一个Person的FirstName和LastName两者的组合必须是唯一的，那么需要这样设置：\nunique_together = ((\"first_name\", \"last_name\"),)\n•verbose_name\nverbose_name的意思很简单，就是给你的模型类起一个更可读的名字：\nverbose_name = \"pizza\"\n•verbose_name_plural\n这个选项是指定，模型的复数形式是什么，比如：\nverbose_name_plural = \"stories\"\n如果不指定Django会自动在模型名称后加一个’s’\n```\n```\n1.class Register(models.Model):\n2.    id = models.IntegerField(primary_key = True, db_column=\"ID\")\n3.    mid = models.IntegerField(db_column = \"MID\")\n4.    name = models.CharField(max_length = 10 , db_column = \"NAME\")\n5.    nickName = models.CharField(max_length = 100 ,db_column = \"NICK_NAME\")\n6.    slo = models.CharField(max_length = 50, db_column = \"SLOGAN\")\n7.    status = models.SmallIntegerField(db_column = \"STATUS\")\n8.    cnt = models.IntegerField(db_column = \"CNT\")\n9.    createdDate = models.DateTimeField(db_column = \"CREATED_DATE\")\n10.\n11.    class Meta:\n12.        db_table = \"A111208FACTIONVOTETOP10_REGISTER\"\n13.        managed = False\n```\n\n\n原文：[django模型中的抽象类（abstract）](https://www.cnblogs.com/xuchunlin/p/5920545.html)\n","tags":["抽象类","abstract"],"categories":["django"]},{"title":"supervisor安装、配置及常用命令","url":"/2021/05/07/Linux web/supervisor安装、配置及常用命令/","content":"\n\n\n\n# **前言**\n\n在 web 应用部署到线上后，需要保证应用一直处于运行状态，在遇到程序异常、报错等情况，导致 web 应用终止时，需要保证程序可以立刻重启，继续提供服务。\n\n而Supervisor 就是解决这种问题的工具，可以提供程序崩溃后，重新把程序启动起来等功能。\n\n<!--more-->\n\n------\n\n# **简介**\n\nSupervisor 是一个用 Python 写的进程管理工具，可以很方便的用来在 UNIX-like 系统（不支持 Windows）下启动、重启（自动重启程序）、关闭进程（不仅仅是 Python 进程）。\n\n> Supervisor is a client/server system that allows its users to control a number of processes on UNIX-like operating systems.\n\n------\n\n# **安装**\n\n1、redhat/centos系统\n\n安装命令：yum install supervisor，通过这种方式安装后，自动设置为开机启动\n\n2、Ubuntu系统：\n\n安装命令：apt-get install supervisor，通过这种方式安装后，自动设置为开机启动\n\n3、pip命令（不推荐）\n\n通过 pip install supervisor 进行安装，但是需要手动启动，然后设置为开机启动。\n\n------\n\n# **Supervisor 配置**\n\nSupervisor 是一个 C/S 模型的程序，supervisord 是 server 端，supervisorctl 是 client 端。\n\n**supervisord**\n\n下面介绍 supervisord 配置方法。supervisord 的配置文件默认位于 /etc/supervisord.conf，内容如下（;后面为注释）：\n\n```\n[unix_http_server]\nfile=/tmp/supervisor.sock ;UNIX socket 文件，supervisorctl 会使用\n;chmod=0700 ;socket文件的mode，默认是0700\n;chown=nobody:nogroup ;socket文件的owner，格式：uid:gid\n;[inet_http_server] ;HTTP服务器，提供web管理界面\n;port=127.0.0.1:9001 ;Web管理后台运行的IP和端口，如果开放到公网，需要注意安全性\n;username=user ;登录管理后台的用户名\n;password=123 ;登录管理后台的密码\n[supervisord]\nlogfile=/tmp/supervisord.log ;日志文件，默认是 $CWD/supervisord.log\nlogfile_maxbytes=50MB ;日志文件大小，超出会rotate，默认 50MB，如果设成0，表示不限制大小\nlogfile_backups=10 ;日志文件保留备份数量默认10，设为0表示不备份\nloglevel=info ;日志级别，默认info，其它: debug,warn,trace\npidfile=/tmp/supervisord.pid ;pid 文件\nnodaemon=false ;是否在前台启动，默认是false，即以 daemon 的方式启动\nminfds=1024 ;可以打开的文件描述符的最小值，默认 1024\nminprocs=200 ;可以打开的进程数的最小值，默认 200\n[supervisorctl]\nserverurl=unix:///tmp/supervisor.sock ;通过UNIX socket连接supervisord，路径与unix_http_server部分的file一致\n;serverurl=http://127.0.0.1:9001 ; 通过HTTP的方式连接supervisord\n; [program:xx]是被管理的进程配置参数，xx是进程的名称\n[program:xx]\ncommand=/opt/apache-tomcat-8.0.35/bin/catalina.sh run ; 程序启动命令\nautostart=true ; 在supervisord启动的时候也自动启动\nstartsecs=10 ; 启动10秒后没有异常退出，就表示进程正常启动了，默认为1秒\nautorestart=true ; 程序退出后自动重启,可选值：[unexpected,true,false]，默认为unexpected，表示进程意外杀死后才重启\nstartretries=3 ; 启动失败自动重试次数，默认是3\nuser=tomcat ; 用哪个用户启动进程，默认是root\npriority=999 ; 进程启动优先级，默认999，值小的优先启动\nredirect_stderr=true ; 把stderr重定向到stdout，默认false\nstdout_logfile_maxbytes=20MB ; stdout 日志文件大小，默认50MB\nstdout_logfile_backups = 20 ; stdout 日志文件备份数，默认是10\n; stdout 日志文件，需要注意当指定目录不存在时无法正常启动，所以需要手动创建目录（supervisord 会自动创建日志文件）\nstdout_logfile=/opt/apache-tomcat-8.0.35/logs/catalina.out\nstopasgroup=false ;默认为false,进程被杀死时，是否向这个进程组发送stop信号，包括子进程\nkillasgroup=false ;默认为false，向进程组发送kill信号，包括子进程\n;包含其它配置文件\n[include]\nfiles = relative/directory/*.ini ;可以指定一个或多个以.ini结束的配置文件\n```\n\n------\n\n# **配置管理进程**\n\n进程管理配置参数，不建议全都写在supervisord.conf文件中，应该每个进程写一个配置文件放在include指定的目录下包含进supervisord.conf文件中。\n\n1> 创建/etc/supervisor/config.d目录，用于存放进程管理的配置文件\n\n2> 修改/etc/supervisor/supervisord.conf中的include参数，将/etc/supervisor/conf.d目录添加到include\n\n下面是配置Tomcat进程的一个例子：\n\n```\n[program:tomcat]\ncommand=/opt/apache-tomcat-8.0.35/bin/catalina.sh run\nstdout_logfile=/opt/apache-tomcat-8.0.35/logs/catalina.out\nautostart=true\nautorestart=true\nstartsecs=5\npriority=1\nstopasgroup=true\nkillasgroup=true\n```\n\n------\n\n# **启动Supervisor服务**\n\n```\nsupervisord -c /etc/supervisor/supervisord.conf\n```\n\n------\n\n# Web管理界面\n\n出于安全考虑，默认配置是没有开启web管理界面，需要修改supervisord.conf配置文件打开http访权限，将下面的配置：\n\n```\n;[inet_http_server] ; inet (TCP) server disabled by default\n;port=127.0.0.1:9001 ; (ip_address:port specifier, *:port for all iface)\n;username=user ; (default is no username (open server))\n;password=123 ; (default is no password (open server))\n```\n\n修改成：\n\n```\n[inet_http_server] ; inet (TCP) server disabled by default\nport=0.0.0.0:9001 ; (ip_address:port specifier, *:port for all iface)\nusername=user ; (default is no username (open server))\npassword=123 ; (default is no password (open server))\n```\n\n\n# 注意：一定要重启supervisord服务配置才会生效\n![image](https://user-images.githubusercontent.com/28568478/117405693-98200300-af3e-11eb-9d60-ad2de6f7cf8f.png)\n重启后访问 ip:9001 (账号密码为user/123)\n\n![image](https://user-images.githubusercontent.com/28568478/117405729-a8d07900-af3e-11eb-9b65-92e32c697a91.png)\n![image](https://user-images.githubusercontent.com/28568478/117405738-ad952d00-af3e-11eb-97bd-952990e6d59e.png)\n\n\n\n\n------\n\n# **supervisorctl 操作**\n\nsupervisorctl 是 supervisord 的命令行客户端工具，使用的配置和 supervisord 一样，这里就不再说了。下面，主要介绍 supervisorctl 操作的常用命令：\n\n输入命令 supervisorctl 进入 supervisorctl 的 shell 交互界面（还是纯命令行），就可以在下面输入命令了。：\n\n- help # 查看帮助\n- status # 查看程序状态\n- stop program_name # 关闭 指定的程序\n- start program_name # 启动 指定的程序\n- restart program_name # 重启 指定的程序\n- tail -f program_name # 查看 该程序的日志\n- update # 重启配置文件修改过的程序（修改了配置，通过这个命令加载新的配置)\n\n也可以直接通过 shell 命令操作：\n\n- supervisorctl status\n- supervisorctl update\n- .....\n\n![image](https://user-images.githubusercontent.com/28568478/117405782-bdad0c80-af3e-11eb-8c93-59c6b3998e66.png)\n\n# 配置详情\n```\n[unix_http_server]\nfile=/tmp/supervisor.sock   ; socket文件的路径，supervisorctl用XML_RPC和supervisord通信就是通过它进行\n                              的。如果不设置的话，supervisorctl也就不能用了\n                              不设置的话，默认为none。 非必须设置\n;chmod=0700                 ; 这个简单，就是修改上面的那个socket文件的权限为0700\n                              不设置的话，默认为0700。 非必须设置\n;chown=nobody:nogroup       ; 这个一样，修改上面的那个socket文件的属组为user.group\n                              不设置的话，默认为启动supervisord进程的用户及属组。非必须设置\n;username=user              ; 使用supervisorctl连接的时候，认证的用户\n                               不设置的话，默认为不需要用户。 非必须设置\n;password=123               ; 和上面的用户名对应的密码，可以直接使用明码，也可以使用SHA加密\n                              如：{SHA}82ab876d1387bfafe46cc1c8a2ef074eae50cb1d\n                              默认不设置。。。非必须设置\n\n;[inet_http_server]         ; 侦听在TCP上的socket，Web Server和远程的supervisorctl都要用到他\n                              不设置的话，默认为不开启。非必须设置\n;port=127.0.0.1:9001        ; 这个是侦听的IP和端口，侦听所有IP用 :9001或*:9001。\n                              这个必须设置，只要上面的[inet_http_server]开启了，就必须设置它\n;username=user              ; 这个和上面的uinx_http_server一个样。非必须设置\n;password=123               ; 这个也一个样。非必须设置\n\n[supervisord]                ;这个主要是定义supervisord这个服务端进程的一些参数的\n                              这个必须设置，不设置，supervisor就不用干活了\nlogfile=/tmp/supervisord.log ; 这个是supervisord这个主进程的日志路径，注意和子进程的日志不搭嘎。\n                               默认路径$CWD/supervisord.log，$CWD是当前目录。。非必须设置\nlogfile_maxbytes=50MB        ; 这个是上面那个日志文件的最大的大小，当超过50M的时候，会生成一个新的日\n                               志文件。当设置为0时，表示不限制文件大小\n                               默认值是50M，非必须设置。\nlogfile_backups=10           ; 日志文件保持的数量，supervisor在启动程序时，会自动创建10个buckup文件，用于log rotate\n                               当设置为0时，表示不限制文件的数量。\n                               默认情况下为10。。。非必须设置\nloglevel=info                ; 日志级别，有critical, error, warn, info, debug, trace, or blather等\n                               默认为info。。。非必须设置项\npidfile=/tmp/supervisord.pid ; supervisord的pid文件路径。\n                               默认为$CWD/supervisord.pid。。。非必须设置\nnodaemon=false               ; 如果是true，supervisord进程将在前台运行\n                               默认为false，也就是后台以守护进程运行。。。非必须设置\nminfds=1024                  ; 这个是最少系统空闲的文件描述符，低于这个值supervisor将不会启动。\n                               系统的文件描述符在这里设置cat /proc/sys/fs/file-max\n                               默认情况下为1024。。。非必须设置\nminprocs=200                 ; 最小可用的进程描述符，低于这个值supervisor也将不会正常启动。\n                              ulimit  -u这个命令，可以查看linux下面用户的最大进程数\n                              默认为200。。。非必须设置\n;umask=022                   ; 进程创建文件的掩码\n                               默认为022。。非必须设置项\n;user=chrism                 ; 这个参数可以设置一个非root用户，当我们以root用户启动supervisord之后。\n                               我这里面设置的这个用户，也可以对supervisord进行管理\n                               默认情况是不设置。。。非必须设置项\n;identifier=supervisor       ; 这个参数是supervisord的标识符，主要是给XML_RPC用的。当你有多个\n                               supervisor的时候，而且想调用XML_RPC统一管理，就需要为每个\n                               supervisor设置不同的标识符了\n                               默认是supervisord。。。非必需设置\n;directory=/tmp              ; 这个参数是当supervisord作为守护进程运行的时候，设置这个参数的话，启动\n                               supervisord进程之前，会先切换到这个目录\n                               默认不设置。。。非必须设置\n;nocleanup=true              ; 这个参数当为false的时候，会在supervisord进程启动的时候，把以前子进程\n                               产生的日志文件(路径为AUTO的情况下)清除掉。有时候咱们想要看历史日志，当\n                               然不想日志被清除了。所以可以设置为true\n                               默认是false，有调试需求的同学可以设置为true。。。非必须设置\n;childlogdir=/tmp            ; 当子进程日志路径为AUTO的时候，子进程日志文件的存放路径。\n                               默认路径是这个东西，执行下面的这个命令看看就OK了，处理的东西就默认路径\n                               python -c \"import tempfile;print tempfile.gettempdir()\"\n                               非必须设置\n;environment=KEY=\"value\"     ; 这个是用来设置环境变量的，supervisord在linux中启动默认继承了linux的\n                               环境变量，在这里可以设置supervisord进程特有的其他环境变量。\n                               supervisord启动子进程时，子进程会拷贝父进程的内存空间内容。 所以设置的\n                               这些环境变量也会被子进程继承。\n                               小例子：environment=name=\"haha\",age=\"hehe\"\n                               默认为不设置。。。非必须设置\n;strip_ansi=false            ; 这个选项如果设置为true，会清除子进程日志中的所有ANSI 序列。什么是ANSI\n                               序列呢？就是我们的\\n,\\t这些东西。\n                               默认为false。。。非必须设置\n\n; the below section must remain in the config file for RPC\n; (supervisorctl/web interface) to work, additional interfaces may be\n; added by defining them in separate rpcinterface: sections\n[rpcinterface:supervisor]    ;这个选项是给XML_RPC用的，当然你如果想使用supervisord或者web server 这\n                              个选项必须要开启的\nsupervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface\n\n[supervisorctl]              ;这个主要是针对supervisorctl的一些配置\nserverurl=unix:///tmp/supervisor.sock ; 这个是supervisorctl本地连接supervisord的时候，本地UNIX socket\n                                        路径，注意这个是和前面的[unix_http_server]对应的\n                                        默认值就是unix:///tmp/supervisor.sock。。非必须设置\n;serverurl=http://127.0.0.1:9001 ; 这个是supervisorctl远程连接supervisord的时候，用到的TCP socket路径\n                                   注意这个和前面的[inet_http_server]对应\n                                   默认就是http://127.0.0.1:9001。。。非必须项\n\n;username=chris              ; 用户名\n                               默认空。。非必须设置\n;password=123                ; 密码\n                              默认空。。非必须设置\n;prompt=mysupervisor         ; 输入用户名密码时候的提示符\n                               默认supervisor。。非必须设置\n;history_file=~/.sc_history  ; 这个参数和shell中的history类似，我们可以用上下键来查找前面执行过的命令\n                               默认是no file的。。所以我们想要有这种功能，必须指定一个文件。。。非\n                               必须设置\n\n; The below sample program section shows all possible program subsection values,\n; create one or more 'real' program: sections to be able to control them under\n; supervisor.\n\n;[program:theprogramname]      ;这个就是咱们要管理的子进程了，\":\"后面的是名字，最好别乱写和实际进程\n                                有点关联最好。这样的program我们可以设置一个或多个，一个program就是\n                                要被管理的一个进程\n;command=/bin/cat              ; 这个就是我们的要启动进程的命令路径了，可以带参数\n                                例子：/home/test.py -a 'hehe'\n                                有一点需要注意的是，我们的command只能是那种在终端运行的进程，不能是\n                                守护进程。这个想想也知道了，比如说command=service httpd start。\n                                httpd这个进程被linux的service管理了，我们的supervisor再去启动这个命令\n                                这已经不是严格意义的子进程了。\n                                这个是个必须设置的项\n;process_name=%(program_name)s ; 这个是进程名，如果我们下面的numprocs参数为1的话，就不用管这个参数\n                                 了，它默认值%(program_name)s也就是上面的那个program冒号后面的名字，\n                                 但是如果numprocs为多个的话，那就不能这么干了。想想也知道，不可能每个\n                                 进程都用同一个进程名吧。\n\n\n;numprocs=1                    ; 启动进程的数目。当不为1时，就是进程池的概念，注意process_name的设置\n                                 默认为1    。。非必须设置\n;directory=/tmp                ; 进程运行前，会前切换到这个目录\n                                 默认不设置。。。非必须设置\n;umask=022                     ; 进程掩码，默认none，非必须\n;priority=999                  ; 子进程启动关闭优先级，优先级低的，最先启动，关闭的时候最后关闭\n                                 默认值为999 。。非必须设置\n;autostart=true                ; 如果是true的话，子进程将在supervisord启动后被自动启动\n                                 默认就是true   。。非必须设置\n;autorestart=unexpected        ; 这个是设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected\n                                 和true。如果为false的时候，无论什么情况下，都不会被重新启动，\n                                 如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的退\n                                 出码的时候，才会被自动重启。当为true的时候，只要子进程挂掉，将会被无\n                                 条件的重启\n;startsecs=1                   ; 这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启\n                                 动成功了\n                                 默认值为1 。。非必须设置\n;startretries=3                ; 当进程启动失败后，最大尝试启动的次数。。当超过3次后，supervisor将把\n                                 此进程的状态置为FAIL\n                                 默认值为3 。。非必须设置\n;exitcodes=0,2                 ; 注意和上面的的autorestart=unexpected对应。。exitcodes里面的定义的\n                                 退出码是expected的。\n;stopsignal=QUIT               ; 进程停止信号，可以为TERM, HUP, INT, QUIT, KILL, USR1, or USR2等信号\n                                  默认为TERM 。。当用设定的信号去干掉进程，退出码会被认为是expected\n                                  非必须设置\n;stopwaitsecs=10               ; 这个是当我们向子进程发送stopsignal信号后，到系统返回信息\n                                 给supervisord，所等待的最大时间。 超过这个时间，supervisord会向该\n                                 子进程发送一个强制kill的信号。\n                                 默认为10秒。。非必须设置\n;stopasgroup=false             ; 这个东西主要用于，supervisord管理的子进程，这个子进程本身还有\n                                 子进程。那么我们如果仅仅干掉supervisord的子进程的话，子进程的子进程\n                                 有可能会变成孤儿进程。所以咱们可以设置可个选项，把整个该子进程的\n                                 整个进程组都干掉。 设置为true的话，一般killasgroup也会被设置为true。\n                                 需要注意的是，该选项发送的是stop信号\n                                 默认为false。。非必须设置。。\n;killasgroup=false             ; 这个和上面的stopasgroup类似，不过发送的是kill信号\n;user=chrism                   ; 如果supervisord是root启动，我们在这里设置这个非root用户，可以用来\n                                 管理该program\n                                 默认不设置。。。非必须设置项\n;redirect_stderr=true          ; 如果为true，则stderr的日志会被写入stdout日志文件中\n                                 默认为false，非必须设置\n;stdout_logfile=/a/path        ; 子进程的stdout的日志路径，可以指定路径，AUTO，none等三个选项。\n                                 设置为none的话，将没有日志产生。设置为AUTO的话，将随机找一个地方\n                                 生成日志文件，而且当supervisord重新启动的时候，以前的日志文件会被\n                                 清空。当 redirect_stderr=true的时候，sterr也会写进这个日志文件\n;stdout_logfile_maxbytes=1MB   ; 日志文件最大大小，和[supervisord]中定义的一样。默认为50\n;stdout_logfile_backups=10     ; 和[supervisord]定义的一样。默认10\n;stdout_capture_maxbytes=1MB   ; 这个东西是设定capture管道的大小，当值不为0的时候，子进程可以从stdout\n                                 发送信息，而supervisor可以根据信息，发送相应的event。\n                                 默认为0，为0的时候表达关闭管道。。。非必须项\n;stdout_events_enabled=false   ; 当设置为ture的时候，当子进程由stdout向文件描述符中写日志的时候，将\n                                 触发supervisord发送PROCESS_LOG_STDOUT类型的event\n                                 默认为false。。。非必须设置\n;stderr_logfile=/a/path        ; 这个东西是设置stderr写的日志路径，当redirect_stderr=true。这个就不用\n                                 设置了，设置了也是白搭。因为它会被写入stdout_logfile的同一个文件中\n                                 默认为AUTO，也就是随便找个地存，supervisord重启被清空。。非必须设置\n;stderr_logfile_maxbytes=1MB   ; 这个出现好几次了，就不重复了\n;stderr_logfile_backups=10     ; 这个也是\n;stderr_capture_maxbytes=1MB   ; 这个一样，和stdout_capture一样。 默认为0，关闭状态\n;stderr_events_enabled=false   ; 这个也是一样，默认为false\n;environment=A=\"1\",B=\"2\"       ; 这个是该子进程的环境变量，和别的子进程是不共享的\n;serverurl=AUTO                ;\n\n; The below sample eventlistener section shows all possible\n; eventlistener subsection values, create one or more 'real'\n; eventlistener: sections to be able to handle event notifications\n; sent by supervisor.\n\n;[eventlistener:theeventlistenername] ;这个东西其实和program的地位是一样的，也是suopervisor启动的子进\n                                       程，不过它干的活是订阅supervisord发送的event。他的名字就叫\n                                       listener了。我们可以在listener里面做一系列处理，比如报警等等\n                                       楼主这两天干的活，就是弄的这玩意\n;command=/bin/eventlistener    ; 这个和上面的program一样，表示listener的可执行文件的路径\n;process_name=%(program_name)s ; 这个也一样，进程名，当下面的numprocs为多个的时候，才需要。否则默认就\n                                 OK了\n;numprocs=1                    ; 相同的listener启动的个数\n;events=EVENT                  ; event事件的类型，也就是说，只有写在这个地方的事件类型。才会被发送\n\n\n;buffer_size=10                ; 这个是event队列缓存大小，单位不太清楚，楼主猜测应该是个吧。当buffer\n                                 超过10的时候，最旧的event将会被清除，并把新的event放进去。\n                                 默认值为10。。非必须选项\n;directory=/tmp                ; 进程执行前，会切换到这个目录下执行\n                                 默认为不切换。。。非必须\n;umask=022                     ; 淹没，默认为none，不说了\n;priority=-1                   ; 启动优先级，默认-1，也不扯了\n;autostart=true                ; 是否随supervisord启动一起启动，默认true\n;autorestart=unexpected        ; 是否自动重启，和program一个样，分true,false,unexpected等，注意\n                                  unexpected和exitcodes的关系\n;startsecs=1                   ; 也是一样，进程启动后跑了几秒钟，才被认定为成功启动，默认1\n;startretries=3                ; 失败最大尝试次数，默认3\n;exitcodes=0,2                 ; 期望或者说预料中的进程退出码，\n;stopsignal=QUIT               ; 干掉进程的信号，默认为TERM，比如设置为QUIT，那么如果QUIT来干这个进程\n                                 那么会被认为是正常维护，退出码也被认为是expected中的\n;stopwaitsecs=10               ; max num secs to wait b4 SIGKILL (default 10)\n;stopasgroup=false             ; send stop signal to the UNIX process group (default false)\n;killasgroup=false             ; SIGKILL the UNIX process group (def false)\n;user=chrism                   ;设置普通用户，可以用来管理该listener进程。\n                                默认为空。。非必须设置\n;redirect_stderr=true          ; 为true的话，stderr的log会并入stdout的log里面\n                                默认为false。。。非必须设置\n;stdout_logfile=/a/path        ; 这个不说了，好几遍了\n;stdout_logfile_maxbytes=1MB   ; 这个也是\n;stdout_logfile_backups=10     ; 这个也是\n;stdout_events_enabled=false   ; 这个其实是错的，listener是不能发送event\n;stderr_logfile=/a/path        ; 这个也是\n;stderr_logfile_maxbytes=1MB   ; 这个也是\n;stderr_logfile_backups        ; 这个不说了\n;stderr_events_enabled=false   ; 这个也是错的，listener不能发送event\n;environment=A=\"1\",B=\"2\"       ; 这个是该子进程的环境变量\n                                 默认为空。。。非必须设置\n;serverurl=AUTO                ; override serverurl computation (childutils)\n\n; The below sample group section shows all possible group values,\n; create one or more 'real' group: sections to create \"heterogeneous\"\n; process groups.\n\n;[group:thegroupname]  ;这个东西就是给programs分组，划分到组里面的program。我们就不用一个一个去操作了\n                         我们可以对组名进行统一的操作。 注意：program被划分到组里面之后，就相当于原来\n                         的配置从supervisor的配置文件里消失了。。。supervisor只会对组进行管理，而不再\n                         会对组里面的单个program进行管理了\n;programs=progname1,progname2  ; 组成员，用逗号分开\n                                 这个是个必须的设置项\n;priority=999                  ; 优先级，相对于组和组之间说的\n                                 默认999。。非必须选项\n\n; The [include] section can just contain the \"files\" setting.  This\n; setting can list multiple files (separated by whitespace or\n; newlines).  It can also contain wildcards.  The filenames are\n; interpreted as relative to this file.  Included files *cannot*\n; include files themselves.\n\n;[include]                         ;这个东西挺有用的，当我们要管理的进程很多的时候，写在一个文件里面\n                                    就有点大了。我们可以把配置信息写到多个文件中，然后include过来\n;files = relative/directory/*.ini\n\n\nOK,上面提到的非必须设置项，一般来说，都是有默认值的，可以根据自己的需要去设置。。。如果不设置的，supervisor也能用起来\n\n这一篇先总结到这里了，下一篇搞搞event和xml_rpc\n\n```\n[原文链接](https://www.toutiao.com/a6714650171180843524/?tt_from=weixin&utm_campaign=client_share&wxshare_count=1&timestamp=1563958876&app=news_article&utm_source=weixin&utm_medium=toutiao_android&req_id=201907241701150100230730859727DA2&group_id=6714650171180843524)\n","tags":["supervisor"],"categories":["Linux web"]},{"title":"apache 代理转发","url":"/2021/05/07/Linux web/apache 代理转发/","content":"## 一、转发时需开启如下：\n```\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_ajp\nsudo a2enmod proxy_balancer\nsudo a2enmod proxy_connect\nsudo a2enmod proxy_html\n```\n[参考链接](https://superuser.com/questions/1287647/apache2-not-starting-error-invalid-command-proxyrequests)\n\n<!--more-->\n\n## 二、相关代码：\n#### 80端口转发其他多个项目端口\n```\n<VirtualHost *:80>\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/html/xunke\n        ErrorLog /etc/apache2/error.log\n        CustomLog /etc/apache2/access.log combined\n        #ProxyPassMatch  /xunke/  http://localhost:8003/   # 匹配带有xunke的url 不适用\n        ProxyPass  /xunke  http://localhost:8003/\n        ProxyPassReverse /xunke  http://localhost:8003/\n</VirtualHost>\n<VirtualHost *:80>\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/html/topmps\n        ErrorLog /etc/apache2/error.log\n        CustomLog /etc/apache2/access.log combined\n        #ProxyPassMatch  /topmps/  http://localhost:8002/\n        ProxyPass  /topmps/  http://localhost:8002/\n        ProxyPassReverse  /topmps/  http://localhost:8002/\n</VirtualHost>\n```\n[参考链接1](https://blog.csdn.net/xiaokui_wingfly/article/details/51481653)\n[参考链接2](https://blog.csdn.net/u011277123/article/details/77165137)\n\n#### apache 监听多个端口\n- 目录：/etc/apache2/ports.conf\n```\nListen 80\nListen 8003\nListen 8004\n\n<IfModule ssl_module>\n        Listen 443\n</IfModule>\n\n<IfModule mod_gnutls.c>\n        Listen 443\n</IfModule>\n```\n\n- 目录：/etc/apache2/sites-available/000-default.conf\n```\n<VirtualHost *:8003>\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/html/xunke\n        ErrorLog /etc/apache2/error.log\n        CustomLog /etc/apache2/access.log combined\n</VirtualHost>\n\n<VirtualHost *:8004>\n        ServerAdmin webmaster@localhost\n        DocumentRoot /var/www/html/qibo\n        ErrorLog /etc/apache2/error.log\n        CustomLog /etc/apache2/access.log combined\n</VirtualHost>\n```\n\n","tags":["apache"],"categories":["Linux web"]},{"title":"nginx 只配置了一个域名，结果另一个域名也能访问","url":"/2021/05/07/Linux web/nginx 只配置了一个域名，结果另一个域名也能访问/","content":"今天自己在部署业务的时候， 一个同事说他用另一个域名访问到了我这个域名下的网页, 看来我自己的Nginx的配置，感觉没什么问题！\n```\nserver {\n           listen 80;\n           server_name www.hehe.com;\n           root /data1/htdocs/kaixuan.hehe.com/;\n           location ~ \\.php$ {\n                fastcgi_pass   127.0.0.1:9000;\n                fastcgi_index  index.php;\n                fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name;\n                include        fastcgi_params;\n           }\n           location / {\n                 index index.html;\n           }\n}\n```\n\n后来网上查了一下，发现如果当所有server的规则都不匹配时，nginx会采用第一条server配置，所以一般第一条server会使用阻止页面。这样的话，就需要在server上边再加一条server，加一条默认的阻挡。\n```\nserver {\n    listen  80 default;\n    listen  [::]:80 default;\n    server_name  _;\n\n    return 403;\n}\n\nserver {\n           listen 80;\n           server_name www.hehe.com;\n           root /data1/htdocs/kaixuan.hehe.com/;\n           location ~ \\.php$ {\n                fastcgi_pass   127.0.0.1:9000;\n                fastcgi_index  index.php;\n                fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name;\n                include        fastcgi_params;\n           }\n           location / {\n                 index index.html;\n           }\n}\n```\n","tags":["nginx"],"categories":["Linux web"]},{"title":"netstat 参数 及常用命令","url":"/2021/05/07/Linux/netstat 参数 及常用命令/","content":"### netstat 中参数选项\n```\n-a或--all：显示所有连线中的Socket；\n-A<网络类型>或--<网络类型>：列出该网络类型连线中的相关地址；\n-c或--continuous：持续列出网络状态；\n-C或--cache：显示路由器配置的快取信息；\n-e或--extend：显示网络其他相关信息；\n-F或--fib：显示FIB；\n-g或--groups：显示多重广播功能群组组员名单；\n-h或--help：在线帮助；\n-i或--interfaces：显示网络界面信息表单；\n-l或--listening：显示监控中的服务器的Socket；\n-M或--masquerade：显示伪装的网络连线；\n-n或--numeric：直接使用ip地址，而不通过域名服务器；\n-N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称；\n-o或--timers：显示计时器；\n-p或--programs：显示正在使用Socket的程序识别码和程序名称；\n-r或--route：显示Routing Table；\n-s或--statistice：显示网络工作信息统计表；\n-t或--tcp：显示TCP传输协议的连线状况；\n-u或--udp：显示UDP传输协议的连线状况；\n-v或--verbose：显示指令执行过程；\n-V或--version：显示版本信息；\n-w或--raw：显示RAW传输协议的连线状况；\n-x或--unix：此参数的效果和指定\"-A unix\"参数相同；\n--ip或--inet：此参数的效果和指定\"-A inet\"参数相同。\n```\n<!--more-->\n\n## 常用命令\n###查看linux的连接数，输出每个ip的连接数，以及总的各个状态的连接数\n```\nnetstat -n | awk '/^tcp/ {n=split($(NF-1),array,\":\");if(n<=2)++S[array[(1)]];else++S[array[(4)]];++s[$NF];++N} END {for(a in S){printf(\"%-20s %s\\n\", a, S[a]);++I}printf(\"%-20s %s\\n\",\"TOTAL_IP\",I);for(a in s) printf(\"%-20s %s\\n\",a, s[a]);printf(\"%-20s %s\\n\",\"TOTAL_LINK\",N);}'\n```\n###查看日志中不同端口对应的连接数\n```\ncat /home/ubuntu/log/gears-proxy-error.log |  egrep ':922[6-9]' | awk -F \"upstream\\\": \" '{print $2}' | awk -F, '{print $1}' |sort | uniq -c | sort -rn\n```\n###查看某些端口 有哪些ip链接  并且连接数有多少\n```\nnetstat -ntu |  egrep ':922[6-9]' | awk '{print $5}' | cut -d: -f1 | awk '{++ip[$1]} END {for(i in ip) print ip[i],\"\\t\",i}' | sort -nr\n```\n###如果发现某个端口被占用后，可以用命令查看，该端口到底是被哪个进程所占用。命令如下：\n```\nnetstat -pan | grep 5623\n```\n###查看进程程序名称\n```\nps -aux | grep pid\n```\n###查看tcp连接数量\n```\nnetstat -anptl | wc -l\n```\n###查看每个ip跟服务器建立的连接数\n```\nnetstat -nat|awk '{print$5}'|awk -F : '{print$1}'|sort|uniq -c|sort -rn\n```\n[netstat监控大量ESTABLISHED连接数和TIME_WAIT连接数题解决](https://blog.csdn.net/bluetjs/article/details/80965967\n)\n###查看每个ip跟服务器建立的连接数\n--（PS：正则解析：显示第5列，-F : 以：分割，显示列，sort 排序，uniq -c统计排序过程中的重复行，sort -rn 按纯数字进行逆序排序）\n```\nnetstat -nat|awk '{print$5}'|awk -F : '{print$1}'|sort|uniq -c|sort -rn\n```\n###查看每个ip建立的ESTABLISHED/TIME_OUT状态的连接数\n```\nnetstat -nat|grep ESTABLISHED|awk '{print$5}'|awk -F : '{print$1}'|sort|uniq -c|sort -rn\n```\n###查看不同状态的连接数数量\n```\nnetstat -an | awk '/^tcp/ {++y[$NF]} END {for(w in y) print w, y[w]}'\n```\n","tags":["netstat"],"categories":["Linux"]},{"title":"ubuntu 16.04 忘记root密码","url":"/2021/05/07/Linux/ubuntu 16.04 忘记root密码/","content":"\n**阅读目录**\n\n虚拟机中安装的ubuntu 16.04。\n\n## 方法一 \n\n如果用户具有sudo权限，那么直接可以运行如下命令： \n\n```\nsudo su root\n#输入当前用户的密码\npasswd\n#输入密码\n#再次输入密码\n```\n\n![image](https://user-images.githubusercontent.com/28568478/117405988-0a90e300-af3f-11eb-90d1-041c5b6914de.png)\n\n<!--more-->\n\n##  方法二\n\n如果用户不具备sudo权限，则方法一不能用，并需进入GRUB修改kernel镜像启动参数。 \n\n1、重启，按住shift键，出现如下界面，选中如下选项\n\n![image](https://user-images.githubusercontent.com/28568478/117406054-1aa8c280-af3f-11eb-9231-aac4431727a4.png)\n\n2、按回车键进入如下界面，然后选中有recovery mode的选项\n\n![image](https://user-images.githubusercontent.com/28568478/117406070-1f6d7680-af3f-11eb-88f2-f5bbaab55246.png)\n\n\n3、按e进入如下界面，找到图中红色框的recovery nomodeset并将其删掉，再在这一行的后面输入\n\n```\nquiet splash rw init=/bin/bash\n```\n![image](https://user-images.githubusercontent.com/28568478/117406078-23999400-af3f-11eb-9bb5-b42fb14bcefb.png)\n![image](https://user-images.githubusercontent.com/28568478/117406095-27c5b180-af3f-11eb-8b34-75af15a47672.png)\n\n\n4、接着按F10或者Ctrl+x 后出现如下界面，在命令行内输入passwd后进行修改密码即可\n\n![image](https://user-images.githubusercontent.com/28568478/117406109-2b593880-af3f-11eb-8ae9-fe7af949ff5d.png)\n\n\n修改完之后重启系统。\n\n---\n\n#修改某个用户目录下的sudo密码：\n\n1、进入root目录下\n2：输入命令 passwd david，(david是系统中已有的username)\n![image](https://user-images.githubusercontent.com/28568478/117406121-301dec80-af3f-11eb-98e6-bd744bc54431.png)\n\n3、重新定义密码即可\n\n#### [原文链接:  ubuntu 16.04 忘记root密码](https://www.cnblogs.com/xiaojianliu/p/8520313.html)\n","tags":["root","密码"],"categories":["Linux"]},{"title":"Linux增加开机启动项","url":"/2021/05/07/Linux web/Linux增加开机启动项/","content":"* vi /etc/rc.local\n* 按i键进入编辑模式，然后在最后一行加入需要开机启动的命令  例如:\n* ssserver -c /etc/shadowsocks/config.json -d start  --log-file /etc/shadowsocks/ss.log --pid-file /etc/shadowsocks/ss.pid\n","tags":["开机"],"categories":["Linux web"]},{"title":"linux下free命令详解","url":"/2021/05/07/Linux/linux下free命令详解/","content":"\nfree 命令显示系统内存的使用情况，包括物理内存、交换内存(swap)和内核缓冲区内存。\n\n![image](https://user-images.githubusercontent.com/28568478/117406270-6e1b1080-af3f-11eb-82b0-07b44528dcc8.png)\n\n\n如果加上 -h 选项，输出的结果会友好很多：\n\n![image](https://user-images.githubusercontent.com/28568478/117406278-71ae9780-af3f-11eb-8dc8-937a2f08fb00.png)\n\n\n<!--more-->\n\n有时我们需要持续的观察内存的状况，此时可以使用 -s 选项并指定间隔的秒数， -c选项指定展示次数：\n\n```\n$ free -h -c 100 -s 3\n```\n![image](https://user-images.githubusercontent.com/28568478/117406294-75421e80-af3f-11eb-9093-97ddd1dc614f.png)\n\n\n上面的命令每隔 3 秒输出一次内存的使用情况，直到你按下 ctrl + c。\n\n由于 free 命令本身比较简单，所以本文的重点会放在如何通过 free 命令了解系统当前的内存使用状况。\n\n# 输出简介\n\n下面先解释一下输出的内容：\n**Mem** 行(第二行)是内存的使用情况。\n**Swap** 行(第三行)是交换空间的使用情况。\n**total** 列显示系统总的可用物理内存和交换空间大小。\n**used** 列显示已经被使用的物理内存和交换空间。\n**free** 列显示还有多少物理内存和交换空间可用使用。\n**shared** 列显示被共享使用的物理内存大小。\n**buff/cache** 列显示被 buffer 和 cache 使用的物理内存大小。\n**available** 列显示还可以被应用程序使用的物理内存大小。\n\n我想只有在理解了一些基本概念之后，上面的输出才能帮助我们了解系统的内存状况。\n\n# buff/cache\n\n先来提一个问题： buffer 和 cache 应该是两种类型的内存，但是 free 命令为什么会把它们放在一起呢？要回答这个问题需要我们做些准备工作。让我们先来搞清楚 buffer 与 cache 的含义。\n\n**buffer** 在操作系统中指 buffer cache， 中文一般翻译为 \"缓冲区\"。要理解缓冲区，必须明确另外两个概念：\"扇区\" 和 \"块\"。扇区是设备的最小寻址单元，也叫 \"硬扇区\" 或 \"设备块\"。块是操作系统中文件系统的最小寻址单元，也叫 \"文件块\" 或 \"I/O 块\"。每个块包含一个或多个扇区，但大小不能超过一个页面，所以一个页可以容纳一个或多个内存中的块。当一个块被调入内存时，它要存储在一个缓冲区中。每个缓冲区与一个块对应，它相当于是磁盘块在内存中的表示(下图来自互联网)：\n\n![image](https://user-images.githubusercontent.com/28568478/117406319-7a9f6900-af3f-11eb-846f-4047421d5435.png)\n\n\n注意，buffer cache 只有块的概念而没有文件的概念，它只是把磁盘上的块直接搬到内存中而不关心块中究竟存放的是什么格式的文件。\n\n**cache** 在操作系统中指 page cache，中文一般翻译为 \"页高速缓存\"。页高速缓存是内核实现的磁盘缓存。它主要用来减少对磁盘的 I/O 操作。具体地讲，是通过把磁盘中的数据缓存到物理内存中，把对磁盘的访问变为对物理内存的访问。页高速缓存缓存的是内存页面。**缓存中的页来自对普通文件、块设备文件(这个指的就是 buffer cache 呀)和内存映射文件的读写**。\n页高速缓存对普通文件的缓存我们可以这样理解：当内核要读一个文件(比如 /etc/hosts)时，它会先检查这个文件的数据是不是已经在页高速缓存中了。如果在，就放弃访问磁盘，直接从内存中读取。这个行为称为缓存命中。如果数据不在缓存中，就是未命中缓存，此时内核就要调度块 I/O 操作从磁盘去读取数据。然后内核将读来的数据放入页高速缓存中。这种缓存的目标是文件系统可以识别的文件(比如 /etc/hosts)。\n页高速缓存对块设备文件的缓存就是我们在前面介绍的 buffer cahce。因为独立的磁盘块通过缓冲区也被存入了页高速缓存(缓冲区最终是由页高速缓存来承载的)。\n\n到这里我们应该搞清楚了：无论是缓冲区还是页高速缓存，它们的实现方式都是一样的。缓冲区只不过是一种概念上比较特殊的页高速缓存罢了。\n那么为什么 free 命令不直接称为 cache 而非要写成 buff/cache？ 这是因为缓冲区和页高速缓存的实现并非天生就是统一的。在 linux 内核 2.4 中才将它们统一。更早的内核中有两个独立的磁盘缓存：页高速缓存和缓冲区高速缓存。前者缓存页面，后者缓存缓冲区。当你知道了这些故事之后，输出中列的名称可能已经不再重要了。\n\n# free 与 available\n\n在 free 命令的输出中，有一个 free 列，同时还有一个 available 列。这二者到底有何区别？\nfree 是真正尚未被使用的物理内存数量。至于 available 就比较有意思了，它是从应用程序的角度看到的可用内存数量。Linux 内核为了提升磁盘操作的性能，会消耗一部分内存去缓存磁盘数据，就是我们介绍的 buffer 和 cache。所以对于内核来说，buffer 和 cache 都属于已经被使用的内存。当应用程序需要内存时，如果没有足够的 free 内存可以用，内核就会从 buffer 和 cache 中回收内存来满足应用程序的请求。所以从应用程序的角度来说，**available  = free + buffer + cache**。请注意，这只是一个很理想的计算方式，实际中的数据往往有较大的误差。\n\n# 交换空间(swap space)\n\nswap space 是磁盘上的一块区域，可以是一个分区，也可以是一个文件。所以具体的实现可以是 swap 分区也可以是 swap 文件。当系统物理内存吃紧时，Linux 会将内存中不常访问的数据保存到 swap 上，这样系统就有更多的物理内存为各个进程服务，而当系统需要访问 swap 上存储的内容时，再将 swap 上的数据加载到内存中，这就是常说的换出和换入。交换空间可以在一定程度上缓解内存不足的情况，但是它需要读写磁盘数据，所以性能不是很高。\n\n现在的机器一般都不太缺内存，如果系统默认还是使用了 swap 是不是会拖累系统的性能？理论上是的，但实际上可能性并不是很大。并且内核提供了一个叫做 swappiness 的参数，用于配置需要将内存中不常用的数据移到 swap 中去的紧迫程度。这个参数的取值范围是 0～100，0 告诉内核尽可能的不要将内存数据移到 swap 中，也即只有在迫不得已的情况下才这么做，而 100 告诉内核只要有可能，尽量的将内存中不常访问的数据移到 swap 中。在 ubuntu 系统中，swappiness 的默认值是 60。如果我们觉着内存充足，可以在 /etc/sysctl.conf 文件中设置 swappiness：\n\n```\nvm.swappiness=10\n```\n如果系统的内存不足，则需要根据物理内存的大小来设置交换空间的大小。具体的策略网上有很丰富的资料，这里笔者不再赘述。\n\n# /proc/meminfo 文件\n\n其实 free 命令中的信息都来自于 /proc/meminfo 文件。/proc/meminfo 文件包含了更多更原始的信息，只是看起来不太直观：\n```\n$ cat /proc/meminfo\n```\n![image](https://user-images.githubusercontent.com/28568478/117406335-7ffcb380-af3f-11eb-84d7-8e6fadde9dcb.png)\n\n\n有兴趣的同学可以直接查看这个文件。\n\n# 总结\n\nfree 命令是一个既简单又复杂的命令。简单是因为这个命令的参数少，输出结果清晰。说它复杂则是因为它背后是比较晦涩的操作系统中的概念，如果不清楚这些概念，即便看了 free 命令的输出也 get 不到多少有价值的信息。\n\n#### [原文：linux下free命令详解](https://www.cnblogs.com/ultranms/p/9254160.html)\n","tags":["free"],"categories":["Linux"]},{"title":"批量关闭linux进程","url":"/2021/05/07/Linux web/批量关闭linux进程/","content":"# 批量关闭linux进程\n\n你是否经常遇到需要批量杀死很多进程的情况？而你是否还在一个一个的`kill`。\n\n接下来我教你一个小秘诀吧。\n\n1、首先我们查看当前的进程列表。\n\n我们以查看`nginx`进程为例，通过`ps -ef`显示当前机器运行的所有进程，再通过`grep nginx`过滤出包含`nginx`字符串的进程。完成命令为`ps -ef|grep nginx`。\n![image](https://user-images.githubusercontent.com/28568478/117406501-ba665080-af3f-11eb-8030-bf89ef6e6b26.png)\n\n<!--more-->\n2、获取进程ID\n采用`awk`工具提取进程ID。`awk`是一种很棒的语言，适合文本处理和报表生成。在这里我们通过`awk`处理第一步中得到的进程列表，提取进程ID。完成命令为`ps -ef|grep nginx|awk '{print $2}'`\n![image](https://user-images.githubusercontent.com/28568478/117406514-bf2b0480-af3f-11eb-86ae-0c8dfd059eaf.png)\n\n\n3、批量kill\n`xargs` 是一条 Unix 和类 Unix 操作系统的常用命令；它的作用是将参数列表转换成小块分段传递给其他命令，以避免参数列表过长的问题。接下来将使用`xargs`把第二步中得到的进程ID列表传递给`kill`命令。完成命令为`ps -ef|grep nginx|awk '{print $2}'|xargs kill -9`。\n![image](https://user-images.githubusercontent.com/28568478/117406622-e4b80e00-af3f-11eb-980b-64844240af0b.png)\n\n\nOK。大功告成，现在可以批量\b`kill`进程了。\n","tags":["进程"],"categories":["Linux web"]},{"title":"nohup详解 Python不挂断运行后台程序","url":"/2021/05/07/Linux web/nohup 详解 Python不挂断运行后台程序/","content":"# nohup 详解\n\n## nohup\n\nnohup 命令运行由 Command参数和任何相关的 Arg参数指定的命令，忽略所有挂断（SIGHUP）信号。在注销后使用 nohup 命令运行后台中的程序。要运行后台中的 nohup 命令，添加 & （ 表示“and”的符号）到命令的尾部。\n\nnohup 是 no hang up 的缩写，就是不挂断的意思。\n\nnohup命令：如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。\n\n在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。\n\n<!--more-->\n\n## 案例\n\n1. nohup command > myout.file 2>&1 &   \n\n在上面的例子中，0 – stdin (standard input)，1 – stdout (standard output)，2 – stderr (standard error) ；\n\n2>&1是将标准错误（2）重定向到标准输出（&1），标准输出（&1）再被重定向输入到myout.file文件中。\n\n2. 0 22 * * * /usr/bin/python /home/pu/download_pdf/download_dfcf_pdf_to_oss.py > /home/pu/download_pdf/download_dfcf_pdf_to_oss.log 2>&1\n\n这是放在crontab中的定时任务，晚上22点时候怕这个任务，启动这个python的脚本，并把日志写在download_dfcf_pdf_to_oss.log文件中\n\n\n## nohup和&的区别\n\n& ： 指在后台运行\n\nnohup ： 不挂断的运行，注意并没有后台运行的功能，，就是指，用nohup运行命令可以使命令永久的执行下去，和用户终端没有关系，例如我们断开SSH连接都不会影响他的运行，注意了nohup没有后台运行的意思；&才是后台运行\n\n* * *\n\n&是指在后台运行，但当用户推出(挂起)的时候，命令自动也跟着退出\n\n那么，我们可以巧妙的吧他们结合起来用就是\nnohup COMMAND &\n这样就能使命令永久的在后台执行\n\n例如：\n\n1\\. sh test.sh &  \n将sh test.sh任务放到后台 ，即使关闭xshell退出当前session依然继续运行，但**标准输出和标准错误信息会丢失（缺少的日志的输出）**\n\n将sh test.sh任务放到后台 ，关闭xshell，对应的任务也跟着停止。\n2\\. nohup sh test.sh  \n将sh test.sh任务放到后台，关闭标准输入，**终端不再能够接收任何输入（标准输入）**，重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。\n3\\. nohup sh test.sh  & \n将sh test.sh任务放到后台，但是依然可以使用标准输入，**终端能够接收任何输入**，重定向标准输出和标准错误到当前目录下的nohup.out文件，即使关闭xshell退出当前session依然继续运行。\n\n\n## 参考链接\n\nhttps://blog.csdn.net/u011095110/article/details/78666833\n\nhttps://baike.baidu.com/item/nohup/5683841\n\n[原文链接](https://www.cnblogs.com/jinxiao-pu/p/9131057.html)\n","tags":["nohup"],"categories":["Linux web"]},{"title":"linux服务器搭建 shadowsocks","url":"/2021/05/07/proxy/linux服务器搭建 shadowsocks/","content":"\n```\nsudo apt-get update    # 更新软件源列表\nsudo apt-get -y install python-gevent python-pip  # 安装所需服务\nsudo apt-get -y install python-m2crypto\nsudo pip install shadowsocks   # 安装影梭\n```\n\nshadowsocks安装完毕后，可以查看使用ssserver命令进行查看。如下：\nssserver -h\n\n在 /etc/shadowsocks/ 下写入以 .json结尾的配置文件 如下:\n```\n{\n    \"server\":\"0.0.0.0\",\n    \"server_port\":443,  #普通用户目录下 采用大于1024的端口 root目录下可以使用443\n    \"local_address\":\"127.0.0.1\",\n    \"local_port\":1080,\n    \"password\":\"你的密码\",\n    \"timeout\":300,\n    \"method\":\"aes-256-cfb\",\n    \"fast_open\":false,\n    \"workers\": 1\n}\n```\n\n常用命令\n\n```\n# 启动\nssserver -c /etc/shadowsocks/ss.json -d start\n# 停止\nssserver -c /etc/shadowsocks/ss.json -d stop\n# 重启\nssserver -c /etc/shadowsocks/ss.json -d restart\n-d  后台启动  及日志记录\nssserver -c /etc/shadowsocks/config.json -d start  --log-file ./ss.log --pid-file ./ss.pid\n\nnetstat -tunlp 查看服务是否启动\n\nvi /etc/rc.local\n按i键进入编辑模式，然后在最后一行加入\nssserver -c /etc/shadowsocks/config.json -d start  --log-file /etc/shadowsocks/ss.log --pid-file /etc/shadowsocks/ss.pid\n```\n","tags":["代理"],"categories":["proxy"]},{"title":"Vue.js - Day5","url":"/2021/05/04/vue/vue2.0基础课程/day5/","content":"\n# Vue.js - Day5 - Webpack\n\n## 在网页中会引用哪些常见的静态资源？\n+ JS\n - .js  .jsx  .coffee  .ts（TypeScript  类 C# 语言）\n+ CSS\n - .css  .less   .sass  .scss\n+ Images\n - .jpg   .png   .gif   .bmp   .svg\n+ 字体文件（Fonts）\n - .svg   .ttf   .eot   .woff   .woff2\n+ 模板文件\n - .ejs   .jade  .vue【这是在webpack中定义组件的方式，推荐这么用】\n\n\n## 网页中引入的静态资源多了以后有什么问题？？？\n1. 网页加载速度慢， 因为 我们要发起很多的二次请求；\n2. 要处理错综复杂的依赖关系\n\n\n## 如何解决上述两个问题\n1. 合并、压缩、精灵图、图片的Base64编码\n2. 可以使用之前学过的requireJS、也可以使用webpack可以解决各个包之间的复杂依赖关系；\n\n## 什么是webpack?\nwebpack 是前端的一个项目构建工具，它是基于 Node.js 开发出来的一个前端工具；\n\n\n## 如何完美实现上述的2种解决方案\n1. 使用Gulp， 是基于 task 任务的；\n2. 使用Webpack， 是基于整个项目进行构建的；\n+ 借助于webpack这个前端自动化构建工具，可以完美实现资源的合并、打包、压缩、混淆等诸多功能。\n+ 根据官网的图片介绍webpack打包的过程\n+ [webpack官网](http://webpack.github.io/)\n+ [webpack中文文档](https://webpack.docschina.org/)\n\n## webpack安装的两种方式\n1. 运行`npm i webpack -g`全局安装webpack，这样就能在全局使用webpack的命令\n2. 在项目根目录中运行`npm i webpack --save-dev`安装到项目依赖中\n\n## 初步使用webpack打包构建列表隔行变色案例\n1. 运行`npm init`初始化项目，使用npm管理项目中的依赖包 (npm init -y 一键初始化) 自动生成package.json文件\n2. 创建项目基本的目录结构 src:存放源代码  dist: 项目发布后的文件存放目录\n3. 使用`cnpm i jquery --save`安装jquery类库 （npm i jquery -s） 安装包后自动生成node_modules文件夹，并把包安装到这个文件夹\n4. 创建`main.js`并书写各行变色的代码逻辑：\n```\n\t// 导入jquery类库\n    import $ from 'jquery'\n\n    // 设置偶数行背景色，索引从0开始，0是偶数\n    $('#list li:even').css('backgroundColor','lightblue');\n    // 设置奇数行背景色\n    $('#list li:odd').css('backgroundColor','pink');\n```\n5. 直接在页面上引用`main.js`会报错，因为浏览器不认识`import`这种高级的JS语法，需要使用webpack进行处理，webpack默认会把这种高级的语法转换为低级的浏览器能识别的语法；\n6. 运行`webpack 入口文件路径 输出文件路径`对`main.js`进行处理：\n```\nwebpack src/js/main.js dist/bundle.js\n```\n\n## 使用webpack的配置文件简化打包时候的命令\n1. 在项目根目录中创建`webpack.config.js`\n2. 由于运行webpack命令的时候，webpack需要指定入口文件和输出文件的路径，所以，我们需要在`webpack.config.js`中配置这两个路径：\n```\n    // 导入处理路径的模块\n    var path = require('path');\n\n    // 导出一个配置对象，将来webpack在启动的时候，会默认来查找webpack.config.js，并读取这个文件中导出的配置对象，来进行打包处理\n    module.exports = {\n        entry: path.resolve(__dirname, 'src/js/main.js'), // 项目入口文件\n        output: { // 配置输出选项\n            path: path.resolve(__dirname, 'dist'), // 配置输出的路径\n            filename: 'bundle.js' // 配置输出的文件名\n        }\n    }\n```\n\n## 实现webpack的实时打包构建\n1. 由于每次重新修改代码之后，都需要手动运行webpack打包的命令，比较麻烦，所以使用`webpack-dev-server`来实现代码实时打包编译，当修改代码之后，会自动进行打包构建。\n2. 运行`cnpm i webpack-dev-server --save-dev`安装到开发依赖\n3. 安装完成之后，在命令行直接运行`webpack-dev-server`来进行打包，发现报错，此时需要借助于`package.json`文件中的指令，来进行运行`webpack-dev-server`命令，在`scripts`节点下新增`\"dev\": \"webpack-dev-server\"`指令，发现可以进行实时打包，但是dist目录下并没有生成`bundle.js`文件，这是因为`webpack-dev-server`将打包好的文件放在了内存中\n + 把`bundle.js`放在内存中的好处是：由于需要实时打包编译，所以放在内存中速度会非常快\n + 这个时候访问webpack-dev-server启动的`http://localhost:8080/`网站，发现是一个文件夹的面板，需要点击到src目录下，才能打开我们的index首页，此时引用不到bundle.js文件，需要修改index.html中script的src属性为:`<script src=\"../bundle.js\"></script>`\n + 为了能在访问`http://localhost:8080/`的时候直接访问到index首页，可以使用`--contentBase src`指令来修改dev指令，指定启动的根目录：\n ```\n \"dev\": \"webpack-dev-server --contentBase src\"\n ```\n 同时修改index页面中script的src属性为`<script src=\"bundle.js\"></script>`\n\n## 使用`html-webpack-plugin`插件配置启动页面\n由于使用`--contentBase`指令的过程比较繁琐，需要指定启动的目录，同时还需要修改index.html中script标签的src属性，所以推荐大家使用`html-webpack-plugin`插件配置启动页面.\n1. 运行`cnpm i html-webpack-plugin --save-dev`安装到开发依赖\n2. 修改`webpack.config.js`配置文件如下：\n```\n    // 导入处理路径的模块\n    var path = require('path');\n    // 导入自动生成HTMl文件的插件\n    var htmlWebpackPlugin = require('html-webpack-plugin');\n\n    module.exports = {\n        entry: path.resolve(__dirname, 'src/js/main.js'), // 项目入口文件\n        output: { // 配置输出选项\n            path: path.resolve(__dirname, 'dist'), // 配置输出的路径\n            filename: 'bundle.js' // 配置输出的文件名\n        },\n        plugins:[ // 添加plugins节点配置插件\n            new htmlWebpackPlugin({\n                template:path.resolve(__dirname, 'src/index.html'),//模板路径\n                filename:'index.html'//自动生成的HTML文件的名称\n            })\n        ]\n    }\n```\n3. 修改`package.json`中`script`节点中的dev指令如下：\n```\n\"dev\": \"webpack-dev-server\"\n```\n4. 将index.html中script标签注释掉，因为`html-webpack-plugin`插件会自动把bundle.js注入到index.html页面中！\n\n## 实现自动打开浏览器、热更新和配置浏览器的默认端口号\n**注意：热更新在JS中表现的不明显，可以从一会儿要讲到的CSS身上进行介绍说明！**\n### 方式1：\n+ 修改`package.json`的script节点如下，其中`--open`表示自动打开浏览器，`--port 4321`表示打开的端口号为4321，`--hot`表示启用浏览器热更新：\n```\n\"dev\": \"webpack-dev-server --hot --port 4321 --open\"\n```\n\n### 方式2：\n1. 修改`webpack.config.js`文件，新增`devServer`节点如下：\n```\ndevServer:{\n        hot:true,\n        open:true,\n        port:4321\n    }\n```\n2. 在头部引入`webpack`模块：\n```\nvar webpack = require('webpack');\n```\n3. 在`plugins`节点下新增：\n```\nnew webpack.HotModuleReplacementPlugin()\n```\n\n## 使用webpack打包css文件\n1. 运行`cnpm i style-loader css-loader --save-dev`\n2. 修改`webpack.config.js`这个配置文件：\n```\nmodule: { // 用来配置第三方loader模块的\n        rules: [ // 文件的匹配规则\n            { test: /\\.css$/, use: ['style-loader', 'css-loader'] }//处理css文件的规则\n        ]\n    }\n```\n3. 注意：`use`表示使用哪些模块来处理`test`所匹配到的文件；`use`中相关loader模块的调用顺序是从后向前调用的；\n\n## 使用webpack打包less文件\n1. 运行`cnpm i less-loader less -D`\n2. 修改`webpack.config.js`这个配置文件：\n```\n{ test: /\\.less$/, use: ['style-loader', 'css-loader', 'less-loader'] },\n```\n\n## 使用webpack打包sass文件\n1. 运行`cnpm i sass-loader node-sass --save-dev`\n2. 在`webpack.config.js`中添加处理sass文件的loader模块：\n```\n{ test: /\\.scss$/, use: ['style-loader', 'css-loader', 'sass-loader'] }\n```\n\n## 使用webpack处理css中的路径\n1. 运行`cnpm i url-loader file-loader --save-dev`\n2. 在`webpack.config.js`中添加处理url路径的loader模块：\n```\n{ test: /\\.(png|jpg|gif)$/, use: 'url-loader' }\n```\n3. 可以通过`limit`指定进行base64编码的图片大小；只有小于指定字节（byte）的图片才会进行base64编码：\n```\n{ test: /\\.(png|jpg|gif)$/, use: 'url-loader?limit=43960' },\n```\n\n## 使用babel处理高级JS语法\n1. 运行`cnpm i babel-core babel-loader babel-plugin-transform-runtime --save-dev`安装babel的相关loader包\n2. 运行`cnpm i babel-preset-es2015 babel-preset-stage-0 --save-dev`安装babel转换的语法\n3. 在`webpack.config.js`中添加相关loader模块，其中需要注意的是，一定要把`node_modules`文件夹添加到排除项：\n```\n{ test: /\\.js$/, use: 'babel-loader', exclude: /node_modules/ }\n```\n4. 在项目根目录中添加`.babelrc`文件，并修改这个配置文件如下：\n```\n{\n    \"presets\":[\"es2015\", \"stage-0\"],\n    \"plugins\":[\"transform-runtime\"]\n}\n```\n5. **注意：语法插件`babel-preset-es2015`可以更新为`babel-preset-env`，它包含了所有的ES相关的语法；**\n\n## 相关文章\n[babel-preset-env：你需要的唯一Babel插件](https://segmentfault.com/p/1210000008466178)\n[Runtime transform 运行时编译es6](https://segmentfault.com/a/1190000009065987)\n","tags":["vue2.0基础课程","Webpack"],"categories":["vue2.0基础课程"]},{"title":"安装Python","url":"/2021/05/04/python/安装Python/","content":"\n### [How to Install Python](https://linuxize.com/post/how-to-install-python-3-7-on-ubuntu-18-04/)\n```\napt-get install zlib1g-dev libbz2-dev libssl-dev libncurses5-dev  libsqlite3-dev libreadline-dev tk-dev libgdbm-dev libdb-dev libpcap-dev xz-utils libexpat1-dev   liblzma-dev libffi-dev  libc6-dev\n\n1. 下载源码包\n     wget   https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz\n2. mkdir -p /usr/local/python3\n3. mv Python-3.7.3.tgz /usr/local/python3\n4. tar -zxf Python-3.7.3.tgz\n5. cd python3.7.3\n6. ./configure --prefix=/usr/local/python3 --with-ssl  --enable-optimizations\n7. make\n8. make install\n```\n\n```\nln -s /usr/local/python3/bin/python3 /usr/bin/python3 [#确认是否是需要的版本]\nln -s /usr/local/python3/bin/pip3.7 /usr/bin/pip3\n\n安装pip3 apt-get install python3-pip\n```\n\n### mac 安装python\n```\nbrew install python\nbrew install python@3.9 # 安装指定版本\n```\n","categories":["python"]},{"title":"python 列表加法\"+\"和\"extend\"的区别","url":"/2021/05/04/python/python 列表加法\"+\"和\"extend\"的区别/","content":"\n### 相同点:　　\n -  \"+\"和\"extend\"都能将两个列表成员拼接到到一起\n\n\n\n### 不同点:　　\n -  \\+ : 生成的是一个新列表(id改变)\n\n - extend : 是将一个列表的成员一个个取出添加到原列表中 , 改变的是原列表的值 , id不变\n\n<!--more-->\n\n![image](https://user-images.githubusercontent.com/28568478/117003180-21ed8780-ad17-11eb-97fc-fedcf041db5d.png)\n\n","tags":["list"],"categories":["python"]},{"title":"浅析深拷贝浅拷贝","url":"/2021/05/04/python/浅析深拷贝浅拷贝/","content":"\n本文主要介绍python中的深拷贝和浅拷贝究竟从底层ID来看是怎么回事\n\n<!--more-->\n![](https://user-images.githubusercontent.com/28568478/117002717-80663600-ad16-11eb-9151-ccd5e05a538b.png)\n\n\n![image](https://user-images.githubusercontent.com/28568478/117002786-94119c80-ad16-11eb-9c19-dd44d0494db8.png)\n![image](https://user-images.githubusercontent.com/28568478/117002798-996ee700-ad16-11eb-9a72-4cbcdd780596.png)\n![image](https://user-images.githubusercontent.com/28568478/117002811-9e339b00-ad16-11eb-9c55-2555f7bfc671.png)\n![image](https://user-images.githubusercontent.com/28568478/117002821-a2f84f00-ad16-11eb-8d56-7d4aa3dde512.png)\n![image](https://user-images.githubusercontent.com/28568478/117002835-a7246c80-ad16-11eb-8ce6-85764cfff7fe.png)\n![image](https://user-images.githubusercontent.com/28568478/117002847-abe92080-ad16-11eb-95f5-a8381a3042f4.png)\n\n","tags":["python 深拷贝浅拷贝"],"categories":["python"]},{"title":"Python中字典的键为什么要是不可变类型","url":"/2021/05/04/python/Python中字典的键为什么要是不可变类型/","content":"\n\n很多python初学者经常会有这样的疑问，为什么Python有tuple（元组）和list（列表）两种类型？为什么tuple可以作为字典的key，list不可以？要理解这个问题，首先要明白python的字典工作原理。\n\n<!--more-->\n\nPython的字典是如何工作的\n\n在Python中，字典也就是一个个的“映射”，将key映射到value：\n\n对一个特定的key可以得到一个value value = d[key]\n\n为了实现这个功能，Python必须能够做到，给出一个key，找到哪一个value与这个key对应。先来考虑一种比较简单的实现，将所有的key-value键值对存放到一个list中，每当需要的时候，就去遍历这个list，用key去和键值对的key匹配，如果相等，就拿到value。但是这种实现在数据量很大的时候就变得很低效。它的算法复杂度是O(n)，n是存放键值对的数量。\n\n为此，Python使用了hash（哈希）的方法来实现，要求每一个存放到字典中的对象都要实现hash函数，这个函数可以产生一个int值，叫做hash value（哈希值），通过这个int值，就可以快速确定对象在字典中的位置。\n\n这个查询的大致过程如下：\n\ndef lookup(d, key): '''字典的查询过程概括为下面3步: 1. 通过hash函数将key计算为哈希值. 2. 通过hash值确定一个位置，这个位置是一个存放着 可能存在冲突的元素的数组（很多地方叫做“桶”，bucket）， 每一个元素都是一个键值对，理想情况下，这个数组里只有1个元素. 3. 遍历这个数组，找到目标key，返回对应的value. ''' h = hash(key)# step 1 cl = d.data[h]# step 2 for pairin cl:# step 3 if key == pair[0]: return pair[1] else: raise KeyError, \"Key %s not found.\" % key\n\n要使这个查找过程正常工作，hash函数必须满足条件： 如果两个key产生了不同的hash value，那么这两个key对象是不想等的。 即\n\nfor alli1, i2, if hash(i1) != hash(i2), then i1 != i2\n\n否则的话，hash value不同，对象却相同，那么相同的对象产生不同的hash value，查找的时候就会进错桶（step 2），在错误的桶里永远也找不到你要找的value。\n\n另外，要让字典保持高查找效率，还要保证： 当两个key产生相同的hash value，那么他们是相等的。\n\nfor alli1, i2, if hash(i1) == hash(i2), then i1 == i2\n\n这样做的目的是，尽量满足每个hash桶只有一个元素。为什么要这样呢？ 考虑下面这个hash函数。\n\ndef hash(obj): return 1\n\n这个hash函数是满足上面我们谈的第一个条件的：如果两个key的hash value不同，那么两个key对象不相同。因为所有的对象产生的hash value都是1，所以不存在能产生不同hash value的key，也就不存在不满足的情况。但是这样做的坏处是，因为所有的hash value都相同，所以就把所有的对象分到了同一个地方。查找的时候，进行到第三步，遍历的效率就变成了O(n).\n\nHash函数应该保证所有的元素平均的分配到每一个桶中，理想的情况是，每一个位置只有一个元素。\n\n字典Key要满足的要求\n\n经过上面的讨论，我们应该明白Python为什么对字典的key有这样的要求了：\n\n要作为字典的key，对象必须要支持hash函数（即__hash__），相等比较(__eq__或__cmp__），并且满足上面我们讨论过的条件。\n\nList为什么不能作为key\n\n至于这个问题，最直接的答案就是：list没有支持__hash__方法，那么为什么呢？\n\n对于list的hash函数，我们可能有下面两种实现的方式：\n\n第一种，基于id。这满足条件，“如果hash值不同，那么他们的id当然不同”。但考虑到list一般是作为容器，基于id来hash可能会导致下面两种情况：\n\n用相同的list作为key去字典中找某个元素可能会得到不同的结果，因为是基于id hash的，所以即使他们的内容相同，字典依然将他们作为不同的元素对待。 创建一个一模一样的list用字典查找永远会得到一个KeyError。\n\n第二种，基于内容。tuple就是这样做的，但是要注意一点，list是可以修改的。当list修改之后，你就永远别想再从字典中拿回来了。见下面的代码。\n```\n>>> l = [1, 2]\n>>> d = {}\n>>> d[l] = 42\n>>> l.append(3)\n>>> d[l]\n# 原来的hash值是基于[1, 2]hash的，\n# 现在是基于[1, 2, 3]，所以找不到 Traceback (mostrecentcalllast): File \"\", line 1, in ? KeyError: [1, 2, 3]\n>>> d[[1, 2]] # 基于hash [1, 2]\n# 但是遍历的时候找不到key相等的键值对\n#（因为字典里的key变成了[1, 2, 3] Traceback (mostrecentcalllast): File \"\", line 1, in ? KeyError: [1, 2]\n```\n鉴于两种实现的方式都存在一定的副作用，所以Python规定：\n\n内置的list不能作为字典的key.\n\n但tuple是不可变，所以tuple可以作为字典的key。\n\n自定义的类型作为字典的Key\n\n用户自定义的类型就可以作为key了，默认的 hash(object) 是 id(object) , 默认的 cmp(object1,object2) 是 cmp(id(object1),id(object2))， 同样是可以修改的对象，为什么这里就没有上面说的问题呢？\n\n一般来说，在映射中比较常见的需求是用一个object替换掉原来的，所以id比内容更重要，就可以基于id来hash 如果内容重要的话，自定义的类型可以通过覆盖__hash__函数和__cmp__函数或__eq__函数来实现\n\n值得注意的是：将对象和一个value关联起来，更好的做法是将value设置为对象的一个属性。\n","tags":["不可变类型","字典"],"categories":["python"]},{"title":"python 生成器和迭代器","url":"/2021/05/04/python/python 生成器和迭代器/","content":"\n本节主要记录一下列表生成式，生成器和迭代器的知识点\n\n### **列表生成器**\n\n**首先举个例子**\n\n现在有个需求，看列表 [0，1，2，3，4，5，6，7，8，9]，要求你把列表里面的每个值加1，你怎么实现呢？\n\n方法一（简单）：\n```\ninfo = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nb = []\n# for index,i in enumerate(info):\n#     print(i+1)\n#     b.append(i+1)\n# print(b)\nfor index,i in enumerate(info):\n    info[index] +=1\nprint(info)\n```\n<!--more-->\n\n方法二（一般）：\n\n```\ninfo = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\na = map(lambda x:x+1,info)\nprint(a)\nfor i in a:\n    print(i)\n```\n\n方法三（高级）：\n```\ninfo = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\na = [i+1 for i in range(10)]\nprint(a)\n```\n### 　　生成器\n\n#### 什么是生成器？\n\n　　通过列表生成式，我们可以直接创建一个列表，但是，受到内存限制，列表容量肯定是有限的，而且创建一个包含100万个元素的列表，不仅占用很大的存储空间，如果我们仅仅需要访问前面几个元素，那后面绝大多数元素占用的空间都白白浪费了。\n\n　　所以，如果列表元素可以按照某种算法推算出来，那我们是否可以在循环的过程中不断推算出后续的元素呢？这样就不必创建完整的list，从而节省大量的空间，**在Python中，这种一边循环一边计算的机制，称为生成器：generator**\n\n　　生成器是一个特殊的程序，可以被用作控制循环的迭代行为，**python中生成器是迭代器的一种**，使用yield返回值函数，每次调用yield会暂停，而可以使用next()函数和send()函数恢复生成器。\n\n　　生成器类似于返回值为数组的一个函数，这个函数可以接受参数，可以被调用，但是，不同于一般的函数会一次性返回包括了所有数值的数组，生成器一次只能产生一个值，这样消耗的内存数量将大大减小，而且允许调用函数可以很快的处理前几个返回值，因此生成器看起来像是一个函数，但是表现得却像是迭代器\n\n#### python中的生成器\n\n　　要创建一个generator，有很多种方法，第一种方法很简单，**只有把一个列表生成式的[]中括号改为（）小括号，就创建一个generator**\n\n举例如下：\n```\n#列表生成式\nlis = [x*x for x in range(10)]\nprint(lis)\n#生成器\ngenerator_ex = (x*x for x in range(10))\nprint(generator_ex)\n\n结果：\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n<generator object <genexpr> at 0x000002A4CBF9EBA0>\n```\n　　那么创建list和generator_ex，的区别是什么呢？从表面看就是[  ]和（）,但是结果却不一样，一个打印出来是列表（因为是列表生成式），而第二个打印出来却是<generator object <genexpr> at 0x000002A4CBF9EBA0>，那么如何打印出来generator_ex的每一个元素呢？\n\n　　如果要一个个打印出来，可以通过next（）函数获得generator的下一个返回值：\n```\n#生成器\ngenerator_ex = (x*x for x in range(10))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\nprint(next(generator_ex))\n结果：\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\nTraceback (most recent call last):\n\n  File \"列表生成式.py\", line 42, in <module>\n\n    print(next(generator_ex))\n\nStopIteration\n```\n　　大家可以看到，generator保存的是算法，每次调用next(generaotr_ex)就计算出他的下一个元素的值，直到计算出最后一个元素，没有更多的元素时，抛出StopIteration的错误，而且上面这样不断调用是一个不好的习惯，正确的方法是使用for循环，因为generator也是可迭代对象：\n```\n#生成器\ngenerator_ex = (x*x for x in range(10))\nfor i in generator_ex:\n    print(i)\n\n结果：\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n```\n　　所以我们创建一个generator后，基本上永远不会调用next()，而是通过for循环来迭代，并且不需要关心StopIteration的错误，generator非常强大，如果推算的算法比较复杂，用类似列表生成式的for循环无法实现的时候，还可以用函数来实现。\n\n比如著名的斐波那契数列，除第一个和第二个数外，任何一个数都可以由前两个相加得到：\n\n1，1，2，3，5，8，12，21，34.....\n\n斐波那契数列用列表生成式写不出来，但是，用函数把它打印出来却很容易：\n\n```\n#fibonacci数列\ndef fib(max):\n    n,a,b =0,0,1\n    while n < max:\n        a,b =b,a+b\n        n = n+1\n        print(a)\n    return 'done'\n\na = fib(10)\nprint(fib(10))\n```\n\n　　a,b = b ,a+b  其实相当于 t =a+b ,a =b ,b =t  ，所以不必写显示写出临时变量t，就可以输出斐波那契数列的前N个数字。上面输出的结果如下：\n\n```\n1\n1\n2\n3\n5\n8\n13\n21\n34\n55\n1\n1\n2\n3\n5\n8\n13\n21\n34\n55\ndone\n```\n　　仔细观察，可以看出，`fib`函数实际上是定义了斐波拉契数列的推算规则，可以从第一个元素开始，推算出后续任意的元素，这种逻辑其实非常类似generator。\n\n　　也就是说上面的函数也可以用generator来实现，上面我们发现，print(b)每次函数运行都要打印，占内存，所以为了不占内存，我们也可以使用生成器，这里叫yield。如下：\n```\ndef fib(max):\n    n,a,b =0,0,1\n    while n < max:\n        yield b\n        a,b =b,a+b\n        n = n+1\n    return 'done'\n\na = fib(10)\nprint(fib(10))\n```\n　　但是返回的不再是一个值，而是一个生成器，和上面的例子一样，大家可以看一下结果：\n\n```\n<generator object fib at 0x000001C03AC34FC0>\n\n```\n　　那么这样就不占内存了，这里说一下generator和函数的执行流程，函数是顺序执行的，遇到return语句或者最后一行函数语句就返回。而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次被next（）调用时候从上次的返回yield语句处急需执行，也就是用多少，取多少，不占内存。\n```\ndef fib(max):\n    n,a,b =0,0,1\n    while n < max:\n        yield b\n        a,b =b,a+b\n        n = n+1\n    return 'done'\n\na = fib(10)\nprint(fib(10))\nprint(a.__next__())\nprint(a.__next__())\nprint(a.__next__())\nprint(\"可以顺便干其他事情\")\nprint(a.__next__())\nprint(a.__next__())\n\n结果：\n<generator object fib at 0x0000023A21A34FC0>\n1\n1\n2\n可以顺便干其他事情\n3\n5\n```\n　　在上面fib的例子，我们在循环过程中不断调用`yield`，就会不断中断。当然要给循环设置一个条件来退出循环，不然就会产生一个无限数列出来。同样的，把函数改成generator后，我们基本上从来不会用`next()`来获取下一个返回值，而是直接使用`for`循环来迭代：\n```\ndef fib(max):\n    n,a,b =0,0,1\n    while n < max:\n        yield b\n        a,b =b,a+b\n        n = n+1\n    return 'done'\nfor i in fib(6):\n    print(i)\n\n结果：\n1\n1\n2\n3\n5\n8\n```\n　　但是用for循环调用generator时，发现拿不到generator的return语句的返回值。如果拿不到返回值，那么就会报错，所以为了不让报错，就要进行异常处理，拿到返回值，如果想要拿到返回值，必须捕获StopIteration错误，返回值包含在StopIteration的value中：\n```\ndef fib(max):\n    n,a,b =0,0,1\n    while n < max:\n        yield b\n        a,b =b,a+b\n        n = n+1\n    return 'done'\ng = fib(6)\nwhile True:\n    try:\n        x = next(g)\n        print('generator: ',x)\n    except StopIteration as e:\n        print(\"生成器返回值：\",e.value)\n        break\n\n\n结果：\ngenerator:  1\ngenerator:  1\ngenerator:  2\ngenerator:  3\ngenerator:  5\ngenerator:  8\n生成器返回值： done\n```\n**还可以通过yield实现在单线程的情况下实现并发运算的效果**\n\n```\nimport time\ndef consumer(name):\n    print(\"%s 准备学习啦!\" %name)\n    while True:\n       lesson = yield\n\n       print(\"开始[%s]了,[%s]老师来讲课了!\" %(lesson,name))\n\n\ndef producer(name):\n    c = consumer('A')\n    c2 = consumer('B')\n    c.__next__()\n    c2.__next__()\n    print(\"同学们开始上课 了!\")\n    for i in range(10):\n        time.sleep(1)\n        print(\"到了两个同学!\")\n        c.send(i)\n        c2.send(i)\n\n结果：\nA 准备学习啦!\nB 准备学习啦!\n同学们开始上课 了!\n到了两个同学!\n开始[0]了,[A]老师来讲课了!\n开始[0]了,[B]老师来讲课了!\n到了两个同学!\n开始[1]了,[A]老师来讲课了!\n开始[1]了,[B]老师来讲课了!\n到了两个同学!\n开始[2]了,[A]老师来讲课了!\n开始[2]了,[B]老师来讲课了!\n到了两个同学!\n开始[3]了,[A]老师来讲课了!\n开始[3]了,[B]老师来讲课了!\n到了两个同学!\n开始[4]了,[A]老师来讲课了!\n开始[4]了,[B]老师来讲课了!\n到了两个同学!\n开始[5]了,[A]老师来讲课了!\n开始[5]了,[B]老师来讲课了!\n到了两个同学!\n开始[6]了,[A]老师来讲课了!\n开始[6]了,[B]老师来讲课了!\n到了两个同学!\n```\n　　由上面的例子我么可以发现，python提供了两种基本的方式\n\n**生成器函数：也是用def定义的，利用关键字yield一次性返回一个结果，阻塞，重新开始**\n\n**生成器表达式：返回一个对象，这个对象只有在需要的时候才产生结果**\n\n#### ——生成器函数\n\n为什么叫生成器函数？因为它随着时间的推移生成了一个数值队列。一般的函数在执行完毕之后会返回一个值然后退出，但是生成器函数会自动挂起，然后重新拾起急需执行，他会利用yield关键字关起函数，给调用者返回一个值，同时保留了当前的足够多的状态，可以使函数继续执行，生成器和迭代协议是密切相关的，**迭代器都有一个__next__()__成员方法，**这个方法要么返回迭代的下一项，要买引起异常结束迭代。\n\n```\n# 函数有了yield之后，函数名+（）就变成了生成器\n# return在生成器中代表生成器的中止，直接报错\n# next的作用是唤醒并继续执行\n# send的作用是唤醒并继续执行，发送一个信息到生成器内部\n'''生成器'''\n\ndef create_counter(n):\n    print(\"create_counter\")\n    while True:\n        yield n\n        print(\"increment n\")\n        n +=1\n\ngen = create_counter(2)\nprint(gen)\nprint(next(gen))\nprint(next(gen))\n\n结果：\n<generator object create_counter at 0x0000023A1694A938>\ncreate_counter\n2\nincrement n\n3\nProcess finished with exit code 0\n```\n\n#### ——生成器表达式\n生成器表达式来源于迭代和列表解析的组合，生成器和列表解析类似，但是它使用尖括号而不是方括号\n```\n>>> # 列表解析生成列表\n>>> [ x ** 3 for x in range(5)]\n[0, 1, 8, 27, 64]\n>>>\n>>> # 生成器表达式\n>>> (x ** 3 for x in range(5))\n<generator object <genexpr> at 0x000000000315F678>\n>>> # 两者之间转换\n>>> list(x ** 3 for x in range(5))\n[0, 1, 8, 27, 64]\n```\n\n　　**一个迭代既可以被写成生成器函数，也可以被协程生成器表达式，均支持自动和手动迭代。而且这些生成器只支持一个active迭代，也就是说生成器的迭代器就是生成器本身。**\n\n### 迭代器（迭代就是循环）\n\n**　　迭代器包含有next方法的实现，在正确的范围内返回期待的数据以及超出范围后能够抛出StopIteration的错误停止迭代。**\n\n　　我们已经知道，可以直接作用于for循环的数据类型有以下几种：\n\n一类是集合数据类型，如list,tuple,dict,set,str等\n\n一类是generator，包括生成器和带yield的generator function\n\n这些可以直接作用于for 循环的对象统称为可迭代对象：Iterable\n\n可以使用isinstance()判断一个对象是否为可**Iterable**对象\n\n```\n>>> from collections import Iterable\n>>> isinstance([], Iterable)\nTrue\n>>> isinstance({}, Iterable)\nTrue\n>>> isinstance('abc', Iterable)\nTrue\n>>> isinstance((x for x in range(10)), Iterable)\nTrue\n>>> isinstance(100, Iterable)\nFalse\n```\n　　而生成器不但可以作用于for循环，还可以被next()函数不断调用并返回下一个值，直到最后抛出StopIteration错误表示无法继续返回下一个值了。\n\n所以这里讲一下迭代器\n\n**一个实现了iter方法的对象时可迭代的，一个实现next方法的对象是迭代器**\n\n**可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator。**\n\n可以使用isinstance()判断一个对象是否是**Iterator**对象：\n\n```\n>>> from collections import Iterator\n>>> isinstance((x for x in range(10)), Iterator)\nTrue\n>>> isinstance([], Iterator)\nFalse\n>>> isinstance({}, Iterator)\nFalse\n>>> isinstance('abc', Iterator)\nFalse\n\n```\n生成器都是`Iterator`对象，但`list`、`dict`、`str`虽然是`Iterable（可迭代对象）`，却不是`Iterator（迭代器）`。\n\n**把`list`、`dict`、`str`等`Iterable`变成`Iterator`**可以使用`iter()`函数**：**\n\n```\n>>> isinstance(iter([]), Iterator)\nTrue\n>>> isinstance(iter('abc'), Iterator)\nTrue\n```\n你可能会问，为什么`list`、`dict`、`str`等数据类型不是`Iterator`？\n\n这是因为Python的`Iterator`对象表示的是一个**数据流**，Iterator对象可以被`next()`函数调用并不断返回下一个数据，直到没有数据时抛出`StopIteration`错误。可以把这个数据流看做是一个有序序列，但我们却不能提前知道序列的长度，只能不断通过`next()`函数实现按需计算下一个数据，所以`Iterator`的计算是惰性的，只有在需要返回下一个数据时它才会计算。\n\n`Iterator`甚至可以表示一个无限大的数据流，例如全体自然数。而使用list是永远不可能存储全体自然数的。\n\n**判断下列数据类型是可迭代对象or迭代器**\n\n```\ns='hello'\nl=[1,2,3,4]\nt=(1,2,3)\nd={'a':1}\nset={1,2,3}\nf=open('a.txt')\n\ns='hello'     #字符串是可迭代对象，但不是迭代器\nl=[1,2,3,4]     #列表是可迭代对象，但不是迭代器\nt=(1,2,3)       #元组是可迭代对象，但不是迭代器\nd={'a':1}        #字典是可迭代对象，但不是迭代器\nset={1,2,3}     #集合是可迭代对象，但不是迭代器\n# *************************************\nf=open('test.txt') #文件是可迭代对象，是迭代器\n\n#如何判断是可迭代对象，只有__iter__方法，执行该方法得到的迭代器对象。\n# 及可迭代对象通过__iter__转成迭代器对象\nfrom collections import Iterator  #迭代器\nfrom collections import Iterable  #可迭代对象\n\nprint(isinstance(s,Iterator))     #判断是不是迭代器\nprint(isinstance(s,Iterable))       #判断是不是可迭代对象\n\n#把可迭代对象转换为迭代器\nprint(isinstance(iter(s),Iterator))\n```\n**　注意：文件的判断**\n```\nf = open('housing.csv')\nfrom collections import Iterator\nfrom collections import Iterable\n\nprint(isinstance(f,Iterator))\nprint(isinstance(f,Iterable))\n\nTrue\nTrue\n```\n\n　　**结论：文件是可迭代对象，也是迭代器**\n\n**小结：**\n\n*   凡是可作用于`for`循环的对象都是`Iterable`类型；\n*   凡是可作用于`next()`函数的对象都是`Iterator`类型，它们表示一个惰性计算的序列；\n*   集合数据类型如`list`、`dict`、`str`等是`Iterable`但不是`Iterator`，不过可以通过`iter()`函数获得一个`Iterator`对象。\n\nPython3的`for`循环本质上就是通过不断调用`next()`函数实现的，例如：\n\n```\nfor x in [1, 2, 3, 4, 5]:\n    pass\n```\n\n　实际上完全等价于\n```\n# 首先获得Iterator对象:\nit = iter([1, 2, 3, 4, 5])\n# 循环:\nwhile True:\n    try:\n        # 获得下一个值:\n        x = next(it)\n    except StopIteration:\n        # 遇到StopIteration就退出循环\n        break\n```\n### 对yield的总结\n\n　　（1）通常的for..in...循环中，in后面是一个数组，这个数组就是一个可迭代对象，类似的还有链表，字符串，文件。他可以是a = [1,2,3]，也可以是a = [x*x for x in range(3)]。\n\n它的缺点也很明显，就是所有数据都在内存里面，如果有海量的数据，将会非常耗内存。\n\n　　（2）生成器是可以迭代的，但是只可以读取它一次。因为用的时候才生成，比如a = (x*x for x in range(3))。!!!!注意这里是小括号而不是方括号。\n\n　　（3）生成器（generator）能够迭代的关键是他有next()方法，工作原理就是通过重复调用next()方法，直到捕获一个异常。\n\n　　（4）带有yield的函数不再是一个普通的函数，而是一个生成器generator，可用于迭代\n\n　　（5）yield是一个类似return 的关键字，迭代一次遇到yield的时候就返回yield后面或者右面的值。而且下一次迭代的时候，从上一次迭代遇到的yield后面的代码开始执行\n\n　　（6）yield就是return返回的一个值，并且记住这个返回的位置。下一次迭代就从这个位置开始。\n\n　　（7）带有yield的函数不仅仅是只用于for循环，而且可用于某个函数的参数，只要这个函数的参数也允许迭代参数。\n\n　　（8）send()和next()的区别就在于send可传递参数给yield表达式，这时候传递的参数就会作为yield表达式的值，而yield的参数是返回给调用者的值，也就是说send可以强行修改上一个yield表达式值。\n\n　　（9）send()和next()都有返回值，他们的返回值是当前迭代遇到的yield的时候，yield后面表达式的值，其实就是当前迭代yield后面的参数。\n\n　　（10）第一次调用时候必须先next（）或send（）,否则会报错，send后之所以为None是因为这时候没有上一个yield，所以也可以认为next（）等同于send(None)\n\n\n##### [原文：python 生成器和迭代器有这篇就够了](https://www.cnblogs.com/wj-1314/p/8490822.html)\n","tags":["python生成器","python迭代器"],"categories":["python"]},{"title":"Python中tuple+=赋值的四个问题","url":"/2021/05/04/python/Python中tuple+=赋值的四个问题/","content":"\n## 问题\n\n首先看第一个问题, 如下面的代码段:\n\n```\n\n>>> t = (1,2, [30,40])\n\n>>> t[2] += [50,60]\n\n```\n会产生什么结果呢？ 给出了四个选项:\n1. `t` 变成 `[1,2, [30,40,50,60]` \n2. `TypeError is raised with the message 'tuple' object does not support item assignment` \n3\\. Neither 1 nor 2\n4\\. Both 1 and 2\n\n按照之前的理解, `tuple`里面的元素是不能被修改的，因此会选`2`. 如果真是这样的话，这篇笔记就没必要了，Fluent Python中也就不会拿出一节来讲了。 正确答案是`4`\n\n\n<!--more-->\n\n\n```\n\n>>> t = (1,2,[30,40])\n\n>>> t[2] += [50,60]\n\nTraceback (most recent call last):\n\n  File \"<stdin>\", line 1, in <module>\n\nTypeError: 'tuple' object does not support item assignment\n\n>>> t\n\n(1, 2, [30, 40, 50, 60])\n\n```\n\n问题来了，为什么异常都出来了， `t`还是变了? 再看第二种情况，稍微变化一下,将`+=`变为`=`:\n\n```\n>>> t = (1,2, [30,40])\n\n>>> t[2] = [50,60]\n\n```\n结果就成酱紫了:\n\n```\n>>> t = (1,2, [30,40])\n\n>>> t[2] = [50,60]\n\nTraceback (most recent call last):\n\n  File \"<stdin>\", line 1, in <module>\n\nTypeError: 'tuple' object does not support item assignment\n\n>>> t\n\n(1, 2, [30, 40])\n```\n\n再看第三种情况,只把`+=`换为`extend`或者`append`,:\n\n```\n>>> t = (1, 2, [30,40])\n\n>>> t[2].extend([50,60])\n\n>>> t\n\n(1, 2, [30, 40, 50, 60])\n\n>>> t[2].append(70)\n\n>>> t\n\n(1, 2, [30, 40, 50, 60, 70])\n\n```\n又正常了,没抛出异常?\n\n最后第四种情况, 用变量的形式:\n\n```\n>>> a = [30,40]\n\n>>> t = (1, 2, a)\n\n>>> a+=[50,60]\n\n>>> a\n\n[30, 40, 50, 60]\n\n>>> t\n\n(1, 2, [30, 40, 50, 60])\n\n>>> t[2] += [70,80]\n\nTraceback (most recent call last):\n\n  File \"<stdin>\", line 1, in <module>\n\nTypeError: 'tuple' object does not support item assignment\n\n>>> t\n\n(1, 2, [30, 40, 50, 60, 70, 80])\n\n```\n又是一种情况, 下面就探究一下其中的原因.\n\n## 原因\n\n首先需要重温`+=`这个运算符,如`a+=b`:\n\n*   对于可变对象(mutable object)如`list`, `+=`操作的结果会直接在`a`对应的变量进行修改，而`a`对应的地址不变.\n*   对于不可变对象(imutable object)如`tuple`, `+=`则是等价于`a = a+b` 会产生新的变量，然后绑定到`a`上而已.\n\n如下代码段, 可以看出来:\n\n```\n>>> a = [1,2,3]\n\n>>> id(a)\n\n53430752\n\n>>> a+=[4,5]\n\n>>> a\n\n[1, 2, 3, 4, 5]\n\n>>> id(a)\n\n53430752 # 地址没有变化\n\n>>> b = (1,2,3)\n\n>>> id(b)\n\n49134888\n\n>>> b += (4,5)\n\n>>> b\n\n(1, 2, 3, 4, 5)\n\n>>> id(b)\n\n48560912 # 地址变化了\n\n```\n\n此外还需要注意的是, python中的`tuple`作为不可变对象, 也就是我们平时说的元素不能改变, 实际上从报错信息`TypeError: 'tuple' object does not support item assignment`来看, 更准确的说法是指其中的元素不支持赋值操作`=`(**assignment**).\n\n先看最简单的第二种情况, 它的结果是符合我们的预期, 因为`=`产生了`assign`的操作.(在[由一个例子到python的名字空间](http://shomy.top/2016/03/01/python-namespace-1/) 中指出了赋值操作`=`就是创建新的变量), 因此`s[2]=[50,60]`就会抛出异常.\n\n再看第三种情况,包含`extend/append`的, 结果tuple中的列表值发生了变化,但是没有异常抛出. 这个其实也相对容易理解. 因为我们知道`tuple`中存储的其实是元素所对应的地址(id), 因此如果没有赋值操作且tuple中的元素的`id`不变,即可,而`list.extend/append`只是修改了列表的元素,而列表本身id并没有变化,看看下面的例子:\n\n```\n>>> a=(1,2,[30,40])\n\n>>> id(a[2])\n\n140628739513736\n\n>>> a[2].extend([50,60])\n\n>>> a\n\n(1, 2, [30, 40, 50, 60])\n\n>>> id(a[2])\n\n140628739513736\n```\n目前解决了第二个和第三个问题, 先梳理一下, 其实就是两点:\n\n*   tuple内部的元素不支持赋值操作\n*   在第一条的基础上, 如果元素的`id`没有变化, 元素其实是可以改变的.\n\n现在再来看最初的第一个问题: `t[2] += [50,60]` 按照上面的结论, 不应该抛异常啊,因为在我们看来`+=` 对于可变对象`t[2]`来说, 属于`in-place`操作,也就是直接修改自身的内容, `id`并不变, 确认下id并没有变化:\n\n```\n>>> a=(1,2,[30,40])\n\n>>> id(a[2])\n\n140628739587392\n\n>>> a[2]+=[50,60]\n\nTraceback (most recent call last):\n\n  File \"<stdin>\", line 1, in <module>\n\nTypeError: 'tuple' object does not support item assignment\n\n>>> a\n\n(1, 2, [30, 40, 50, 60])\n\n>>> id(a[2]) # ID 并没有发生改变\n\n140628739587392\n\n```\n跟第三个问题仅仅从`t[2].extend`改成了`t[2]+=`, 就抛出异常了,所以问题应该是出在`+=`上了. 下面用`dis`模块看看它俩执行的步骤: 对下面的代码块执行`dis`:\n\n```\nt = (1,2, [30,40])\n\nt[2] += [50,60]\n\nt[2].extend([70, 80])\n\n```\n执行`python -m dis test.py`,结果如下，下面只保留第2,3行代码的执行过程，以及关键步骤的注释如下:\n\n```\n2          21 LOAD_NAME                0 (t)\n\n           24 LOAD_CONST               1 (2)\n\n           27 DUP_TOPX                 2\n\n           30 BINARY_SUBSCR\n\n           31 LOAD_CONST               4 (50)\n\n           34 LOAD_CONST               5 (60)\n\n           37 BUILD_LIST               2\n\n           40 INPLACE_ADD\n\n           41 ROT_THREE\n\n           42 STORE_SUBSCR\n\n3          43 LOAD_NAME                0 (t)\n\n           46 LOAD_CONST               1 (2)\n\n           49 BINARY_SUBSCR\n\n           50 LOAD_ATTR                1 (extend)\n\n           53 LOAD_CONST               6 (70)\n\n           56 LOAD_CONST               7 (80)\n\n           59 BUILD_LIST               2\n\n           62 CALL_FUNCTION            1\n\n           65 POP_TOP\n\n           66 LOAD_CONST               8 (None)\n\n           69 RETURN_VALUE\n\n```\n解释一下关键的语句:\n\n*   `30 BINARY_SUBSCR`: 表示将`t[2]`的值放在TOS(Top of Stack)，这里是指`[30, 40]`这个列表\n*   `40 INPLACE_ADD`: 表示`TOS += [50,60]` 执行这一步是可以成功的，修改了TOS的列表为`[30,40,50,60]`\n*   `42 STORE_SUBSCR`: 表示`s[2] = TOS` 问题就出在这里了，这里产生了一个**赋值操作**，因此会抛异常！但是上述对列表的修改已经完成, 这也就解释了开篇的第一个问题。\n\n再看`extend`的过程，前面都一样，只有这一行:\n\n*   `62 CALL_FUNCTION`: 这个直接调用内置extend函数完成了对原列表的修改，其中并没有`assign`操作，因此可以正常执行。\n\n现在逐渐清晰了， 换句话说，`+=`**并不是原子操作**，相当于下面的两步:\n\n```\nt[2].extend([50,60])\n\nt[2] = t[2]\n\n```\n第一步可以正确执行，但是第二步有了`=`，肯定会抛异常的。 同样这也可以解释在使用`+=`的时候，为何`t[2]`的`id`明明没有变化，但是仍然抛出异常了。\n\n现在用一句话总结下:\n\n> tuple中元素不支持`assign`操作，但是对于那些是可变对象的元素如列表，字典等，在没有`assign`操作的基础上，比如一些`in-place`操作，是可以修改内容的\n\n可以用第四个问题来简单验证一下，使用一个指向`[30,40]`的名称`a`来作为元素的值，然后对`a`做`in-place`的修改，其中并没有涉及到对tuple的`assign`操作，那肯定是正常执行的。\n\n## 总结\n\n这个问题其实以前也就遇到过，但是没想过具体的原理，后来翻书的时候又看到了， 于是花了点时间把这一个系列查了部分资料以及结合自己的理解都整理了出来, 算是饭后茶点吧, 不严谨的地方烦请指出.\n\n部分参考如下:\n\n*   [python bugs](http://bugs.python.org/issue11562)\n*   [python faq](https://docs.python.org/2/faq/programming.html#why-does-a-tuple-i-item-raise-an-exception-when-the-addition-works)\n*   [stackoverflow](https://stackoverflow.com/questions/10397121/why-does-of-a-list-within-a-python-tuple-raise-typeerror-but-modify-the-list)\n*   Fluent Python\n\n\n本文链接: [http://shomy.top/2017/08/17/python-tuple-assign/](http://shomy.top/2017/08/17/python-tuple-assign/)\n","tags":["tuple"],"categories":["python"]},{"title":"Python3 利用string模块生成密码","url":"/2021/05/04/python/Python3 利用string模块生成密码/","content":"\n\nstring模块中定义了一些常用的属性，包含所有数字、字母、可打印的所有ascii码等\n\n实例\n\n1. ascii_letters 生成所有大小写字母（a-z A-Z）\n```\nimport string\n\nletters = string.ascii_letters\nprint(letters)\n\n>>> abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n```\n\n<!--more-->\n\n2. ascii_lowercase 生成所有小写字母（a-z）\n```\nimport string\n\nlowercase = string.asscii_lowercase\nprint(lowercase)\n\n>>> abcdefghijklmnopqrstuvwxyz\n ```\n\n3. ascii_uppercase 生成所有大写字母（A-Z）\n```\nimport string\n\nuppercase = string.ascii_uppercase\nprint(uppercase)\n\n>>> ABCDEFGHIJKLMNOPQRSTUVWXYZ\n ```\n\n4. digits 生成所有数字（0-9）\n```\nimport string\n\ndigits = string.digits\nprint(digits)\n\n>>> 0123456789\n ```\n\n5. punctuation 生成所有标点符号\n```\nimport string\n\npunctuation = string.punctuation\nprint(punctuotion)\n\n>>> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n```\n\n#密码生成器\n ```\ndef main():\n    \"\"\"密码生成器\"\"\"\n    a = string.ascii_letters + string.digits + string.punctuation\n    key = random.sample(a, 16)\n    keys = \"\".join(key)\n    print(keys)\n```\n","tags":["密码","gen-secret"],"categories":["python"]},{"title":"paramiko 远程执行命令","url":"/2021/05/04/python/paramiko 远程执行命令/","content":"\n# [Python学习总结 06 paramiko 远程执行命令](https://www.cnblogs.com/wangshuo1/p/6265360.html)\n\n  有时会需要在远程的机器上执行一个命令，并获得其返回结果。对于这种情况，python 可以很容易的实现。\n\n# 1 工具\n\nPython paramiko\n\n1) Paramiko模块安装\n\n　　在Linux的Terminal中，直接输入pip install paramiko 命令安装。\n\n2）确定paramiko安装成功\n\n　　在python命令行输入import paramiko，确认是否安装成功，没报错就没问题。\n\n<!--more-->\n\n# 2 步骤\n\n1 导入 paramiko 模块\n\n```\n#!/usr/bin/python\nimport paramiko\n```\n\n2 创建 ssh 连接函数\n\n```\ndef ssh_connect( _host, _username, _password ):\n    try:\n        _ssh_fd = paramiko.SSHClient()\n        _ssh_fd.set_missing_host_key_policy( paramiko.AutoAddPolicy() )\n        _ssh_fd.connect( _host, username = _username, password = _password )\n    except Exception, e:\n        print( 'ssh %s@%s: %s' % (_username, _host, e) )\n        exit()\n    return _ssh_fd\n```\n\n3 创建命令执行函数\n\n```\ndef ssh_exec_cmd( _ssh_fd, _cmd ):\n    return _ssh_fd.exec_command( _cmd )\n```\n4 创建关闭 ssh 函数\n\n```\ndef ssh_close( _ssh_fd ):\n    _ssh_fd.close()\n```\n\n5 使用示例\n\n```def main():\n    hostname = '192.168.55.243'\n    port = 22\n    username = 'root'\n    password = 'P@ssw0rd'\n    cmd = \"ps -ef|grep java\"\n\n    sshd = ssh_connect( hostname , username , password )\n    stdin, stdout, stderr = ssh_exec_cmd( sshd, cmd )\n    err_list = stderr.readlines()\n\n    if len( err_list ) > 0:\n        print 'ERROR:' + err_list[0]\n        exit()\n\n    for item in stdout.readlines():\n        print item,\n    ssh_close( sshd )\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n　　如果执行脚本成功，会成功返回以下结果。\n\n```\nroot      2540  2536  2 14:13 pts/4    00:01:21 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.resource.App\nroot      3442  3387  0  2016 ?        01:09:00 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.product.App\nroot      3451  3390  0  2016 ?        01:04:54 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.report.App\nroot      3452  3388  0  2016 ?        00:51:00 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.workflow.launcher.App\nroot      3892  3886  0  2016 ?        00:29:59 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.charge.App\nroot      4509  4507  0 15:09 ?        00:00:00 bash -c ps -ef|grep java\nroot      4519  4509  0 15:09 ?        00:00:00 grep java\nroot     12861 12857  0 Jan06 ?        00:09:06 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.workorder.App\nroot     16484 16480  0  2016 ?        00:45:27 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.billing.App\nroot     18699 18694  0 Jan06 ?        00:09:30 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.order.App\nroot     21902 21898  0 Jan05 ?        00:18:46 java -Ddefault.client.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.language=Zh -Duser.region=CN -Duser.timezone=GMT+08 cn.com.ctsi.csdp.user.launcher.App\n ```\n\n 　　在实际的开发中，每次更新模块的jar包时，都需要使用 ps -ef | grep java, 查看模块的进程号，然后使用使用命令 kill -9 进程号，处理掉进程，然后重新启动 模块。\n\n下面尝试使用python脚本来代替手工输入代码。\n\n# 3 实例\n\n1） 启动模块\n\n```\n# -*- coding: utf-8 -*-\n\nimport paramiko\nssh = paramiko.SSHClient()\nssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\nssh.connect('192.168.55.243', username = 'root', password = 'P@ssw0rd', timeout = 5)\ncmd = 'nohup /csdp/charge_launcher-1.0-release/bin/run.sh > /csdp/charge_launcher-1.0-release/bin/nohup.out 2>&1 & \\r\\n'\n\npassword= 'P@ssw0rd'\n\nstdin, stdout, stderr = ssh.exec_command( cmd )\n##stdin, stdout, stderr = ssh.exec_command('sudo -S %s\\n' % cmd )\n##stdin.write('%s\\r\\n' % password)\n##stdin.flush()\nprint \"------------------------\"\n##print stdout.readlines()\n##print stderr.read()\n\n\nprint \"------------------------\"\ncmd = 'pwd'\nstdin, stdout, stderr = ssh.exec_command(cmd )\nprint stdout.readlines()\n\nssh.close()\n```\n2） 远程上传文件\n\n```\n# -*- coding: utf-8 -*-\nimport paramiko\n\nserverIp = '192.168.55.243'\nserverUser = 'root'\nserverPwd = 'P@ssw0rd'\n\nlocalFile = 'user-1.0-release.jar'\nlocalpath = r'D:\\workspace\\csdp201512041\\csdp-ningxia\\csdp_user\\user\\target' + os.sep + localFile\n\nremotepath = '/csdp/user_launcher-1.0-dev/lib/' + localFile\n\ndef ftpModuleFile():\n    t = paramiko.Transport(( serverIp ,22))\n    t.connect(username = serverUser , password = serverPwd)\n    sftp = paramiko.SFTPClient.from_transport(t)\n   # remotepath='/csdp/user_launcher-1.0-dev/user-1.0-release.jar'\n   # localpath= r'D:\\workspace\\csdp201512041\\csdp-ningxia\\csdp_user\\user\\target\\user-1.0-release.jar'\n    sftp.put(localpath,remotepath)\n    t.close()\n    print(\"：） 成功上传%s文件。\" % remotepath)\n\nif __name__ == '__main__':\n   ftpModuleFile()\n```\n3) 执行远程linux命令\n\n```\n# -*- coding: utf-8 -*-\nimport paramiko\n\n\nif __name__ == \"__main__\":\n    hostname = '192.168.55.243'\n    port = 22\n    username = 'root'\n    password = 'P@ssw0rd'\n    cmd = \"ps -ef|grep java\"\n\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    #ssh.connect( hostname ,22, username , password )\n    ssh.connect(hostname,username=username,password=password,allow_agent=False,look_for_keys=False)\n    stdin, stdout, stderr = ssh.exec_command(cmd )\n    list = stdout.readlines()\n    print( list )\n\n    ssh.close()\n```\n","tags":["paramiko"],"categories":["python"]},{"title":"python内置函数-排列组合函数","url":"/2021/05/04/python/python内置函数-排列组合函数/","content":"\nproduct 笛卡尔积　　（有放回抽样排列）\n\npermutations 排列　　（不放回抽样排列）\n\ncombinations 组合,没有重复　　（不放回抽样组合）\n\ncombinations_with_replacement 组合,有重复　　（有放回抽样组合）\n\n<!--more-->\n\n详细的参见[官网](https://docs.python.org/2/library/itertools.html)。\n```\n>>> for i in itertools.product('ABCD', repeat = 2):\n...     print(i)\n...\n('A', 'A') ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'B') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'C') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C') ('D', 'D')\n>>> for i in itertools.permutations('ABCD', 2):\n...     print(i)\n...\n('A', 'B') ('A', 'C') ('A', 'D') ('B', 'A') ('B', 'C') ('B', 'D') ('C', 'A') ('C', 'B') ('C', 'D') ('D', 'A') ('D', 'B') ('D', 'C')\n>>> for i in itertools.combinations('ABCD', 2):\n...     print(i)\n...\n('A', 'B') ('A', 'C') ('A', 'D') ('B', 'C') ('B', 'D') ('C', 'D')\n>>> for i in itertools.combinations_with_replacement('ABCD', 2):\n...     print(i)\n...\n('A', 'A') ('A', 'B') ('A', 'C') ('A', 'D') ('B', 'B') ('B', 'C') ('B', 'D') ('C', 'C') ('C', 'D') ('D', 'D')</pre>\n```\n\n还有就是，combinations和permutations返回的是对象地址，原因是在python3里面，返回值已经不再是list,而是iterators（迭代器）, 所以想要使用，只用将iterator 转换成list 即可， 还有其他一些函数返回的也是一个对象，需要list转换，比如 list(map())等\n\n","tags":["python内置函数"],"categories":["python"]},{"title":"Vue.js - Day4","url":"/2021/05/04/vue/vue2.0基础课程/day4/","content":"\n# Vue.js - Day4\n\n## 父组件向子组件传值\n1. 组件实例定义方式，注意：一定要使用`props`属性来定义父组件传递过来的数据\n```\n<script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: '这是父组件中的消息'\n      },\n      components: {\n        son: {\n          template: '<h1>这是子组件 --- {{finfo}}</h1>',\n          props: ['finfo']\n        }\n      }\n    });\n  </script>\n```\n2. 使用`v-bind`或简化指令，将数据传递到子组件中：\n```\n<div id=\"app\">\n    <son :finfo=\"msg\"></son>\n  </div>\n```\n\n## 子组件向父组件传值\n1. 原理：父组件将方法的引用，传递到子组件内部，子组件在内部调用父组件传递过来的方法，同时把要发送给父组件的数据，在调用方法的时候当作参数传递进去；\n2. 父组件将方法的引用传递给子组件，其中，`getMsg`是父组件中`methods`中定义的方法名称，`func`是子组件调用传递过来方法时候的方法名称\n```\n<son @func=\"getMsg\"></son>\n```\n3. 子组件内部通过`this.$emit('方法名', 要传递的数据)`方式，来调用父组件中的方法，同时把数据传递给父组件使用\n```\n<div id=\"app\">\n    <!-- 引用父组件 -->\n    <son @func=\"getMsg\"></son>\n\n    <!-- 组件模板定义 -->\n    <script type=\"x-template\" id=\"son\">\n      <div>\n        <input type=\"button\" value=\"向父组件传值\" @click=\"sendMsg\" />\n      </div>\n    </script>\n  </div>\n\n  <script>\n    // 子组件的定义方式\n    Vue.component('son', {\n      template: '#son', // 组件模板Id\n      methods: {\n        sendMsg() { // 按钮的点击事件\n          this.$emit('func', 'OK'); // 调用父组件传递过来的方法，同时把数据传递出去\n        }\n      }\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {\n        getMsg(val){ // 子组件中，通过 this.$emit() 实际调用的方法，在此进行定义\n          alert(val);\n        }\n      }\n    });\n  </script>\n```\n\n## 组件中data和props的区别\n\n## 评论列表案例\n目标：主要练习父子组件之间传值\n\n## 使用 `this.$refs` 来获取元素和组件\n```\n  <div id=\"app\">\n    <div>\n      <input type=\"button\" value=\"获取元素内容\" @click=\"getElement\" />\n      <!-- 使用 ref 获取元素 -->\n      <h1 ref=\"myh1\">这是一个大大的H1</h1>\n\n      <hr>\n      <!-- 使用 ref 获取子组件 -->\n      <my-com ref=\"mycom\"></my-com>\n    </div>\n  </div>\n\n  <script>\n    Vue.component('my-com', {\n      template: '<h5>这是一个子组件</h5>',\n      data() {\n        return {\n          name: '子组件'\n        }\n      }\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {\n        getElement() {\n          // 通过 this.$refs 来获取元素\n          console.log(this.$refs.myh1.innerText);\n          // 通过 this.$refs 来获取组件\n          console.log(this.$refs.mycom.name);\n        }\n      }\n    });\n  </script>\n```\n\n## 什么是路由\n1. **后端路由：**对于普通的网站，所有的超链接都是URL地址，所有的URL地址都对应服务器上对应的资源；\n\n2. **前端路由：**对于单页面应用程序来说，主要通过URL中的hash(#号)来实现不同页面之间的切换，同时，hash有一个特点：HTTP请求中不会包含hash相关的内容；所以，单页面程序中的页面跳转主要用hash实现；\n\n3. 在单页面应用程序中，这种通过hash改变来切换页面的方式，称作前端路由（区别于后端路由）；\n\n## 在 vue 中使用 vue-router\n1. 导入 vue-router 组件类库：\n```\n<!-- 1. 导入 vue-router 组件类库 -->\n  <script src=\"./lib/vue-router-2.7.0.js\"></script>\n```\n2. 使用 router-link 组件来导航\n```\n<!-- 2. 使用 router-link 组件来导航 -->\n<router-link to=\"/login\">登录</router-link>\n<router-link to=\"/register\">注册</router-link>\n```\n3. 使用 router-view 组件来显示匹配到的组件\n```\n<!-- 3. 使用 router-view 组件来显示匹配到的组件 -->\n<router-view></router-view>\n```\n4. 创建使用`Vue.extend`创建组件\n```\n    // 4.1 使用 Vue.extend 来创建登录组件\n    var login = Vue.extend({\n      template: '<h1>登录组件</h1>'\n    });\n\n    // 4.2 使用 Vue.extend 来创建注册组件\n    var register = Vue.extend({\n      template: '<h1>注册组件</h1>'\n    });\n```\n5. 创建一个路由 router 实例，通过 routers 属性来定义路由匹配规则\n```\n// 5. 创建一个路由 router 实例，通过 routers 属性来定义路由匹配规则\n    var router = new VueRouter({\n      routes: [\n        { path: '/login', component: login },\n        { path: '/register', component: register }\n      ]\n    });\n```\n6. 使用 router 属性来使用路由规则\n```\n// 6. 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      router: router // 使用 router 属性来使用路由规则\n    });\n```\n\n## 使用tag属性指定router-link渲染的标签类型\n\n## 设置路由重定向\n\n## 设置路由高亮\n\n## 设置路由切换动效\n\n## 在路由规则中定义参数\n1. 在规则中定义参数：\n```\n{ path: '/register/:id', component: register }\n```\n2. 通过 `this.$route.params`来获取路由中的参数：\n```\nvar register = Vue.extend({\n      template: '<h1>注册组件 --- {{this.$route.params.id}}</h1>'\n    });\n```\n\n## 使用 `children` 属性实现路由嵌套\n```\n  <div id=\"app\">\n    <router-link to=\"/account\">Account</router-link>\n\n    <router-view></router-view>\n  </div>\n\n  <script>\n    // 父路由中的组件\n    const account = Vue.extend({\n      template: `<div>\n        这是account组件\n        <router-link to=\"/account/login\">login</router-link> |\n        <router-link to=\"/account/register\">register</router-link>\n        <router-view></router-view>\n      </div>`\n    });\n\n    // 子路由中的 login 组件\n    const login = Vue.extend({\n      template: '<div>登录组件</div>'\n    });\n\n    // 子路由中的 register 组件\n    const register = Vue.extend({\n      template: '<div>注册组件</div>'\n    });\n\n    // 路由实例\n    var router = new VueRouter({\n      routes: [\n        { path: '/', redirect: '/account/login' }, // 使用 redirect 实现路由重定向\n        {\n          path: '/account',\n          component: account,\n          children: [ // 通过 children 数组属性，来实现路由的嵌套\n            { path: 'login', component: login }, // 注意，子路由的开头位置，不要加 / 路径符\n            { path: 'register', component: register }\n          ]\n        }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      components: {\n        account\n      },\n      router: router\n    });\n  </script>\n```\n\n## 命名视图实现经典布局\n1. 标签代码结构：\n```\n<div id=\"app\">\n    <router-view></router-view>\n    <div class=\"content\">\n      <router-view name=\"a\"></router-view>\n      <router-view name=\"b\"></router-view>\n    </div>\n  </div>\n```\n2. JS代码：\n```\n<script>\n    var header = Vue.component('header', {\n      template: '<div class=\"header\">header</div>'\n    });\n\n    var sidebar = Vue.component('sidebar', {\n      template: '<div class=\"sidebar\">sidebar</div>'\n    });\n\n    var mainbox = Vue.component('mainbox', {\n      template: '<div class=\"mainbox\">mainbox</div>'\n    });\n\n    // 创建路由对象\n    var router = new VueRouter({\n      routes: [\n        {\n          path: '/', components: {\n            default: header,\n            a: sidebar,\n            b: mainbox\n          }\n        }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      router\n    });\n  </script>\n```\n3. CSS 样式：\n```\n  <style>\n    .header {\n      border: 1px solid red;\n    }\n\n    .content{\n      display: flex;\n    }\n    .sidebar {\n      flex: 2;\n      border: 1px solid green;\n      height: 500px;\n    }\n    .mainbox{\n      flex: 8;\n      border: 1px solid blue;\n      height: 500px;\n    }\n  </style>\n```\n\n## `watch`属性的使用\n考虑一个问题：想要实现 `名` 和 `姓` 两个文本框的内容改变，则全名的文本框中的值也跟着改变；（用以前的知识如何实现？？？）\n\n1. 监听`data`中属性的改变：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\"> +\n    <input type=\"text\" v-model=\"lastName\"> =\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen',\n        fullName: 'jack - chen'\n      },\n      methods: {},\n      watch: {\n        'firstName': function (newVal, oldVal) { // 第一个参数是新数据，第二个参数是旧数据\n          this.fullName = newVal + ' - ' + this.lastName;\n        },\n        'lastName': function (newVal, oldVal) {\n          this.fullName = this.firstName + ' - ' + newVal;\n        }\n      }\n    });\n  </script>\n```\n2. 监听路由对象的改变：\n```\n<div id=\"app\">\n    <router-link to=\"/login\">登录</router-link>\n    <router-link to=\"/register\">注册</router-link>\n\n    <router-view></router-view>\n  </div>\n\n  <script>\n    var login = Vue.extend({\n      template: '<h1>登录组件</h1>'\n    });\n\n    var register = Vue.extend({\n      template: '<h1>注册组件</h1>'\n    });\n\n    var router = new VueRouter({\n      routes: [\n        { path: \"/login\", component: login },\n        { path: \"/register\", component: register }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      router: router,\n      watch: {\n        '$route': function (newVal, oldVal) {\n          if (newVal.path === '/login') {\n            console.log('这是登录组件');\n          }\n        }\n      }\n    });\n  </script>\n```\n\n## `computed`计算属性的使用\n1. 默认只有`getter`的计算属性：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\"> +\n    <input type=\"text\" v-model=\"lastName\"> =\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen'\n      },\n      methods: {},\n      computed: { // 计算属性； 特点：当计算属性中所以来的任何一个 data 属性改变之后，都会重新触发 本计算属性 的重新计算，从而更新 fullName 的值\n        fullName() {\n          return this.firstName + ' - ' + this.lastName;\n        }\n      }\n    });\n  </script>\n```\n2. 定义有`getter`和`setter`的计算属性：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\">\n    <input type=\"text\" v-model=\"lastName\">\n    <!-- 点击按钮重新为 计算属性 fullName 赋值 -->\n    <input type=\"button\" value=\"修改fullName\" @click=\"changeName\">\n\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen'\n      },\n      methods: {\n        changeName() {\n          this.fullName = 'TOM - chen2';\n        }\n      },\n      computed: {\n        fullName: {\n          get: function () {\n            return this.firstName + ' - ' + this.lastName;\n          },\n          set: function (newVal) {\n            var parts = newVal.split(' - ');\n            this.firstName = parts[0];\n            this.lastName = parts[1];\n          }\n        }\n      }\n    });\n  </script>\n```\n\n## `watch`、`computed`和`methods`之间的对比\n1. `computed`属性的结果会被缓存，除非依赖的响应式属性变化才会重新计算。主要当作属性来使用；\n2. `methods`方法表示一个具体的操作，主要书写业务逻辑；\n3. `watch`一个对象，键是需要观察的表达式，值是对应回调函数。主要用来监听某些特定数据的变化，从而进行某些具体的业务逻辑操作；可以看作是`computed`和`methods`的结合体；\n\n## `nrm`的安装使用\n作用：提供了一些最常用的NPM包镜像地址，能够让我们快速的切换安装包时候的服务器地址；\n什么是镜像：原来包刚一开始是只存在于国外的NPM服务器，但是由于网络原因，经常访问不到，这时候，我们可以在国内，创建一个和官网完全一样的NPM服务器，只不过，数据都是从人家那里拿过来的，除此之外，使用方式完全一样；\n1. 运行`npm i nrm -g`全局安装`nrm`包；\n2. 使用`nrm ls`查看当前所有可用的镜像源地址以及当前所使用的镜像源地址；\n3. 使用`nrm use npm`或`nrm use taobao`切换不同的镜像源地址；\n\n> 注意： nrm 只是单纯的提供了几个常用的 下载包的 URL地址，并能够让我们在 这几个 地址之间，很方便的进行切换，但是，我们每次装包的时候，使用的 装包工具，都是  npm\n\n> npm i cnpm -g\n[npm 和 cnpm 的区别，你真的搞懂了嘛](https://www.cnblogs.com/chase-star/p/10455703.html)\n\n## 相关文件\n1. [URL中的hash（井号）](http://www.cnblogs.com/joyho/articles/4430148.html)\n","tags":["vue","vue2.0基础课程"],"categories":["vue2.0基础课程"]},{"title":"Vue.js - Day3","url":"/2021/05/04/vue/vue2.0基础课程/day3/","content":"\n# Vue.js - Day3\n\n## 定义Vue组件\n什么是组件： 组件的出现，就是为了拆分Vue实例的代码量的，能够让我们以不同的组件，来划分不同的功能模块，将来我们需要什么样的功能，就可以去调用对应的组件即可；\n组件化和模块化的不同：\n + 模块化： 是从代码逻辑的角度进行划分的；方便代码分层开发，保证每个功能模块的职能单一；\n + 组件化： 是从UI界面的角度进行划分的；前端的组件化，方便UI组件的重用；\n### 全局组件定义的三种方式\n1. 使用 Vue.extend 配合 Vue.component 方法：\n```\nvar login = Vue.extend({\n      template: '<h1>登录</h1>'\n    });\n    Vue.component('login', login);\n```\n2. 直接使用 Vue.component 方法：\n```\nVue.component('register', {\n      template: '<h1>注册</h1>'\n    });\n```\n3. 将模板字符串，定义到script标签种：\n```\n<script id=\"tmpl\" type=\"x-template\">\n      <div><a href=\"#\">登录</a> | <a href=\"#\">注册</a></div>\n    </script>\n```\n同时，需要使用 Vue.component 来定义组件：\n```\nVue.component('account', {\n      template: '#tmpl'\n    });\n```\n\n> 注意： 组件中的DOM结构，有且只能有唯一的根元素（Root Element）来进行包裹！\n\n### 组件中展示数据和响应事件\n1. 在组件中，`data`需要被定义为一个方法，例如：\n```\nVue.component('account', {\n      template: '#tmpl',\n      data() {\n        return {\n          msg: '大家好！'\n        }\n      },\n      methods:{\n        login(){\n          alert('点击了登录按钮');\n        }\n      }\n    });\n```\n2. 在子组件中，如果将模板字符串，定义到了script标签中，那么，要访问子组件身上的`data`属性中的值，需要使用`this`来访问；\n\n### 【重点】为什么组件中的data属性必须定义为一个方法并返回一个对象\n1. 通过计数器案例演示\n\n### 使用`components`属性定义局部子组件\n1. 组件实例定义方式：\n```\n<script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      components: { // 定义子组件\n        account: { // account 组件\n          template: '<div><h1>这是Account组件{{name}}</h1><login></login></div>', // 在这里使用定义的子组件\n          components: { // 定义子组件的子组件\n            login: { // login 组件\n              template: \"<h3>这是登录组件</h3>\"\n            }\n          }\n        }\n      }\n    });\n  </script>\n```\n2. 引用组件：\n```\n<div id=\"app\">\n    <account></account>\n  </div>\n```\n\n## 使用`flag`标识符结合`v-if`和`v-else`切换组件\n1. 页面结构：\n```\n<div id=\"app\">\n    <input type=\"button\" value=\"toggle\" @click=\"flag=!flag\">\n    <my-com1 v-if=\"flag\"></my-com1>\n    <my-com2 v-else=\"flag\"></my-com2>\n  </div>\n```\n2. Vue实例定义：\n```\n<script>\n    Vue.component('myCom1', {\n      template: '<h3>奔波霸</h3>'\n    })\n\n    Vue.component('myCom2', {\n      template: '<h3>霸波奔</h3>'\n    })\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        flag: true\n      },\n      methods: {}\n    });\n  </script>\n```\n\n## 使用`:is`属性来切换不同的子组件,并添加切换动画\n1. 组件实例定义方式：\n```\n  // 登录组件\n    const login = Vue.extend({\n      template: `<div>\n        <h3>登录组件</h3>\n      </div>`\n    });\n    Vue.component('login', login);\n\n    // 注册组件\n    const register = Vue.extend({\n      template: `<div>\n        <h3>注册组件</h3>\n      </div>`\n    });\n    Vue.component('register', register);\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: { comName: 'login' },\n      methods: {}\n    });\n```\n2. 使用`component`标签，来引用组件，并通过`:is`属性来指定要加载的组件：\n```\n  <div id=\"app\">\n    <a href=\"#\" @click.prevent=\"comName='login'\">登录</a>\n    <a href=\"#\" @click.prevent=\"comName='register'\">注册</a>\n    <hr>\n    <transition mode=\"out-in\">\n      <component :is=\"comName\"></component>\n    </transition>\n  </div>\n```\n3. 添加切换样式：\n```\n  <style>\n    .v-enter,\n    .v-leave-to {\n      opacity: 0;\n      transform: translateX(30px);\n    }\n\n    .v-enter-active,\n    .v-leave-active {\n      position: absolute;\n      transition: all 0.3s ease;\n    }\n\n    h3{\n      margin: 0;\n    }\n  </style>\n```\n\n## 父组件向子组件传值\n1. 组件实例定义方式，注意：一定要使用`props`属性来定义父组件传递过来的数据\n```\n<script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        msg: '这是父组件中的消息'\n      },\n      components: {\n        son: {\n          template: '<h1>这是子组件 --- {{finfo}}</h1>',\n          props: ['finfo']\n        }\n      }\n    });\n  </script>\n```\n2. 使用`v-bind`或简化指令，将数据传递到子组件中：\n```\n<div id=\"app\">\n    <son :finfo=\"msg\"></son>\n  </div>\n```\n\n## 子组件向父组件传值\n1. 原理：父组件将方法的引用，传递到子组件内部，子组件在内部调用父组件传递过来的方法，同时把要发送给父组件的数据，在调用方法的时候当作参数传递进去；\n2. 父组件将方法的引用传递给子组件，其中，`getMsg`是父组件中`methods`中定义的方法名称，`func`是子组件调用传递过来方法时候的方法名称\n```\n<son @func=\"getMsg\"></son>\n```\n3. 子组件内部通过`this.$emit('方法名', 要传递的数据)`方式，来调用父组件中的方法，同时把数据传递给父组件使用\n```\n<div id=\"app\">\n    <!-- 引用父组件 -->\n    <son @func=\"getMsg\"></son>\n\n    <!-- 组件模板定义 -->\n    <script type=\"x-template\" id=\"son\">\n      <div>\n        <input type=\"button\" value=\"向父组件传值\" @click=\"sendMsg\" />\n      </div>\n    </script>\n  </div>\n\n  <script>\n    // 子组件的定义方式\n    Vue.component('son', {\n      template: '#son', // 组件模板Id\n      methods: {\n        sendMsg() { // 按钮的点击事件\n          this.$emit('func', 'OK'); // 调用父组件传递过来的方法，同时把数据传递出去\n        }\n      }\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {\n        getMsg(val){ // 子组件中，通过 this.$emit() 实际调用的方法，在此进行定义\n          alert(val);\n        }\n      }\n    });\n  </script>\n```\n\n## 评论列表案例\n目标：主要练习父子组件之间传值\n\n## 使用 `this.$refs` 来获取元素和组件\n```\n  <div id=\"app\">\n    <div>\n      <input type=\"button\" value=\"获取元素内容\" @click=\"getElement\" />\n      <!-- 使用 ref 获取元素 -->\n      <h1 ref=\"myh1\">这是一个大大的H1</h1>\n\n      <hr>\n      <!-- 使用 ref 获取子组件 -->\n      <my-com ref=\"mycom\"></my-com>\n    </div>\n  </div>\n\n  <script>\n    Vue.component('my-com', {\n      template: '<h5>这是一个子组件</h5>',\n      data() {\n        return {\n          name: '子组件'\n        }\n      }\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {\n        getElement() {\n          // 通过 this.$refs 来获取元素\n          console.log(this.$refs.myh1.innerText);\n          // 通过 this.$refs 来获取组件\n          console.log(this.$refs.mycom.name);\n        }\n      }\n    });\n  </script>\n```\n\n## 什么是路由\n1. 对于普通的网站，所有的超链接都是URL地址，所有的URL地址都对应服务器上对应的资源；\n\n2. 对于单页面应用程序来说，主要通过URL中的hash(#号)来实现不同页面之间的切换，同时，hash有一个特点：HTTP请求中不会包含hash相关的内容；所以，单页面程序中的页面跳转主要用hash实现；\n\n3. 在单页面应用程序中，这种通过hash改变来切换页面的方式，称作前端路由（区别于后端路由）；\n\n## 在 vue 中使用 vue-router\n1. 导入 vue-router 组件类库：\n```\n<!-- 1. 导入 vue-router 组件类库 -->\n  <script src=\"./lib/vue-router-2.7.0.js\"></script>\n```\n2. 使用 router-link 组件来导航\n```\n<!-- 2. 使用 router-link 组件来导航 -->\n<router-link to=\"/login\">登录</router-link>\n<router-link to=\"/register\">注册</router-link>\n```\n3. 使用 router-view 组件来显示匹配到的组件\n```\n<!-- 3. 使用 router-view 组件来显示匹配到的组件 -->\n<router-view></router-view>\n```\n4. 创建使用`Vue.extend`创建组件\n```\n    // 4.1 使用 Vue.extend 来创建登录组件\n    var login = Vue.extend({\n      template: '<h1>登录组件</h1>'\n    });\n\n    // 4.2 使用 Vue.extend 来创建注册组件\n    var register = Vue.extend({\n      template: '<h1>注册组件</h1>'\n    });\n```\n5. 创建一个路由 router 实例，通过 routers 属性来定义路由匹配规则\n```\n// 5. 创建一个路由 router 实例，通过 routers 属性来定义路由匹配规则\n    var router = new VueRouter({\n      routes: [\n        { path: '/login', component: login },\n        { path: '/register', component: register }\n      ]\n    });\n```\n6. 使用 router 属性来使用路由规则\n```\n// 6. 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      router: router // 使用 router 属性来使用路由规则\n    });\n```\n\n## 设置路由高亮\n\n## 设置路由切换动效\n\n## 在路由规则中定义参数\n1. 在规则中定义参数：\n```\n{ path: '/register/:id', component: register }\n```\n2. 通过 `this.$route.params`来获取路由中的参数：\n```\nvar register = Vue.extend({\n      template: '<h1>注册组件 --- {{this.$route.params.id}}</h1>'\n    });\n```\n\n## 使用 `children` 属性实现路由嵌套\n```\n  <div id=\"app\">\n    <router-link to=\"/account\">Account</router-link>\n\n    <router-view></router-view>\n  </div>\n\n  <script>\n    // 父路由中的组件\n    const account = Vue.extend({\n      template: `<div>\n        这是account组件\n        <router-link to=\"/account/login\">login</router-link> |\n        <router-link to=\"/account/register\">register</router-link>\n        <router-view></router-view>\n      </div>`\n    });\n\n    // 子路由中的 login 组件\n    const login = Vue.extend({\n      template: '<div>登录组件</div>'\n    });\n\n    // 子路由中的 register 组件\n    const register = Vue.extend({\n      template: '<div>注册组件</div>'\n    });\n\n    // 路由实例\n    var router = new VueRouter({\n      routes: [\n        { path: '/', redirect: '/account/login' }, // 使用 redirect 实现路由重定向\n        {\n          path: '/account',\n          component: account,\n          children: [ // 通过 children 数组属性，来实现路由的嵌套\n            { path: 'login', component: login }, // 注意，子路由的开头位置，不要加 / 路径符\n            { path: 'register', component: register }\n          ]\n        }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      components: {\n        account\n      },\n      router: router\n    });\n  </script>\n```\n\n## 命名视图实现经典布局\n1. 标签代码结构：\n```\n<div id=\"app\">\n    <router-view></router-view>\n    <div class=\"content\">\n      <router-view name=\"a\"></router-view>\n      <router-view name=\"b\"></router-view>\n    </div>\n  </div>\n```\n2. JS代码：\n```\n<script>\n    var header = Vue.component('header', {\n      template: '<div class=\"header\">header</div>'\n    });\n\n    var sidebar = Vue.component('sidebar', {\n      template: '<div class=\"sidebar\">sidebar</div>'\n    });\n\n    var mainbox = Vue.component('mainbox', {\n      template: '<div class=\"mainbox\">mainbox</div>'\n    });\n\n    // 创建路由对象\n    var router = new VueRouter({\n      routes: [\n        {\n          path: '/', components: {\n            default: header,\n            a: sidebar,\n            b: mainbox\n          }\n        }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      router\n    });\n  </script>\n```\n3. CSS 样式：\n```\n  <style>\n    .header {\n      border: 1px solid red;\n    }\n\n    .content{\n      display: flex;\n    }\n    .sidebar {\n      flex: 2;\n      border: 1px solid green;\n      height: 500px;\n    }\n    .mainbox{\n      flex: 8;\n      border: 1px solid blue;\n      height: 500px;\n    }\n  </style>\n```\n\n## `watch`属性的使用\n考虑一个问题：想要实现 `名` 和 `姓` 两个文本框的内容改变，则全名的文本框中的值也跟着改变；（用以前的知识如何实现？？？）\n\n1. 监听`data`中属性的改变：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\"> +\n    <input type=\"text\" v-model=\"lastName\"> =\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen',\n        fullName: 'jack - chen'\n      },\n      methods: {},\n      watch: {\n        'firstName': function (newVal, oldVal) { // 第一个参数是新数据，第二个参数是旧数据\n          this.fullName = newVal + ' - ' + this.lastName;\n        },\n        'lastName': function (newVal, oldVal) {\n          this.fullName = this.firstName + ' - ' + newVal;\n        }\n      }\n    });\n  </script>\n```\n2. 监听路由对象的改变：\n```\n<div id=\"app\">\n    <router-link to=\"/login\">登录</router-link>\n    <router-link to=\"/register\">注册</router-link>\n\n    <router-view></router-view>\n  </div>\n\n  <script>\n    var login = Vue.extend({\n      template: '<h1>登录组件</h1>'\n    });\n\n    var register = Vue.extend({\n      template: '<h1>注册组件</h1>'\n    });\n\n    var router = new VueRouter({\n      routes: [\n        { path: \"/login\", component: login },\n        { path: \"/register\", component: register }\n      ]\n    });\n\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {},\n      methods: {},\n      router: router,\n      watch: {\n        '$route': function (newVal, oldVal) {\n          if (newVal.path === '/login') {\n            console.log('这是登录组件');\n          }\n        }\n      }\n    });\n  </script>\n```\n\n## `computed`计算属性的使用\n1. 默认只有`getter`的计算属性：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\"> +\n    <input type=\"text\" v-model=\"lastName\"> =\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen'\n      },\n      methods: {},\n      computed: { // 计算属性； 特点：当计算属性中所以来的任何一个 data 属性改变之后，都会重新触发 本计算属性 的重新计算，从而更新 fullName 的值\n        fullName() {\n          return this.firstName + ' - ' + this.lastName;\n        }\n      }\n    });\n  </script>\n```\n2. 定义有`getter`和`setter`的计算属性：\n```\n<div id=\"app\">\n    <input type=\"text\" v-model=\"firstName\">\n    <input type=\"text\" v-model=\"lastName\">\n    <!-- 点击按钮重新为 计算属性 fullName 赋值 -->\n    <input type=\"button\" value=\"修改fullName\" @click=\"changeName\">\n\n    <span>{{fullName}}</span>\n  </div>\n\n  <script>\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        firstName: 'jack',\n        lastName: 'chen'\n      },\n      methods: {\n        changeName() {\n          this.fullName = 'TOM - chen2';\n        }\n      },\n      computed: {\n        fullName: {\n          get: function () {\n            return this.firstName + ' - ' + this.lastName;\n          },\n          set: function (newVal) {\n            var parts = newVal.split(' - ');\n            this.firstName = parts[0];\n            this.lastName = parts[1];\n          }\n        }\n      }\n    });\n  </script>\n```\n\n## `watch`、`computed`和`methods`之间的对比\n1. `computed`属性的结果会被缓存，除非依赖的响应式属性变化才会重新计算。主要当作属性来使用；\n2. `methods`方法表示一个具体的操作，主要书写业务逻辑；\n3. `watch`一个对象，键是需要观察的表达式，值是对应回调函数。主要用来监听某些特定数据的变化，从而进行某些具体的业务逻辑操作；可以看作是`computed`和`methods`的结合体；\n\n## `nrm`的安装使用\n作用：提供了一些最常用的NPM包镜像地址，能够让我们快速的切换安装包时候的服务器地址；\n什么是镜像：原来包刚一开始是只存在于国外的NPM服务器，但是由于网络原因，经常访问不到，这时候，我们可以在国内，创建一个和官网完全一样的NPM服务器，只不过，数据都是从人家那里拿过来的，除此之外，使用方式完全一样；\n1. 运行`npm i nrm -g`全局安装`nrm`包；\n2. 使用`nrm ls`查看当前所有可用的镜像源地址以及当前所使用的镜像源地址；\n3. 使用`nrm use npm`或`nrm use taobao`切换不同的镜像源地址；\n\n## 相关文件\n1. [URL中的hash（井号）](http://www.cnblogs.com/joyho/articles/4430148.html)\n","tags":["vue","vue2.0基础课程"],"categories":["vue2.0基础课程"]},{"title":"Vue.js - Day2","url":"/2021/05/02/vue/vue2.0基础课程/day2/","content":"\n# Vue.js - Day2\n\n## 品牌管理案例\n\n### 添加新品牌\n\n### 删除品牌\n\n### 根据条件筛选品牌\n\n1. 1.x 版本中的filterBy指令，在2.x中已经被废除：\n\n[filterBy - 指令](https://v1-cn.vuejs.org/api/#filterBy)\n\n```\n\n<tr v-for=\"item in list | filterBy searchName in 'name'\">\n\n  <td>{{item.id}}</td>\n\n  <td>{{item.name}}</td>\n\n  <td>{{item.ctime}}</td>\n\n  <td>\n\n    <a href=\"#\" @click.prevent=\"del(item.id)\">删除</a>\n\n  </td>\n\n</tr>\n\n```\n\n2. 在2.x版本中[手动实现筛选的方式](https://cn.vuejs.org/v2/guide/list.html#显示过滤-排序结果)：\n\n+ 筛选框绑定到 VM 实例中的 `searchName` 属性：\n\n```\n\n<hr> 输入筛选名称：\n\n<input type=\"text\" v-model=\"searchName\">\n\n```\n\n+ 在使用 `v-for` 指令循环每一行数据的时候，不再直接 `item in list`，而是 `in` 一个 过滤的methods 方法，同时，把过滤条件`searchName`传递进去：\n\n```\n\n<tbody>\n\n      <tr v-for=\"item in search(searchName)\">\n\n        <td>{{item.id}}</td>\n\n        <td>{{item.name}}</td>\n\n        <td>{{item.ctime}}</td>\n\n        <td>\n\n          <a href=\"#\" @click.prevent=\"del(item.id)\">删除</a>\n\n        </td>\n\n      </tr>\n\n    </tbody>\n\n```\n\n+ `search` 过滤方法中，使用 数组的 `filter` 方法进行过滤：\n\n```\n\nsearch(name) {\n\n  return this.list.filter(x => {\n\n    return x.name.indexOf(name) != -1;\n\n  });\n\n}\n\n```\n\n## Vue调试工具`vue-devtools`的安装步骤和使用\n\n[Vue.js devtools - 翻墙安装方式 - 推荐](https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd?hl=zh-CN)\n\n## 过滤器\n\n概念：Vue.js 允许你自定义过滤器，**可被用作一些常见的文本格式化**。过滤器可以用在两个地方：**mustache 插值和 v-bind 表达式**。过滤器应该被添加在 JavaScript 表达式的尾部，由“管道”符指示；\n\n### 私有过滤器\n\n1. HTML元素：\n\n```\n\n<td>{{item.ctime | dataFormat('yyyy-mm-dd')}}</td>\n\n```\n\n2. 私有 `filters` 定义方式：\n\n```\n\nfilters: { // 私有局部过滤器，只能在 当前 VM 对象所控制的 View 区域进行使用\n\n    dataFormat(input, pattern = \"\") { // 在参数列表中 通过 pattern=\"\" 来指定形参默认值，防止报错\n\n      var dt = new Date(input);\n\n      // 获取年月日\n\n      var y = dt.getFullYear();\n\n      var m = (dt.getMonth() + 1).toString().padStart(2, '0');\n\n      var d = dt.getDate().toString().padStart(2, '0');\n\n\n\n      // 如果 传递进来的字符串类型，转为小写之后，等于 yyyy-mm-dd，那么就返回 年-月-日\n\n      // 否则，就返回  年-月-日 时：分：秒\n\n      if (pattern.toLowerCase() === 'yyyy-mm-dd') {\n\n        return `${y}-${m}-${d}`;\n\n      } else {\n\n        // 获取时分秒\n\n        var hh = dt.getHours().toString().padStart(2, '0');\n\n        var mm = dt.getMinutes().toString().padStart(2, '0');\n\n        var ss = dt.getSeconds().toString().padStart(2, '0');\n\n\n\n        return `${y}-${m}-${d} ${hh}:${mm}:${ss}`;\n\n      }\n\n    }\n\n  }\n\n```\n\n\n\n> 使用ES6中的字符串新方法 String.prototype.padStart(maxLength, fillString='') 或 String.prototype.padEnd(maxLength, fillString='')来填充字符串；\n\n\n\n\n\n### 全局过滤器\n\n```\n\n// 定义一个全局过滤器\n\nVue.filter('dataFormat', function (input, pattern = '') {\n\n  var dt = new Date(input);\n\n  // 获取年月日\n\n  var y = dt.getFullYear();\n\n  var m = (dt.getMonth() + 1).toString().padStart(2, '0');\n\n  var d = dt.getDate().toString().padStart(2, '0');\n\n\n\n  // 如果 传递进来的字符串类型，转为小写之后，等于 yyyy-mm-dd，那么就返回 年-月-日\n\n  // 否则，就返回  年-月-日 时：分：秒\n\n  if (pattern.toLowerCase() === 'yyyy-mm-dd') {\n\n    return `${y}-${m}-${d}`;\n\n  } else {\n\n    // 获取时分秒\n\n    var hh = dt.getHours().toString().padStart(2, '0');\n\n    var mm = dt.getMinutes().toString().padStart(2, '0');\n\n    var ss = dt.getSeconds().toString().padStart(2, '0');\n\n\n\n    return `${y}-${m}-${d} ${hh}:${mm}:${ss}`;\n\n  }\n\n});\n\n```\n\n\n\n> 注意：当有局部和全局两个名称相同的过滤器时候，会以就近原则进行调用，即：局部过滤器优先于全局过滤器被调用！\n\n\n\n## 键盘修饰符以及自定义键盘修饰符\n\n### 1.x中自定义键盘修饰符【了解即可】\n\n```\n\nVue.directive('on').keyCodes.f2 = 113;\n\n```\n\n### [2.x中自定义键盘修饰符](https://cn.vuejs.org/v2/guide/events.html#键值修饰符)\n\n1. 通过`Vue.config.keyCodes.名称 = 按键值`来自定义案件修饰符的别名：\n\n```\n\nVue.config.keyCodes.f2 = 113;\n\n```\n\n2. 使用自定义的按键修饰符：\n\n```\n\n<input type=\"text\" v-model=\"name\" @keyup.f2=\"add\">\n\n```\n\n\n\n\n\n## [自定义指令](https://cn.vuejs.org/v2/guide/custom-directive.html)\n\n1. 自定义全局和局部的 自定义指令：\n\n```\n\n    // 自定义全局指令 v-focus，为绑定的元素自动获取焦点：\n\n    Vue.directive('focus', {\n\n      inserted: function (el) { // inserted 表示被绑定元素插入父节点时调用\n\n        el.focus();\n\n      }\n\n    });\n\n\n\n    // 自定义局部指令 v-color 和 v-font-weight，为绑定的元素设置指定的字体颜色 和 字体粗细：\n\n      directives: {\n\n        color: { // 为元素设置指定的字体颜色\n\n          bind(el, binding) {\n\n            el.style.color = binding.value;\n\n          }\n\n        },\n\n        'font-weight': function (el, binding2) { // 自定义指令的简写形式，等同于定义了 bind 和 update 两个钩子函数\n\n          el.style.fontWeight = binding2.value;\n\n        }\n\n      }\n\n```\n\n2. 自定义指令的使用方式：\n\n```\n\n<input type=\"text\" v-model=\"searchName\" v-focus v-color=\"'red'\" v-font-weight=\"900\">\n\n```\n\n\n\n## Vue 1.x 中 自定义元素指令【已废弃,了解即可】\n```\nVue.elementDirective('red-color', {\n  bind: function () {\n    this.el.style.color = 'red';\n  }\n});\n```\n使用方式：\n```\n<red-color>1232</red-color>\n```\n\n\n## [vue实例的生命周期](https://cn.vuejs.org/v2/guide/instance.html#实例生命周期)\n+ 什么是生命周期：从Vue实例创建、运行、到销毁期间，总是伴随着各种各样的事件，这些事件，统称为生命周期！\n+ [生命周期钩子](https://cn.vuejs.org/v2/api/#选项-生命周期钩子)：就是生命周期事件的别名而已；\n+ 生命周期钩子 = 生命周期函数 = 生命周期事件\n+ 主要的生命周期函数分类：\n - 创建期间的生命周期函数：\n  \t+ beforeCreate：实例刚在内存中被创建出来，此时，还没有初始化好 data 和 methods 属性\n  \t+ created：实例已经在内存中创建OK，此时 data 和 methods 已经创建OK，此时还没有开始 编译模板\n  \t+ beforeMount：此时已经完成了模板的编译，但是还没有挂载到页面中\n  \t+ mounted：此时，已经将编译好的模板，挂载到了页面指定的容器中显示\n - 运行期间的生命周期函数：\n \t+ beforeUpdate：状态更新之前执行此函数， 此时 data 中的状态值是最新的，但是界面上显示的 数据还是旧的，因为此时还没有开始重新渲染DOM节点\n \t+ updated：实例更新完毕之后调用此函数，此时 data 中的状态值 和 界面上显示的数据，都已经完成了更新，界面已经被重新渲染好了！\n - 销毁期间的生命周期函数：\n \t+ beforeDestroy：实例销毁之前调用。在这一步，实例仍然完全可用。\n \t+ destroyed：Vue 实例销毁后调用。调用后，Vue 实例指示的所有东西都会解绑定，所有的事件监听器会被移除，所有的子实例也会被销毁。\n![lifecycle](https://user-images.githubusercontent.com/28568478/116815239-3b63c780-ab8f-11eb-9453-11e40b354c4e.png)\n\n\n## [vue-resource 实现 get, post, jsonp请求](https://github.com/pagekit/vue-resource)\n除了 vue-resource 之外，还可以使用 `axios` 的第三方包实现实现数据的请求\n1. 之前的学习中，如何发起数据请求？\n2. 常见的数据请求类型？  get  post jsonp\n3. 测试的URL请求资源地址：\n + get请求地址： http://vue.studyit.io/api/getlunbo\n + post请求地址：http://vue.studyit.io/api/post\n + jsonp请求地址：http://vue.studyit.io/api/jsonp\n4. JSONP的实现原理\n + 由于浏览器的安全性限制，不允许AJAX访问 协议不同、域名不同、端口号不同的 数据接口，浏览器认为这种访问不安全；\n + 可以通过动态创建script标签的形式，把script标签的src属性，指向数据接口的地址，因为script标签不存在跨域限制，这种数据获取方式，称作JSONP（注意：根据JSONP的实现原理，知晓，JSONP只支持Get请求）；\n + 具体实现过程：\n \t- 先在客户端定义一个回调方法，预定义对数据的操作；\n \t- 再把这个回调方法的名称，通过URL传参的形式，提交到服务器的数据接口；\n \t- 服务器数据接口组织好要发送给客户端的数据，再拿着客户端传递过来的回调方法名称，拼接出一个调用这个方法的字符串，发送给客户端去解析执行；\n \t- 客户端拿到服务器返回的字符串之后，当作Script脚本去解析执行，这样就能够拿到JSONP的数据了；\n + 带大家通过 Node.js ，来手动实现一个JSONP的请求例子；\n ```\n    const http = require('http');\n    // 导入解析 URL 地址的核心模块\n    const urlModule = require('url');\n\n    const server = http.createServer();\n    // 监听 服务器的 request 请求事件，处理每个请求\n    server.on('request', (req, res) => {\n      const url = req.url;\n\n      // 解析客户端请求的URL地址\n      var info = urlModule.parse(url, true);\n\n      // 如果请求的 URL 地址是 /getjsonp ，则表示要获取JSONP类型的数据\n      if (info.pathname === '/getjsonp') {\n        // 获取客户端指定的回调函数的名称\n        var cbName = info.query.callback;\n        // 手动拼接要返回给客户端的数据对象\n        var data = {\n          name: 'zs',\n          age: 22,\n          gender: '男',\n          hobby: ['吃饭', '睡觉', '运动']\n        }\n        // 拼接出一个方法的调用，在调用这个方法的时候，把要发送给客户端的数据，序列化为字符串，作为参数传递给这个调用的方法：\n        var result = `${cbName}(${JSON.stringify(data)})`;\n        // 将拼接好的方法的调用，返回给客户端去解析执行\n        res.end(result);\n      } else {\n        res.end('404');\n      }\n    });\n\n    server.listen(3000, () => {\n      console.log('server running at http://127.0.0.1:3000');\n    });\n ```\n5. vue-resource 的配置步骤：\n + 直接在页面中，通过`script`标签，引入 `vue-resource` 的脚本文件；\n + 注意：引用的先后顺序是：先引用 `Vue` 的脚本文件，再引用 `vue-resource` 的脚本文件；\n6. 发送get请求：\n```\ngetInfo() { // get 方式获取数据\n  this.$http.get('http://127.0.0.1:8899/api/getlunbo').then(res => {\n    console.log(res.body);\n  })\n}\n```\n7. 发送post请求：\n```\npostInfo() {\n  var url = 'http://127.0.0.1:8899/api/post';\n  // post 方法接收三个参数：\n  // 参数1： 要请求的URL地址\n  // 参数2： 要发送的数据对象\n  // 参数3： 指定post提交的编码类型为 application/x-www-form-urlencoded\n  this.$http.post(url, { name: 'zs' }, { emulateJSON: true }).then(res => {\n    console.log(res.body);\n  });\n}\n```\n8. 发送JSONP请求获取数据：\n```\njsonpInfo() { // JSONP形式从服务器获取数据\n  var url = 'http://127.0.0.1:8899/api/jsonp';\n  this.$http.jsonp(url).then(res => {\n    console.log(res.body);\n  });\n}\n```\n\n## 配置本地数据库和数据接口API\n1. 先解压安装 `PHPStudy`;\n2. 解压安装 `Navicat` 这个数据库可视化工具，并激活；\n3. 打开 `Navicat` 工具，新建空白数据库，名为 `dtcmsdb4`;\n4. 双击新建的数据库，连接上这个空白数据库，在新建的数据库上`右键` -> `运行SQL文件`，选择并执行 `dtcmsdb4.sql` 这个数据库脚本文件；如果执行不报错，则数据库导入完成；\n5. 进入文件夹 `vuecms3_nodejsapi` 内部，执行 `npm i` 安装所有的依赖项；\n6. 先确保本机安装了 `nodemon`, 没有安装，则运行 `npm i nodemon -g` 进行全局安装，安装完毕后，进入到 `vuecms3_nodejsapi`目录 -> `src`目录 -> 双击运行 `start.bat`\n7. 如果API启动失败，请检查 PHPStudy 是否正常开启，同时，检查 `app.js` 中第 `14行` 中数据库连接配置字符串是否正确；PHPStudy 中默认的 用户名是root，默认的密码也是root\n\n## 品牌管理改造\n### 展示品牌列表\n\n### 添加品牌数据\n\n### 删除品牌数据\n\n## [Vue中的动画](https://cn.vuejs.org/v2/guide/transitions.html)\n为什么要有动画：动画能够提高用户的体验，帮助用户更好的理解页面中的功能；\n\n### 使用过渡类名\n1. HTML结构：\n```\n<div id=\"app\">\n    <input type=\"button\" value=\"动起来\" @click=\"myAnimate\">\n    <!-- 使用 transition 将需要过渡的元素包裹起来 -->\n    <transition name=\"fade\">\n      <div v-show=\"isshow\">动画哦</div>\n    </transition>\n  </div>\n```\n2. VM 实例：\n```\n// 创建 Vue 实例，得到 ViewModel\nvar vm = new Vue({\n  el: '#app',\n  data: {\n    isshow: false\n  },\n  methods: {\n    myAnimate() {\n      this.isshow = !this.isshow;\n    }\n  }\n});\n```\n3. 定义两组类样式：\n```\n/* 定义进入和离开时候的过渡状态 */\n    .fade-enter-active,\n    .fade-leave-active {\n      transition: all 0.2s ease;\n      position: absolute;\n    }\n\n    /* 定义进入过渡的开始状态 和 离开过渡的结束状态 */\n    .fade-enter,\n    .fade-leave-to {\n      opacity: 0;\n      transform: translateX(100px);\n    }\n```\n\n### [使用第三方 CSS 动画库](https://cn.vuejs.org/v2/guide/transitions.html#自定义过渡类名)\n1. 导入动画类库：\n```\n<link rel=\"stylesheet\" type=\"text/css\" href=\"./lib/animate.css\">\n```\n2. 定义 transition 及属性：\n```\n<transition\n\tenter-active-class=\"fadeInRight\"\n    leave-active-class=\"fadeOutRight\"\n    :duration=\"{ enter: 500, leave: 800 }\">\n  \t<div class=\"animated\" v-show=\"isshow\">动画哦</div>\n</transition>\n```\n\n### 使用动画钩子函数\n1. 定义 transition 组件以及三个钩子函数：\n```\n<div id=\"app\">\n    <input type=\"button\" value=\"切换动画\" @click=\"isshow = !isshow\">\n    <transition\n    @before-enter=\"beforeEnter\"\n    @enter=\"enter\"\n    @after-enter=\"afterEnter\">\n      <div v-if=\"isshow\" class=\"show\">OK</div>\n    </transition>\n  </div>\n```\n2. 定义三个 methods 钩子方法：\n```\nmethods: {\n        beforeEnter(el) { // 动画进入之前的回调\n          el.style.transform = 'translateX(500px)';\n        },\n        enter(el, done) { // 动画进入完成时候的回调\n          el.offsetWidth;\n          el.style.transform = 'translateX(0px)';\n          done();\n        },\n        afterEnter(el) { // 动画进入完成之后的回调\n          this.isshow = !this.isshow;\n        }\n      }\n```\n3. 定义动画过渡时长和样式：\n```\n.show{\n      transition: all 0.4s ease;\n    }\n```\n\n\n### [v-for 的列表过渡](https://cn.vuejs.org/v2/guide/transitions.html#列表的进入和离开过渡)\n1. 定义过渡样式：\n```\n<style>\n    .list-enter,\n    .list-leave-to {\n      opacity: 0;\n      transform: translateY(10px);\n    }\n\n    .list-enter-active,\n    .list-leave-active {\n      transition: all 0.3s ease;\n    }\n</style>\n```\n2. 定义DOM结构，其中，需要使用 transition-group 组件把v-for循环的列表包裹起来：\n```\n  <div id=\"app\">\n    <input type=\"text\" v-model=\"txt\" @keyup.enter=\"add\">\n\n    <transition-group tag=\"ul\" name=\"list\">\n      <li v-for=\"(item, i) in list\" :key=\"i\">{{item}}</li>\n    </transition-group>\n  </div>\n```\n3. 定义 VM中的结构：\n```\n    // 创建 Vue 实例，得到 ViewModel\n    var vm = new Vue({\n      el: '#app',\n      data: {\n        txt: '',\n        list: [1, 2, 3, 4]\n      },\n      methods: {\n        add() {\n          this.list.push(this.txt);\n          this.txt = '';\n        }\n      }\n    });\n```\n\n\n### 列表的排序过渡\n`<transition-group>` 组件还有一个特殊之处。不仅可以进入和离开动画，**还可以改变定位**。要使用这个新功能只需了解新增的 `v-move` 特性，**它会在元素的改变定位的过程中应用**。\n+ `v-move` 和 `v-leave-active` 结合使用，能够让列表的过渡更加平缓柔和：\n```\n.v-move{\n  transition: all 0.8s ease;\n}\n.v-leave-active{\n  position: absolute;\n}\n```\n\n\n\n## 相关文章\n1. [vue.js 1.x 文档](https://v1-cn.vuejs.org/)\n2. [vue.js 2.x 文档](https://cn.vuejs.org/)\n3. [String.prototype.padStart(maxLength, fillString)](http://www.css88.com/archives/7715)\n4. [js 里面的键盘事件对应的键码](http://www.cnblogs.com/wuhua1/p/6686237.html)\n5. [pagekit/vue-resource](https://github.com/pagekit/vue-resource)\n6. [navicat如何导入sql文件和导出sql文件](https://jingyan.baidu.com/article/a65957f4976aad24e67f9b9b.html)\n7. [贝塞尔在线生成器](http://cubic-bezier.com/#.4,-0.3,1,.33)\n","tags":["vue","vue2.0基础课程"],"categories":["vue2.0基础课程"]},{"title":"Vue.js - Day1","url":"/2021/05/02/vue/vue2.0基础课程/day1/","content":"\n# Vue.js - Day1\n\n## 课程介绍\n前5天： 都在学习Vue基本的语法和概念；打包工具 Webpack , Gulp\n后5天： 以项目驱动教学；\n\n\n### 什么是Vue.js\n\n+ Vue.js 是目前最火的一个前端框架，React是最流行的一个前端框架（React除了开发网站，还可以开发手机App， Vue语法也是可以用于进行手机App开发的，需要借助于Weex）\n\n+ Vue.js 是前端的**主流框架之一**，和Angular.js、React.js 一起，并成为前端三大主流框架！\n\n+ Vue.js 是一套构建用户界面的框架，**只关注视图层**，它不仅易于上手，还便于与第三方库或既有项目整合。（Vue有配套的第三方类库，可以整合起来做大型项目的开发）\n\n+ 前端的主要工作？主要负责MVC中的V这一层；主要工作就是和界面打交道，来制作前端页面效果；\n\n\n\n\n\n## 为什么要学习流行框架\n + 企业为了提高开发效率：在企业中，时间就是效率，效率就是金钱；\n  - 企业中，使用框架，能够提高开发的效率；\n\n\n\n + 提高开发效率的发展历程：原生JS -> Jquery之类的类库 -> 前端模板引擎 -> Angular.js / Vue.js（能够帮助我们减少不必要的DOM操作；提高渲染效率；双向数据绑定的概念【通过框架提供的指令，我们前端程序员只需要关心数据的业务逻辑，不再关心DOM是如何渲染的了】）\n + 在Vue中，一个核心的概念，就是让用户不再操作DOM元素，解放了用户的双手，让程序员可以更多的时间去关注业务逻辑；\n\n\n\n + 增强自己就业时候的竞争力\n  - 人无我有，人有我优\n  - 你平时不忙的时候，都在干嘛？\n\n## 框架和库的区别\n\n\n\n + 框架：是一套完整的解决方案；对项目的侵入性较大，项目如果需要更换框架，则需要重新架构整个项目。\n\n  - node 中的 express；\n\n\n\n + 库（插件）：提供某一个小功能，对项目的侵入性较小，如果某个库无法完成某些需求，可以很容易切换到其它库实现需求。\n  - 1. 从Jquery 切换到 Zepto\n  - 2. 从 EJS 切换到 art-template\n\n\n\n\n\n\n\n## Node（后端）中的 MVC 与 前端中的 MVVM 之间的区别\n\n + MVC 是后端的分层开发概念；\n + MVVM是前端视图层的概念，主要关注于 视图层分离，也就是说：MVVM把前端的视图层，分为了 三部分 Model, View , VM ViewModel\n\n + 为什么有了MVC还要有MVVM\n\n\n\n## Vue.js 基本代码 和 MVVM 之间的对应关系\n![01 MVC和MVVM的关系图解](https://user-images.githubusercontent.com/28568478/116802575-5b22cd80-ab46-11eb-8f55-8b75473b6d4a.png)\n\n## Vue之 - `基本的代码结构`和`插值表达式`、`v-cloak`\n\n## Vue指令之`v-text`和`v-html`\n\n## Vue指令之`v-bind`的三种用法\n\n1. 直接使用指令`v-bind`\n\n2. 使用简化指令`:`\n\n3. 在绑定的时候，拼接绑定内容：`:title=\"btnTitle + ', 这是追加的内容'\"`\n\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Document</title>\n    <!-- <script src=\"./lib/vue.js\"></script> -->\n    <style>\n        /* 默认加入v-cloak的元素 隐藏 */\n        [v-cloak] {\n            display: none;\n        }\n    </style>\n</head>\n<body>\n    <div id=\"app\">\n        <!-- 使用v-cloak能够解决插值表达式闪烁的问题 -->\n        <p v-cloak>{{msg}}</p>\n        <!-- 默认v-text是没有闪烁问题的\n            v-text 会覆盖元素中原本的内容，但是插值表达式 只会替换自己的这个占位符 不会吧扎鞥个元素内容清空\n        -->\n        <h4 v-text=\"msg\"></h4>\n\n        <!-- v-html会解析html格式字符串 -->\n        <div>{{msg2}}</div>\n        <div v-text=\"msg2\"></div>\n        <div v-html=\"msg2\"></div>\n\n         <!-- v-bind 是vue中提供用于绑定属性的指令 -->\n         <input type=\"button\" value=\"按钮\" title=\"123\">\n         <input type=\"button\" value=\"按钮\" title=\"mytitle\">\n         <input type=\"button\" value=\"按钮\" v-bind:title=\"mytitle\">\n         <input type=\"button\" value=\"按钮\" v-bind:title=\"mytitle+'123'\">\n         <!-- 简化写 -->\n         <input type=\"button\" value=\"按钮\" :title=\"mytitle+'456'\">\n\n\n         <!-- vue中 提供了v-on 事件绑定机制 -->\n         <!-- 这样写会报错 未定义alert方法 -->\n         <input type=\"button\" value=\"按钮\" :title=\"mytitle+' v-on'\" v-on:click=\"alert('报警')\">\n         <!-- 貌似v-bind:title不生效 -->\n         <input type=\"button\" value=\"按钮\" :title=\"show_title\" v-on:click=\"show\">\n         <input type=\"button\" value=\"按钮\" :title=\"show_title\" v-on:mouseover=\"show\">\n         <!-- v-on缩写 用@代替 v-on:-->\n         <input type=\"button\" value=\"按钮\" :title=\"show_title\" @mouseover=\"show\">\n\n\n    </div>\n\n\n\n    <!-- vue.js放在这里时 如果网速过慢 会导致 {{msg}} 展现在页面\n        需要加v-cloak 设置样式为none 这样在网速慢的时候加载出来不会出现{{msg}}\n        而是当msg数据返回时才会展示信息\n    -->\n    <script src=\"./lib/vue.js\"></script>\n    <script>\n        var vm = new Vue({\n            el: \"#app\",\n            data: {\n                msg:123,\n                msg2:'<h1>哈哈 我是一个大大的H1</h1>',\n                mytitle:\"这是一个自己定义的title\"\n            },\n            methods:{ // 这个methods 属性中国定义了当前vue实例所有可用的方法\n                show: function () {\n                    alert(\"报警\")\n                 }\n            }\n        })\n    </script>\n\n</body>\n</html>\n```\n\n\n## Vue指令之`v-on`和`跑马灯效果`\n\n\n\n### 跑马灯效果\n\n1. HTML结构：\n\n```\n\n<div id=\"app\">\n\n    <p>{{info}}</p>\n\n    <input type=\"button\" value=\"开启\" v-on:click=\"go\">\n\n    <input type=\"button\" value=\"停止\" v-on:click=\"stop\">\n\n  </div>\n\n```\n\n2. Vue实例：\n\n```\n\n\t// 创建 Vue 实例，得到 ViewModel\n\n    var vm = new Vue({\n\n      el: '#app',\n\n      data: {\n\n        info: '猥琐发育，别浪~！',\n\n        intervalId: null\n\n      },\n\n      methods: {\n\n        go() {\n\n          // 如果当前有定时器在运行，则直接return\n\n          if (this.intervalId != null) {\n\n            return;\n\n          }\n\n          // 开始定时器\n\n          this.intervalId = setInterval(() => {\n\n            this.info = this.info.substring(1) + this.info.substring(0, 1);\n\n          }, 500);\n\n        },\n\n        stop() {\n\n          clearInterval(this.intervalId);\n\n        }\n\n      }\n\n    });\n\n```\n\n\n\n\n\n\n\n## Vue指令之`v-on的缩写`和`事件修饰符`\n\n\n\n### 事件修饰符：\n\n+ .stop       阻止冒泡\n\n+ .prevent    阻止默认事件\n\n+ .capture    添加事件侦听器时使用事件捕获模式\n\n+ .self       只当事件在该元素本身（比如不是子元素）触发时触发回调\n\n+ .once       事件只触发一次\n\n\n\n\n\n\n\n## Vue指令之`v-model`和`双向数据绑定`\n\n\n\n\n\n\n\n## 简易计算器案例\n\n1. HTML 代码结构\n\n```\n\n  <div id=\"app\">\n\n    <input type=\"text\" v-model=\"n1\">\n\n    <select v-model=\"opt\">\n\n      <option value=\"0\">+</option>\n\n      <option value=\"1\">-</option>\n\n      <option value=\"2\">*</option>\n\n      <option value=\"3\">÷</option>\n\n    </select>\n\n    <input type=\"text\" v-model=\"n2\">\n\n    <input type=\"button\" value=\"=\" v-on:click=\"getResult\">\n\n    <input type=\"text\" v-model=\"result\">\n\n  </div>\n\n```\n\n2. Vue实例代码：\n\n```\n\n\t// 创建 Vue 实例，得到 ViewModel\n\n    var vm = new Vue({\n\n      el: '#app',\n\n      data: {\n\n        n1: 0,\n\n        n2: 0,\n\n        result: 0,\n\n        opt: '0'\n\n      },\n\n      methods: {\n\n        getResult() {\n\n          switch (this.opt) {\n\n            case '0':\n\n              this.result = parseInt(this.n1) + parseInt(this.n2);\n\n              break;\n\n            case '1':\n\n              this.result = parseInt(this.n1) - parseInt(this.n2);\n\n              break;\n\n            case '2':\n\n              this.result = parseInt(this.n1) * parseInt(this.n2);\n\n              break;\n\n            case '3':\n\n              this.result = parseInt(this.n1) / parseInt(this.n2);\n\n              break;\n\n          }\n\n        }\n\n      }\n\n    });\n\n```\n\n\n\n\n\n\n\n\n\n## 在Vue中使用样式\n\n\n\n### 使用class样式\n\n1. 数组\n```\n<h1 :class=\"['red', 'thin']\">这是一个邪恶的H1</h1>\n```\n\n2. 数组中使用三元表达式\n```\n<h1 :class=\"['red', 'thin', isactive?'active':'']\">这是一个邪恶的H1</h1>\n```\n\n3. 数组中嵌套对象\n```\n<h1 :class=\"['red', 'thin', {'active': isactive}]\">这是一个邪恶的H1</h1>\n```\n\n4. 直接使用对象\n```\n<h1 :class=\"{red:true, italic:true, active:true, thin:true}\">这是一个邪恶的H1</h1>\n```\n\n\n\n### 使用内联样式\n\n1. 直接在元素上通过 `:style` 的形式，书写样式对象\n```\n<h1 :style=\"{color: 'red', 'font-size': '40px'}\">这是一个善良的H1</h1>\n```\n\n2. 将样式对象，定义到 `data` 中，并直接引用到 `:style` 中\n + 在data上定义样式：\n```\ndata: {\n        h1StyleObj: { color: 'red', 'font-size': '40px', 'font-weight': '200' }\n}\n```\n + 在元素中，通过属性绑定的形式，将样式对象应用到元素中：\n```\n<h1 :style=\"h1StyleObj\">这是一个善良的H1</h1>\n```\n\n3. 在 `:style` 中通过数组，引用多个 `data` 上的样式对象\n + 在data上定义样式：\n```\ndata: {\n        h1StyleObj: { color: 'red', 'font-size': '40px', 'font-weight': '200' },\n        h1StyleObj2: { fontStyle: 'italic' }\n}\n```\n + 在元素中，通过属性绑定的形式，将样式对象应用到元素中：\n```\n<h1 :style=\"[h1StyleObj, h1StyleObj2]\">这是一个善良的H1</h1>\n```\n\n\n\n## Vue指令之`v-for`和`key`属性\n\n1. 迭代数组\n\n```\n<ul>\n  <li v-for=\"(item, i) in list\">索引：{{i}} --- 姓名：{{item.name}} --- 年龄：{{item.age}}</li>\n</ul>\n```\n\n2. 迭代对象中的属性\n\n```\n\n\t<!-- 循环遍历对象身上的属性 -->\n\n    <div v-for=\"(val, key, i) in userInfo\">{{val}} --- {{key}} --- {{i}}</div>\n\n```\n\n3. 迭代数字\n\n```\n\n<p v-for=\"i in 10\">这是第 {{i}} 个P标签</p>\n\n```\n\n\n\n> 2.2.0+ 的版本里，**当在组件中使用** v-for 时，key 现在是必须的。\n\n\n\n当 Vue.js 用 v-for 正在更新已渲染过的元素列表时，它默认用 “**就地复用**” 策略。如果数据项的顺序被改变，Vue将**不是移动 DOM 元素来匹配数据项的顺序**， 而是**简单复用此处每个元素**，并且确保它在特定索引下显示已被渲染过的每个元素。\n\n\n\n为了给 Vue 一个提示，**以便它能跟踪每个节点的身份，从而重用和重新排序现有元素**，你需要为每项提供一个唯一 key 属性。\n\n\n\n\n\n\n\n## Vue指令之`v-if`和`v-show`\n\n\n\n\n\n\n\n> 一般来说，v-if 有更高的切换消耗而 v-show 有更高的初始渲染消耗。因此，如果需要频繁切换 v-show 较好，如果在运行时条件不大可能改变 v-if 较好。\n\n\n\n\n\n\n\n## 品牌管理案例\n\n\n\n### 添加新品牌\n\n\n\n### 删除品牌\n\n\n\n### 根据条件筛选品牌\n\n1. 1.x 版本中的filterBy指令，在2.x中已经被废除：\n\n[filterBy - 指令](https://v1-cn.vuejs.org/api/#filterBy)\n\n```\n\n<tr v-for=\"item in list | filterBy searchName in 'name'\">\n\n  <td>{{item.id}}</td>\n\n  <td>{{item.name}}</td>\n\n  <td>{{item.ctime}}</td>\n\n  <td>\n\n    <a href=\"#\" @click.prevent=\"del(item.id)\">删除</a>\n\n  </td>\n\n</tr>\n\n```\n\n2. 在2.x版本中[手动实现筛选的方式](https://cn.vuejs.org/v2/guide/list.html#显示过滤-排序结果)：\n\n+ 筛选框绑定到 VM 实例中的 `searchName` 属性：\n\n```\n\n<hr> 输入筛选名称：\n\n<input type=\"text\" v-model=\"searchName\">\n\n```\n\n+ 在使用 `v-for` 指令循环每一行数据的时候，不再直接 `item in list`，而是 `in` 一个 过滤的methods 方法，同时，把过滤条件`searchName`传递进去：\n\n```\n\n<tbody>\n\n      <tr v-for=\"item in search(searchName)\">\n\n        <td>{{item.id}}</td>\n\n        <td>{{item.name}}</td>\n\n        <td>{{item.ctime}}</td>\n\n        <td>\n\n          <a href=\"#\" @click.prevent=\"del(item.id)\">删除</a>\n\n        </td>\n\n      </tr>\n\n    </tbody>\n\n```\n\n+ `search` 过滤方法中，使用 数组的 `filter` 方法进行过滤：\n\n```\n\nsearch(name) {\n\n  return this.list.filter(x => {\n\n    return x.name.indexOf(name) != -1;\n\n  });\n\n}\n\n```\n\n\n\n\n\n\n\n## Vue调试工具`vue-devtools`的安装步骤和使用\n\n[Vue.js devtools - 翻墙安装方式 - 推荐](https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd?hl=zh-CN)\n\n\n\n\n\n## 过滤器\n\n概念：Vue.js 允许你自定义过滤器，**可被用作一些常见的文本格式化**。过滤器可以用在两个地方：**mustache 插值和 v-bind 表达式**。过滤器应该被添加在 JavaScript 表达式的尾部，由“管道”符指示；\n\n### 私有过滤器\n\n1. HTML元素：\n\n```\n\n<td>{{item.ctime | dataFormat('yyyy-mm-dd')}}</td>\n\n```\n\n2. 私有 `filters` 定义方式：\n\n```\n\nfilters: { // 私有局部过滤器，只能在 当前 VM 对象所控制的 View 区域进行使用\n\n    dataFormat(input, pattern = \"\") { // 在参数列表中 通过 pattern=\"\" 来指定形参默认值，防止报错\n\n      var dt = new Date(input);\n\n      // 获取年月日\n\n      var y = dt.getFullYear();\n\n      var m = (dt.getMonth() + 1).toString().padStart(2, '0');\n\n      var d = dt.getDate().toString().padStart(2, '0');\n\n\n\n      // 如果 传递进来的字符串类型，转为小写之后，等于 yyyy-mm-dd，那么就返回 年-月-日\n\n      // 否则，就返回  年-月-日 时：分：秒\n\n      if (pattern.toLowerCase() === 'yyyy-mm-dd') {\n\n        return `${y}-${m}-${d}`;\n\n      } else {\n\n        // 获取时分秒\n\n        var hh = dt.getHours().toString().padStart(2, '0');\n\n        var mm = dt.getMinutes().toString().padStart(2, '0');\n\n        var ss = dt.getSeconds().toString().padStart(2, '0');\n\n\n\n        return `${y}-${m}-${d} ${hh}:${mm}:${ss}`;\n\n      }\n\n    }\n\n  }\n\n```\n\n\n\n> 使用ES6中的字符串新方法 String.prototype.padStart(maxLength, fillString='') 或 String.prototype.padEnd(maxLength, fillString='')来填充字符串；\n\n\n\n\n\n### 全局过滤器\n\n```\n\n// 定义一个全局过滤器\n\nVue.filter('dataFormat', function (input, pattern = '') {\n\n  var dt = new Date(input);\n\n  // 获取年月日\n\n  var y = dt.getFullYear();\n\n  var m = (dt.getMonth() + 1).toString().padStart(2, '0');\n\n  var d = dt.getDate().toString().padStart(2, '0');\n\n\n\n  // 如果 传递进来的字符串类型，转为小写之后，等于 yyyy-mm-dd，那么就返回 年-月-日\n\n  // 否则，就返回  年-月-日 时：分：秒\n\n  if (pattern.toLowerCase() === 'yyyy-mm-dd') {\n\n    return `${y}-${m}-${d}`;\n\n  } else {\n\n    // 获取时分秒\n\n    var hh = dt.getHours().toString().padStart(2, '0');\n\n    var mm = dt.getMinutes().toString().padStart(2, '0');\n\n    var ss = dt.getSeconds().toString().padStart(2, '0');\n\n\n\n    return `${y}-${m}-${d} ${hh}:${mm}:${ss}`;\n\n  }\n\n});\n\n```\n\n\n\n> 注意：当有局部和全局两个名称相同的过滤器时候，会以就近原则进行调用，即：局部过滤器优先于全局过滤器被调用！\n\n\n\n## 键盘修饰符以及自定义键盘修饰符\n\n### 1.x中自定义键盘修饰符【了解即可】\n\n```\n\nVue.directive('on').keyCodes.f2 = 113;\n\n```\n\n### [2.x中自定义键盘修饰符](https://cn.vuejs.org/v2/guide/events.html#键值修饰符)\n\n1. 通过`Vue.config.keyCodes.名称 = 按键值`来自定义案件修饰符的别名：\n\n```\n\nVue.config.keyCodes.f2 = 113;\n\n```\n\n2. 使用自定义的按键修饰符：\n\n```\n\n<input type=\"text\" v-model=\"name\" @keyup.f2=\"add\">\n\n```\n\n\n\n\n\n## [自定义指令](https://cn.vuejs.org/v2/guide/custom-directive.html)\n\n1. 自定义全局和局部的 自定义指令：\n\n```\n\n    // 自定义全局指令 v-focus，为绑定的元素自动获取焦点：\n\n    Vue.directive('focus', {\n\n      inserted: function (el) { // inserted 表示被绑定元素插入父节点时调用\n\n        el.focus();\n\n      }\n\n    });\n\n\n\n    // 自定义局部指令 v-color 和 v-font-weight，为绑定的元素设置指定的字体颜色 和 字体粗细：\n\n      directives: {\n\n        color: { // 为元素设置指定的字体颜色\n\n          bind(el, binding) {\n\n            el.style.color = binding.value;\n\n          }\n\n        },\n\n        'font-weight': function (el, binding2) { // 自定义指令的简写形式，等同于定义了 bind 和 update 两个钩子函数\n\n          el.style.fontWeight = binding2.value;\n\n        }\n\n      }\n\n```\n\n2. 自定义指令的使用方式：\n\n```\n\n<input type=\"text\" v-model=\"searchName\" v-focus v-color=\"'red'\" v-font-weight=\"900\">\n\n```\n\n\n\n## Vue 1.x 中 自定义元素指令【已废弃,了解即可】\n```\nVue.elementDirective('red-color', {\n  bind: function () {\n    this.el.style.color = 'red';\n  }\n});\n```\n使用方式：\n```\n<red-color>1232</red-color>\n```\n\n## 相关文章\n1. [vue.js 1.x 文档](https://v1-cn.vuejs.org/)\n2. [vue.js 2.x 文档](https://cn.vuejs.org/)\n3. [String.prototype.padStart(maxLength, fillString)](http://www.css88.com/archives/7715)\n4. [js 里面的键盘事件对应的键码](http://www.cnblogs.com/wuhua1/p/6686237.html)\n5. [Vue.js双向绑定的实现原理](http://www.cnblogs.com/kidney/p/6052935.html)\n","tags":["vue","vue2.0基础课程"],"categories":["vue2.0基础课程"]}]